{
  "best_metric": 0.6612387895584106,
  "best_model_checkpoint": "./results/checkpoint-11082",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 27705,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001804728388377549,
      "grad_norm": 5.381821155548096,
      "learning_rate": 2.9989171629669736e-05,
      "loss": 1.1054,
      "step": 10
    },
    {
      "epoch": 0.003609456776755098,
      "grad_norm": 6.578388690948486,
      "learning_rate": 2.997834325933947e-05,
      "loss": 1.1058,
      "step": 20
    },
    {
      "epoch": 0.005414185165132647,
      "grad_norm": 3.7150514125823975,
      "learning_rate": 2.9967514889009202e-05,
      "loss": 1.1059,
      "step": 30
    },
    {
      "epoch": 0.007218913553510196,
      "grad_norm": 6.3208136558532715,
      "learning_rate": 2.995668651867894e-05,
      "loss": 1.1297,
      "step": 40
    },
    {
      "epoch": 0.009023641941887746,
      "grad_norm": 4.099709987640381,
      "learning_rate": 2.9945858148348675e-05,
      "loss": 1.1175,
      "step": 50
    },
    {
      "epoch": 0.010828370330265295,
      "grad_norm": 2.699882745742798,
      "learning_rate": 2.993502977801841e-05,
      "loss": 1.1119,
      "step": 60
    },
    {
      "epoch": 0.012633098718642844,
      "grad_norm": 4.102763652801514,
      "learning_rate": 2.9924201407688142e-05,
      "loss": 1.1103,
      "step": 70
    },
    {
      "epoch": 0.014437827107020393,
      "grad_norm": 2.231818199157715,
      "learning_rate": 2.9913373037357877e-05,
      "loss": 1.1217,
      "step": 80
    },
    {
      "epoch": 0.016242555495397944,
      "grad_norm": 2.2311415672302246,
      "learning_rate": 2.990254466702761e-05,
      "loss": 1.0997,
      "step": 90
    },
    {
      "epoch": 0.01804728388377549,
      "grad_norm": 5.019392013549805,
      "learning_rate": 2.989171629669735e-05,
      "loss": 1.0708,
      "step": 100
    },
    {
      "epoch": 0.019852012272153042,
      "grad_norm": 5.54703426361084,
      "learning_rate": 2.9880887926367085e-05,
      "loss": 1.0853,
      "step": 110
    },
    {
      "epoch": 0.02165674066053059,
      "grad_norm": 4.347332000732422,
      "learning_rate": 2.9870059556036816e-05,
      "loss": 1.071,
      "step": 120
    },
    {
      "epoch": 0.02346146904890814,
      "grad_norm": 5.7787184715271,
      "learning_rate": 2.985923118570655e-05,
      "loss": 1.0478,
      "step": 130
    },
    {
      "epoch": 0.025266197437285687,
      "grad_norm": 7.197530746459961,
      "learning_rate": 2.9848402815376286e-05,
      "loss": 1.0505,
      "step": 140
    },
    {
      "epoch": 0.02707092582566324,
      "grad_norm": 5.500202178955078,
      "learning_rate": 2.983757444504602e-05,
      "loss": 0.9797,
      "step": 150
    },
    {
      "epoch": 0.028875654214040786,
      "grad_norm": 6.094206809997559,
      "learning_rate": 2.9826746074715756e-05,
      "loss": 1.0874,
      "step": 160
    },
    {
      "epoch": 0.030680382602418337,
      "grad_norm": 5.358415603637695,
      "learning_rate": 2.981591770438549e-05,
      "loss": 1.0447,
      "step": 170
    },
    {
      "epoch": 0.03248511099079589,
      "grad_norm": 9.537144660949707,
      "learning_rate": 2.9805089334055226e-05,
      "loss": 1.043,
      "step": 180
    },
    {
      "epoch": 0.03428983937917343,
      "grad_norm": 4.057625770568848,
      "learning_rate": 2.979426096372496e-05,
      "loss": 1.0207,
      "step": 190
    },
    {
      "epoch": 0.03609456776755098,
      "grad_norm": 9.25692367553711,
      "learning_rate": 2.9783432593394695e-05,
      "loss": 1.02,
      "step": 200
    },
    {
      "epoch": 0.03789929615592853,
      "grad_norm": 10.024502754211426,
      "learning_rate": 2.977368706009746e-05,
      "loss": 0.9771,
      "step": 210
    },
    {
      "epoch": 0.039704024544306084,
      "grad_norm": 6.846579551696777,
      "learning_rate": 2.976285868976719e-05,
      "loss": 1.0753,
      "step": 220
    },
    {
      "epoch": 0.04150875293268363,
      "grad_norm": 10.797117233276367,
      "learning_rate": 2.9752030319436925e-05,
      "loss": 0.9516,
      "step": 230
    },
    {
      "epoch": 0.04331348132106118,
      "grad_norm": 6.692244052886963,
      "learning_rate": 2.974120194910666e-05,
      "loss": 0.9748,
      "step": 240
    },
    {
      "epoch": 0.04511820970943873,
      "grad_norm": 8.787487030029297,
      "learning_rate": 2.9730373578776395e-05,
      "loss": 1.0668,
      "step": 250
    },
    {
      "epoch": 0.04692293809781628,
      "grad_norm": 10.048714637756348,
      "learning_rate": 2.9719545208446127e-05,
      "loss": 1.0045,
      "step": 260
    },
    {
      "epoch": 0.04872766648619383,
      "grad_norm": 11.513998031616211,
      "learning_rate": 2.9708716838115865e-05,
      "loss": 0.9533,
      "step": 270
    },
    {
      "epoch": 0.050532394874571375,
      "grad_norm": 9.415833473205566,
      "learning_rate": 2.96978884677856e-05,
      "loss": 0.9783,
      "step": 280
    },
    {
      "epoch": 0.052337123262948926,
      "grad_norm": 7.199966907501221,
      "learning_rate": 2.9687060097455335e-05,
      "loss": 0.8651,
      "step": 290
    },
    {
      "epoch": 0.05414185165132648,
      "grad_norm": 9.867547988891602,
      "learning_rate": 2.967623172712507e-05,
      "loss": 0.8859,
      "step": 300
    },
    {
      "epoch": 0.05594658003970403,
      "grad_norm": 6.908426284790039,
      "learning_rate": 2.96654033567948e-05,
      "loss": 0.9369,
      "step": 310
    },
    {
      "epoch": 0.05775130842808157,
      "grad_norm": 11.54138469696045,
      "learning_rate": 2.9654574986464536e-05,
      "loss": 0.9057,
      "step": 320
    },
    {
      "epoch": 0.05955603681645912,
      "grad_norm": 8.57558536529541,
      "learning_rate": 2.9643746616134274e-05,
      "loss": 0.9377,
      "step": 330
    },
    {
      "epoch": 0.06136076520483667,
      "grad_norm": 8.420949935913086,
      "learning_rate": 2.963291824580401e-05,
      "loss": 0.9223,
      "step": 340
    },
    {
      "epoch": 0.06316549359321422,
      "grad_norm": 7.435086727142334,
      "learning_rate": 2.962208987547374e-05,
      "loss": 0.9743,
      "step": 350
    },
    {
      "epoch": 0.06497022198159177,
      "grad_norm": 9.461044311523438,
      "learning_rate": 2.9611261505143476e-05,
      "loss": 0.9654,
      "step": 360
    },
    {
      "epoch": 0.06677495036996932,
      "grad_norm": 5.387014389038086,
      "learning_rate": 2.960043313481321e-05,
      "loss": 0.986,
      "step": 370
    },
    {
      "epoch": 0.06857967875834686,
      "grad_norm": 10.864046096801758,
      "learning_rate": 2.9589604764482945e-05,
      "loss": 0.9505,
      "step": 380
    },
    {
      "epoch": 0.07038440714672442,
      "grad_norm": 8.110901832580566,
      "learning_rate": 2.9578776394152684e-05,
      "loss": 0.9583,
      "step": 390
    },
    {
      "epoch": 0.07218913553510196,
      "grad_norm": 6.58347225189209,
      "learning_rate": 2.9567948023822415e-05,
      "loss": 0.969,
      "step": 400
    },
    {
      "epoch": 0.07399386392347952,
      "grad_norm": 10.135149955749512,
      "learning_rate": 2.955711965349215e-05,
      "loss": 0.9689,
      "step": 410
    },
    {
      "epoch": 0.07579859231185707,
      "grad_norm": 9.425774574279785,
      "learning_rate": 2.9546291283161885e-05,
      "loss": 0.9796,
      "step": 420
    },
    {
      "epoch": 0.07760332070023461,
      "grad_norm": 3.916858673095703,
      "learning_rate": 2.953546291283162e-05,
      "loss": 0.9626,
      "step": 430
    },
    {
      "epoch": 0.07940804908861217,
      "grad_norm": 3.797269582748413,
      "learning_rate": 2.952463454250135e-05,
      "loss": 0.933,
      "step": 440
    },
    {
      "epoch": 0.08121277747698971,
      "grad_norm": 7.1411213874816895,
      "learning_rate": 2.951380617217109e-05,
      "loss": 0.8616,
      "step": 450
    },
    {
      "epoch": 0.08301750586536726,
      "grad_norm": 8.992775917053223,
      "learning_rate": 2.9502977801840825e-05,
      "loss": 0.8705,
      "step": 460
    },
    {
      "epoch": 0.08482223425374481,
      "grad_norm": 4.926819324493408,
      "learning_rate": 2.949214943151056e-05,
      "loss": 0.9431,
      "step": 470
    },
    {
      "epoch": 0.08662696264212236,
      "grad_norm": 10.201313972473145,
      "learning_rate": 2.9481321061180295e-05,
      "loss": 0.8539,
      "step": 480
    },
    {
      "epoch": 0.08843169103049991,
      "grad_norm": Infinity,
      "learning_rate": 2.9471575527883055e-05,
      "loss": 0.8994,
      "step": 490
    },
    {
      "epoch": 0.09023641941887746,
      "grad_norm": 6.638600826263428,
      "learning_rate": 2.9460747157552786e-05,
      "loss": 0.9019,
      "step": 500
    },
    {
      "epoch": 0.092041147807255,
      "grad_norm": 12.317709922790527,
      "learning_rate": 2.9449918787222524e-05,
      "loss": 0.908,
      "step": 510
    },
    {
      "epoch": 0.09384587619563256,
      "grad_norm": 9.747337341308594,
      "learning_rate": 2.943909041689226e-05,
      "loss": 0.8702,
      "step": 520
    },
    {
      "epoch": 0.0956506045840101,
      "grad_norm": 10.175830841064453,
      "learning_rate": 2.9428262046561994e-05,
      "loss": 0.9077,
      "step": 530
    },
    {
      "epoch": 0.09745533297238766,
      "grad_norm": 6.656169891357422,
      "learning_rate": 2.941743367623173e-05,
      "loss": 0.9614,
      "step": 540
    },
    {
      "epoch": 0.0992600613607652,
      "grad_norm": 4.783633708953857,
      "learning_rate": 2.940660530590146e-05,
      "loss": 1.0309,
      "step": 550
    },
    {
      "epoch": 0.10106478974914275,
      "grad_norm": 6.104677200317383,
      "learning_rate": 2.9395776935571196e-05,
      "loss": 0.9117,
      "step": 560
    },
    {
      "epoch": 0.10286951813752031,
      "grad_norm": 6.944583415985107,
      "learning_rate": 2.9384948565240934e-05,
      "loss": 0.8913,
      "step": 570
    },
    {
      "epoch": 0.10467424652589785,
      "grad_norm": 7.40212869644165,
      "learning_rate": 2.937412019491067e-05,
      "loss": 0.9674,
      "step": 580
    },
    {
      "epoch": 0.1064789749142754,
      "grad_norm": 5.961738586425781,
      "learning_rate": 2.93632918245804e-05,
      "loss": 0.898,
      "step": 590
    },
    {
      "epoch": 0.10828370330265295,
      "grad_norm": 7.5712361335754395,
      "learning_rate": 2.9352463454250135e-05,
      "loss": 0.8813,
      "step": 600
    },
    {
      "epoch": 0.1100884316910305,
      "grad_norm": 5.847611427307129,
      "learning_rate": 2.934163508391987e-05,
      "loss": 0.8294,
      "step": 610
    },
    {
      "epoch": 0.11189316007940805,
      "grad_norm": 9.567779541015625,
      "learning_rate": 2.9330806713589605e-05,
      "loss": 0.9225,
      "step": 620
    },
    {
      "epoch": 0.1136978884677856,
      "grad_norm": 14.120335578918457,
      "learning_rate": 2.9319978343259343e-05,
      "loss": 1.0214,
      "step": 630
    },
    {
      "epoch": 0.11550261685616314,
      "grad_norm": 6.601929187774658,
      "learning_rate": 2.9309149972929075e-05,
      "loss": 0.8976,
      "step": 640
    },
    {
      "epoch": 0.1173073452445407,
      "grad_norm": 10.431164741516113,
      "learning_rate": 2.929832160259881e-05,
      "loss": 0.9949,
      "step": 650
    },
    {
      "epoch": 0.11911207363291824,
      "grad_norm": 6.499758720397949,
      "learning_rate": 2.9287493232268545e-05,
      "loss": 0.8683,
      "step": 660
    },
    {
      "epoch": 0.12091680202129579,
      "grad_norm": 9.061306953430176,
      "learning_rate": 2.927666486193828e-05,
      "loss": 0.7933,
      "step": 670
    },
    {
      "epoch": 0.12272153040967335,
      "grad_norm": 8.551477432250977,
      "learning_rate": 2.926583649160801e-05,
      "loss": 0.9352,
      "step": 680
    },
    {
      "epoch": 0.12452625879805089,
      "grad_norm": 12.354178428649902,
      "learning_rate": 2.925500812127775e-05,
      "loss": 0.8331,
      "step": 690
    },
    {
      "epoch": 0.12633098718642843,
      "grad_norm": 8.04854679107666,
      "learning_rate": 2.9244179750947484e-05,
      "loss": 1.0091,
      "step": 700
    },
    {
      "epoch": 0.128135715574806,
      "grad_norm": 6.022332191467285,
      "learning_rate": 2.923335138061722e-05,
      "loss": 0.8275,
      "step": 710
    },
    {
      "epoch": 0.12994044396318355,
      "grad_norm": 9.238451957702637,
      "learning_rate": 2.922252301028695e-05,
      "loss": 0.9816,
      "step": 720
    },
    {
      "epoch": 0.13174517235156108,
      "grad_norm": 8.982215881347656,
      "learning_rate": 2.9211694639956686e-05,
      "loss": 0.8843,
      "step": 730
    },
    {
      "epoch": 0.13354990073993864,
      "grad_norm": 7.861795902252197,
      "learning_rate": 2.920086626962642e-05,
      "loss": 0.8563,
      "step": 740
    },
    {
      "epoch": 0.1353546291283162,
      "grad_norm": 12.169869422912598,
      "learning_rate": 2.919003789929616e-05,
      "loss": 0.8629,
      "step": 750
    },
    {
      "epoch": 0.13715935751669373,
      "grad_norm": 8.917576789855957,
      "learning_rate": 2.9179209528965894e-05,
      "loss": 0.8671,
      "step": 760
    },
    {
      "epoch": 0.13896408590507128,
      "grad_norm": 7.562767505645752,
      "learning_rate": 2.9168381158635625e-05,
      "loss": 0.8172,
      "step": 770
    },
    {
      "epoch": 0.14076881429344884,
      "grad_norm": 10.754105567932129,
      "learning_rate": 2.915755278830536e-05,
      "loss": 0.9214,
      "step": 780
    },
    {
      "epoch": 0.1425735426818264,
      "grad_norm": 8.553123474121094,
      "learning_rate": 2.9146724417975095e-05,
      "loss": 0.8874,
      "step": 790
    },
    {
      "epoch": 0.14437827107020393,
      "grad_norm": 12.241703033447266,
      "learning_rate": 2.913589604764483e-05,
      "loss": 0.8459,
      "step": 800
    },
    {
      "epoch": 0.1461829994585815,
      "grad_norm": 13.162882804870605,
      "learning_rate": 2.9125067677314565e-05,
      "loss": 0.8952,
      "step": 810
    },
    {
      "epoch": 0.14798772784695904,
      "grad_norm": 11.184222221374512,
      "learning_rate": 2.91142393069843e-05,
      "loss": 0.7972,
      "step": 820
    },
    {
      "epoch": 0.14979245623533657,
      "grad_norm": 5.194579601287842,
      "learning_rate": 2.9103410936654035e-05,
      "loss": 0.8167,
      "step": 830
    },
    {
      "epoch": 0.15159718462371413,
      "grad_norm": 6.674694061279297,
      "learning_rate": 2.909258256632377e-05,
      "loss": 0.857,
      "step": 840
    },
    {
      "epoch": 0.1534019130120917,
      "grad_norm": 9.417603492736816,
      "learning_rate": 2.9081754195993504e-05,
      "loss": 0.6974,
      "step": 850
    },
    {
      "epoch": 0.15520664140046922,
      "grad_norm": 6.694371700286865,
      "learning_rate": 2.9070925825663236e-05,
      "loss": 0.95,
      "step": 860
    },
    {
      "epoch": 0.15701136978884678,
      "grad_norm": 9.837814331054688,
      "learning_rate": 2.9060097455332974e-05,
      "loss": 0.8159,
      "step": 870
    },
    {
      "epoch": 0.15881609817722434,
      "grad_norm": 19.204008102416992,
      "learning_rate": 2.904926908500271e-05,
      "loss": 0.9321,
      "step": 880
    },
    {
      "epoch": 0.16062082656560187,
      "grad_norm": 7.899177551269531,
      "learning_rate": 2.9038440714672444e-05,
      "loss": 0.9432,
      "step": 890
    },
    {
      "epoch": 0.16242555495397942,
      "grad_norm": 6.376984119415283,
      "learning_rate": 2.9027612344342175e-05,
      "loss": 0.8904,
      "step": 900
    },
    {
      "epoch": 0.16423028334235698,
      "grad_norm": 9.205339431762695,
      "learning_rate": 2.901678397401191e-05,
      "loss": 0.8459,
      "step": 910
    },
    {
      "epoch": 0.1660350117307345,
      "grad_norm": 13.533122062683105,
      "learning_rate": 2.9005955603681645e-05,
      "loss": 0.9114,
      "step": 920
    },
    {
      "epoch": 0.16783974011911207,
      "grad_norm": 8.439469337463379,
      "learning_rate": 2.8995127233351384e-05,
      "loss": 0.8715,
      "step": 930
    },
    {
      "epoch": 0.16964446850748963,
      "grad_norm": 11.470810890197754,
      "learning_rate": 2.898429886302112e-05,
      "loss": 0.8693,
      "step": 940
    },
    {
      "epoch": 0.17144919689586718,
      "grad_norm": 9.662571907043457,
      "learning_rate": 2.897347049269085e-05,
      "loss": 0.9115,
      "step": 950
    },
    {
      "epoch": 0.17325392528424471,
      "grad_norm": 12.647553443908691,
      "learning_rate": 2.8962642122360585e-05,
      "loss": 0.7762,
      "step": 960
    },
    {
      "epoch": 0.17505865367262227,
      "grad_norm": 7.335160255432129,
      "learning_rate": 2.895181375203032e-05,
      "loss": 0.8361,
      "step": 970
    },
    {
      "epoch": 0.17686338206099983,
      "grad_norm": 9.232941627502441,
      "learning_rate": 2.8940985381700055e-05,
      "loss": 0.8908,
      "step": 980
    },
    {
      "epoch": 0.17866811044937736,
      "grad_norm": 8.899298667907715,
      "learning_rate": 2.893015701136979e-05,
      "loss": 0.8646,
      "step": 990
    },
    {
      "epoch": 0.18047283883775492,
      "grad_norm": 7.6546783447265625,
      "learning_rate": 2.8919328641039525e-05,
      "loss": 0.7307,
      "step": 1000
    },
    {
      "epoch": 0.18227756722613248,
      "grad_norm": 6.8560791015625,
      "learning_rate": 2.890850027070926e-05,
      "loss": 0.8691,
      "step": 1010
    },
    {
      "epoch": 0.18408229561451,
      "grad_norm": 6.258758068084717,
      "learning_rate": 2.8897671900378994e-05,
      "loss": 0.8431,
      "step": 1020
    },
    {
      "epoch": 0.18588702400288756,
      "grad_norm": 11.247258186340332,
      "learning_rate": 2.888684353004873e-05,
      "loss": 0.7911,
      "step": 1030
    },
    {
      "epoch": 0.18769175239126512,
      "grad_norm": 11.97677993774414,
      "learning_rate": 2.887601515971846e-05,
      "loss": 0.7164,
      "step": 1040
    },
    {
      "epoch": 0.18949648077964265,
      "grad_norm": 12.894329071044922,
      "learning_rate": 2.88651867893882e-05,
      "loss": 1.0256,
      "step": 1050
    },
    {
      "epoch": 0.1913012091680202,
      "grad_norm": 12.357414245605469,
      "learning_rate": 2.8854358419057934e-05,
      "loss": 0.7915,
      "step": 1060
    },
    {
      "epoch": 0.19310593755639777,
      "grad_norm": 15.295920372009277,
      "learning_rate": 2.884353004872767e-05,
      "loss": 0.7476,
      "step": 1070
    },
    {
      "epoch": 0.19491066594477532,
      "grad_norm": 8.479270935058594,
      "learning_rate": 2.88327016783974e-05,
      "loss": 0.9349,
      "step": 1080
    },
    {
      "epoch": 0.19671539433315285,
      "grad_norm": 7.939948081970215,
      "learning_rate": 2.8821873308067135e-05,
      "loss": 0.9242,
      "step": 1090
    },
    {
      "epoch": 0.1985201227215304,
      "grad_norm": 8.2794771194458,
      "learning_rate": 2.881104493773687e-05,
      "loss": 0.8849,
      "step": 1100
    },
    {
      "epoch": 0.20032485110990797,
      "grad_norm": 10.748075485229492,
      "learning_rate": 2.880021656740661e-05,
      "loss": 0.8474,
      "step": 1110
    },
    {
      "epoch": 0.2021295794982855,
      "grad_norm": 7.926647663116455,
      "learning_rate": 2.8789388197076343e-05,
      "loss": 0.7985,
      "step": 1120
    },
    {
      "epoch": 0.20393430788666306,
      "grad_norm": 14.183449745178223,
      "learning_rate": 2.8778559826746075e-05,
      "loss": 0.8243,
      "step": 1130
    },
    {
      "epoch": 0.20573903627504062,
      "grad_norm": 9.600974082946777,
      "learning_rate": 2.876773145641581e-05,
      "loss": 0.7603,
      "step": 1140
    },
    {
      "epoch": 0.20754376466341815,
      "grad_norm": 7.60609769821167,
      "learning_rate": 2.8756903086085545e-05,
      "loss": 0.8822,
      "step": 1150
    },
    {
      "epoch": 0.2093484930517957,
      "grad_norm": 8.339284896850586,
      "learning_rate": 2.874607471575528e-05,
      "loss": 0.808,
      "step": 1160
    },
    {
      "epoch": 0.21115322144017326,
      "grad_norm": 9.39397144317627,
      "learning_rate": 2.8735246345425015e-05,
      "loss": 0.8008,
      "step": 1170
    },
    {
      "epoch": 0.2129579498285508,
      "grad_norm": 6.472289085388184,
      "learning_rate": 2.872441797509475e-05,
      "loss": 0.7493,
      "step": 1180
    },
    {
      "epoch": 0.21476267821692835,
      "grad_norm": 12.333011627197266,
      "learning_rate": 2.8713589604764484e-05,
      "loss": 0.9041,
      "step": 1190
    },
    {
      "epoch": 0.2165674066053059,
      "grad_norm": 4.755180358886719,
      "learning_rate": 2.870276123443422e-05,
      "loss": 0.8969,
      "step": 1200
    },
    {
      "epoch": 0.21837213499368346,
      "grad_norm": 6.841787338256836,
      "learning_rate": 2.8691932864103954e-05,
      "loss": 0.8565,
      "step": 1210
    },
    {
      "epoch": 0.220176863382061,
      "grad_norm": 9.735128402709961,
      "learning_rate": 2.8681104493773686e-05,
      "loss": 0.8336,
      "step": 1220
    },
    {
      "epoch": 0.22198159177043855,
      "grad_norm": 8.670254707336426,
      "learning_rate": 2.8670276123443424e-05,
      "loss": 0.7093,
      "step": 1230
    },
    {
      "epoch": 0.2237863201588161,
      "grad_norm": 11.438429832458496,
      "learning_rate": 2.865944775311316e-05,
      "loss": 0.8197,
      "step": 1240
    },
    {
      "epoch": 0.22559104854719364,
      "grad_norm": 14.295610427856445,
      "learning_rate": 2.8648619382782894e-05,
      "loss": 0.7409,
      "step": 1250
    },
    {
      "epoch": 0.2273957769355712,
      "grad_norm": 9.343807220458984,
      "learning_rate": 2.8637791012452625e-05,
      "loss": 0.9273,
      "step": 1260
    },
    {
      "epoch": 0.22920050532394876,
      "grad_norm": 5.558483600616455,
      "learning_rate": 2.862696264212236e-05,
      "loss": 0.8464,
      "step": 1270
    },
    {
      "epoch": 0.23100523371232629,
      "grad_norm": 8.776065826416016,
      "learning_rate": 2.8616134271792095e-05,
      "loss": 0.8718,
      "step": 1280
    },
    {
      "epoch": 0.23280996210070384,
      "grad_norm": 8.169201850891113,
      "learning_rate": 2.8605305901461833e-05,
      "loss": 0.7899,
      "step": 1290
    },
    {
      "epoch": 0.2346146904890814,
      "grad_norm": 10.679821014404297,
      "learning_rate": 2.8594477531131565e-05,
      "loss": 0.9325,
      "step": 1300
    },
    {
      "epoch": 0.23641941887745893,
      "grad_norm": 8.884833335876465,
      "learning_rate": 2.85836491608013e-05,
      "loss": 0.8253,
      "step": 1310
    },
    {
      "epoch": 0.2382241472658365,
      "grad_norm": 10.629873275756836,
      "learning_rate": 2.8572820790471035e-05,
      "loss": 0.8101,
      "step": 1320
    },
    {
      "epoch": 0.24002887565421405,
      "grad_norm": 8.438162803649902,
      "learning_rate": 2.856199242014077e-05,
      "loss": 0.9526,
      "step": 1330
    },
    {
      "epoch": 0.24183360404259158,
      "grad_norm": 7.036508083343506,
      "learning_rate": 2.8551164049810504e-05,
      "loss": 0.8034,
      "step": 1340
    },
    {
      "epoch": 0.24363833243096913,
      "grad_norm": 10.517512321472168,
      "learning_rate": 2.854033567948024e-05,
      "loss": 0.8327,
      "step": 1350
    },
    {
      "epoch": 0.2454430608193467,
      "grad_norm": 11.35119915008545,
      "learning_rate": 2.8529507309149974e-05,
      "loss": 0.871,
      "step": 1360
    },
    {
      "epoch": 0.24724778920772425,
      "grad_norm": 6.918967247009277,
      "learning_rate": 2.851867893881971e-05,
      "loss": 0.9592,
      "step": 1370
    },
    {
      "epoch": 0.24905251759610178,
      "grad_norm": 10.113490104675293,
      "learning_rate": 2.8507850568489444e-05,
      "loss": 0.922,
      "step": 1380
    },
    {
      "epoch": 0.25085724598447934,
      "grad_norm": 9.332231521606445,
      "learning_rate": 2.8497022198159176e-05,
      "loss": 0.7386,
      "step": 1390
    },
    {
      "epoch": 0.25266197437285687,
      "grad_norm": 8.698925971984863,
      "learning_rate": 2.848619382782891e-05,
      "loss": 0.9369,
      "step": 1400
    },
    {
      "epoch": 0.25446670276123445,
      "grad_norm": 11.54453182220459,
      "learning_rate": 2.847536545749865e-05,
      "loss": 0.81,
      "step": 1410
    },
    {
      "epoch": 0.256271431149612,
      "grad_norm": 6.1561102867126465,
      "learning_rate": 2.8464537087168384e-05,
      "loss": 0.81,
      "step": 1420
    },
    {
      "epoch": 0.2580761595379895,
      "grad_norm": 5.380429267883301,
      "learning_rate": 2.845370871683812e-05,
      "loss": 0.866,
      "step": 1430
    },
    {
      "epoch": 0.2598808879263671,
      "grad_norm": 6.028582572937012,
      "learning_rate": 2.844288034650785e-05,
      "loss": 0.7675,
      "step": 1440
    },
    {
      "epoch": 0.26168561631474463,
      "grad_norm": 5.8368611335754395,
      "learning_rate": 2.8432051976177585e-05,
      "loss": 0.8798,
      "step": 1450
    },
    {
      "epoch": 0.26349034470312216,
      "grad_norm": 6.539690971374512,
      "learning_rate": 2.842122360584732e-05,
      "loss": 0.7348,
      "step": 1460
    },
    {
      "epoch": 0.26529507309149974,
      "grad_norm": 8.442996978759766,
      "learning_rate": 2.8410395235517055e-05,
      "loss": 0.8012,
      "step": 1470
    },
    {
      "epoch": 0.2670998014798773,
      "grad_norm": 8.23554515838623,
      "learning_rate": 2.839956686518679e-05,
      "loss": 0.7542,
      "step": 1480
    },
    {
      "epoch": 0.2689045298682548,
      "grad_norm": 8.134666442871094,
      "learning_rate": 2.8388738494856525e-05,
      "loss": 0.8448,
      "step": 1490
    },
    {
      "epoch": 0.2707092582566324,
      "grad_norm": 13.751035690307617,
      "learning_rate": 2.837791012452626e-05,
      "loss": 0.8442,
      "step": 1500
    },
    {
      "epoch": 0.2725139866450099,
      "grad_norm": 5.6533122062683105,
      "learning_rate": 2.8367081754195994e-05,
      "loss": 0.8655,
      "step": 1510
    },
    {
      "epoch": 0.27431871503338745,
      "grad_norm": 5.423403739929199,
      "learning_rate": 2.835625338386573e-05,
      "loss": 0.798,
      "step": 1520
    },
    {
      "epoch": 0.27612344342176504,
      "grad_norm": 6.365835666656494,
      "learning_rate": 2.834542501353546e-05,
      "loss": 0.8506,
      "step": 1530
    },
    {
      "epoch": 0.27792817181014257,
      "grad_norm": 9.505221366882324,
      "learning_rate": 2.83345966432052e-05,
      "loss": 0.8305,
      "step": 1540
    },
    {
      "epoch": 0.2797329001985201,
      "grad_norm": 12.1632719039917,
      "learning_rate": 2.8323768272874934e-05,
      "loss": 0.7657,
      "step": 1550
    },
    {
      "epoch": 0.2815376285868977,
      "grad_norm": 12.194008827209473,
      "learning_rate": 2.831293990254467e-05,
      "loss": 0.992,
      "step": 1560
    },
    {
      "epoch": 0.2833423569752752,
      "grad_norm": 9.67963695526123,
      "learning_rate": 2.83021115322144e-05,
      "loss": 0.7759,
      "step": 1570
    },
    {
      "epoch": 0.2851470853636528,
      "grad_norm": 9.976266860961914,
      "learning_rate": 2.8291283161884135e-05,
      "loss": 0.8733,
      "step": 1580
    },
    {
      "epoch": 0.2869518137520303,
      "grad_norm": 6.352810382843018,
      "learning_rate": 2.828045479155387e-05,
      "loss": 0.7866,
      "step": 1590
    },
    {
      "epoch": 0.28875654214040786,
      "grad_norm": 5.456390380859375,
      "learning_rate": 2.826962642122361e-05,
      "loss": 0.8241,
      "step": 1600
    },
    {
      "epoch": 0.29056127052878544,
      "grad_norm": 5.738785743713379,
      "learning_rate": 2.8258798050893344e-05,
      "loss": 0.9146,
      "step": 1610
    },
    {
      "epoch": 0.292365998917163,
      "grad_norm": 5.432178020477295,
      "learning_rate": 2.8247969680563075e-05,
      "loss": 0.8304,
      "step": 1620
    },
    {
      "epoch": 0.2941707273055405,
      "grad_norm": 9.96034049987793,
      "learning_rate": 2.823714131023281e-05,
      "loss": 0.7439,
      "step": 1630
    },
    {
      "epoch": 0.2959754556939181,
      "grad_norm": 5.012136936187744,
      "learning_rate": 2.8226312939902545e-05,
      "loss": 0.8754,
      "step": 1640
    },
    {
      "epoch": 0.2977801840822956,
      "grad_norm": 9.156754493713379,
      "learning_rate": 2.821548456957228e-05,
      "loss": 0.8167,
      "step": 1650
    },
    {
      "epoch": 0.29958491247067315,
      "grad_norm": 9.866537094116211,
      "learning_rate": 2.8204656199242015e-05,
      "loss": 0.7164,
      "step": 1660
    },
    {
      "epoch": 0.30138964085905073,
      "grad_norm": 6.990124225616455,
      "learning_rate": 2.819382782891175e-05,
      "loss": 0.7462,
      "step": 1670
    },
    {
      "epoch": 0.30319436924742826,
      "grad_norm": 6.196200370788574,
      "learning_rate": 2.8182999458581484e-05,
      "loss": 0.7993,
      "step": 1680
    },
    {
      "epoch": 0.3049990976358058,
      "grad_norm": 9.949402809143066,
      "learning_rate": 2.817217108825122e-05,
      "loss": 0.9926,
      "step": 1690
    },
    {
      "epoch": 0.3068038260241834,
      "grad_norm": 6.239752769470215,
      "learning_rate": 2.8161342717920954e-05,
      "loss": 0.7981,
      "step": 1700
    },
    {
      "epoch": 0.3086085544125609,
      "grad_norm": 6.0937018394470215,
      "learning_rate": 2.8150514347590686e-05,
      "loss": 0.8551,
      "step": 1710
    },
    {
      "epoch": 0.31041328280093844,
      "grad_norm": 6.160425662994385,
      "learning_rate": 2.8139685977260424e-05,
      "loss": 0.9236,
      "step": 1720
    },
    {
      "epoch": 0.312218011189316,
      "grad_norm": 6.620408058166504,
      "learning_rate": 2.812885760693016e-05,
      "loss": 0.8267,
      "step": 1730
    },
    {
      "epoch": 0.31402273957769355,
      "grad_norm": 9.059073448181152,
      "learning_rate": 2.8118029236599894e-05,
      "loss": 0.8885,
      "step": 1740
    },
    {
      "epoch": 0.3158274679660711,
      "grad_norm": 7.975284576416016,
      "learning_rate": 2.8107200866269625e-05,
      "loss": 0.8609,
      "step": 1750
    },
    {
      "epoch": 0.31763219635444867,
      "grad_norm": 11.196541786193848,
      "learning_rate": 2.809637249593936e-05,
      "loss": 0.7593,
      "step": 1760
    },
    {
      "epoch": 0.3194369247428262,
      "grad_norm": 7.857270240783691,
      "learning_rate": 2.8085544125609095e-05,
      "loss": 0.8247,
      "step": 1770
    },
    {
      "epoch": 0.32124165313120373,
      "grad_norm": 10.492291450500488,
      "learning_rate": 2.8074715755278833e-05,
      "loss": 0.7016,
      "step": 1780
    },
    {
      "epoch": 0.3230463815195813,
      "grad_norm": 7.853212833404541,
      "learning_rate": 2.8063887384948565e-05,
      "loss": 0.834,
      "step": 1790
    },
    {
      "epoch": 0.32485110990795885,
      "grad_norm": 7.814564228057861,
      "learning_rate": 2.80530590146183e-05,
      "loss": 0.8375,
      "step": 1800
    },
    {
      "epoch": 0.3266558382963364,
      "grad_norm": 5.616290092468262,
      "learning_rate": 2.8042230644288035e-05,
      "loss": 0.8288,
      "step": 1810
    },
    {
      "epoch": 0.32846056668471396,
      "grad_norm": 6.239275932312012,
      "learning_rate": 2.803140227395777e-05,
      "loss": 0.7981,
      "step": 1820
    },
    {
      "epoch": 0.3302652950730915,
      "grad_norm": 3.785482406616211,
      "learning_rate": 2.8020573903627505e-05,
      "loss": 0.7544,
      "step": 1830
    },
    {
      "epoch": 0.332070023461469,
      "grad_norm": 5.151004314422607,
      "learning_rate": 2.800974553329724e-05,
      "loss": 0.8492,
      "step": 1840
    },
    {
      "epoch": 0.3338747518498466,
      "grad_norm": 5.341537952423096,
      "learning_rate": 2.7998917162966974e-05,
      "loss": 0.7712,
      "step": 1850
    },
    {
      "epoch": 0.33567948023822414,
      "grad_norm": 9.59676456451416,
      "learning_rate": 2.798808879263671e-05,
      "loss": 0.8027,
      "step": 1860
    },
    {
      "epoch": 0.3374842086266017,
      "grad_norm": 6.514386177062988,
      "learning_rate": 2.7977260422306444e-05,
      "loss": 0.9472,
      "step": 1870
    },
    {
      "epoch": 0.33928893701497925,
      "grad_norm": 12.732770919799805,
      "learning_rate": 2.7966432051976176e-05,
      "loss": 0.7731,
      "step": 1880
    },
    {
      "epoch": 0.3410936654033568,
      "grad_norm": 10.105233192443848,
      "learning_rate": 2.795560368164591e-05,
      "loss": 0.8158,
      "step": 1890
    },
    {
      "epoch": 0.34289839379173437,
      "grad_norm": 7.76979398727417,
      "learning_rate": 2.794477531131565e-05,
      "loss": 1.0126,
      "step": 1900
    },
    {
      "epoch": 0.3447031221801119,
      "grad_norm": 9.543551445007324,
      "learning_rate": 2.7933946940985384e-05,
      "loss": 0.7832,
      "step": 1910
    },
    {
      "epoch": 0.34650785056848943,
      "grad_norm": 10.43824291229248,
      "learning_rate": 2.792311857065512e-05,
      "loss": 0.805,
      "step": 1920
    },
    {
      "epoch": 0.348312578956867,
      "grad_norm": 6.2080230712890625,
      "learning_rate": 2.791229020032485e-05,
      "loss": 0.7813,
      "step": 1930
    },
    {
      "epoch": 0.35011730734524454,
      "grad_norm": 5.748284339904785,
      "learning_rate": 2.7901461829994585e-05,
      "loss": 0.7152,
      "step": 1940
    },
    {
      "epoch": 0.3519220357336221,
      "grad_norm": 8.622621536254883,
      "learning_rate": 2.789063345966432e-05,
      "loss": 0.7386,
      "step": 1950
    },
    {
      "epoch": 0.35372676412199966,
      "grad_norm": 12.721519470214844,
      "learning_rate": 2.787980508933406e-05,
      "loss": 0.8379,
      "step": 1960
    },
    {
      "epoch": 0.3555314925103772,
      "grad_norm": 8.241501808166504,
      "learning_rate": 2.786897671900379e-05,
      "loss": 0.7847,
      "step": 1970
    },
    {
      "epoch": 0.3573362208987547,
      "grad_norm": 8.828120231628418,
      "learning_rate": 2.7858148348673525e-05,
      "loss": 0.7775,
      "step": 1980
    },
    {
      "epoch": 0.3591409492871323,
      "grad_norm": 8.770235061645508,
      "learning_rate": 2.784731997834326e-05,
      "loss": 0.9741,
      "step": 1990
    },
    {
      "epoch": 0.36094567767550984,
      "grad_norm": 6.731145858764648,
      "learning_rate": 2.7836491608012995e-05,
      "loss": 0.7723,
      "step": 2000
    },
    {
      "epoch": 0.36275040606388737,
      "grad_norm": 9.713018417358398,
      "learning_rate": 2.782566323768273e-05,
      "loss": 0.766,
      "step": 2010
    },
    {
      "epoch": 0.36455513445226495,
      "grad_norm": 8.470722198486328,
      "learning_rate": 2.7814834867352464e-05,
      "loss": 0.7894,
      "step": 2020
    },
    {
      "epoch": 0.3663598628406425,
      "grad_norm": 14.241865158081055,
      "learning_rate": 2.78040064970222e-05,
      "loss": 0.9657,
      "step": 2030
    },
    {
      "epoch": 0.36816459122902,
      "grad_norm": 8.097516059875488,
      "learning_rate": 2.7793178126691934e-05,
      "loss": 0.8733,
      "step": 2040
    },
    {
      "epoch": 0.3699693196173976,
      "grad_norm": 11.676615715026855,
      "learning_rate": 2.778234975636167e-05,
      "loss": 0.8036,
      "step": 2050
    },
    {
      "epoch": 0.3717740480057751,
      "grad_norm": 8.768407821655273,
      "learning_rate": 2.77715213860314e-05,
      "loss": 0.8196,
      "step": 2060
    },
    {
      "epoch": 0.37357877639415266,
      "grad_norm": 6.829887390136719,
      "learning_rate": 2.7760693015701136e-05,
      "loss": 0.8036,
      "step": 2070
    },
    {
      "epoch": 0.37538350478253024,
      "grad_norm": 7.286991119384766,
      "learning_rate": 2.7749864645370874e-05,
      "loss": 0.7507,
      "step": 2080
    },
    {
      "epoch": 0.37718823317090777,
      "grad_norm": 8.313789367675781,
      "learning_rate": 2.773903627504061e-05,
      "loss": 0.779,
      "step": 2090
    },
    {
      "epoch": 0.3789929615592853,
      "grad_norm": 6.942253112792969,
      "learning_rate": 2.7728207904710344e-05,
      "loss": 0.8083,
      "step": 2100
    },
    {
      "epoch": 0.3807976899476629,
      "grad_norm": 7.118246078491211,
      "learning_rate": 2.7717379534380075e-05,
      "loss": 0.9237,
      "step": 2110
    },
    {
      "epoch": 0.3826024183360404,
      "grad_norm": 10.525617599487305,
      "learning_rate": 2.770655116404981e-05,
      "loss": 0.7901,
      "step": 2120
    },
    {
      "epoch": 0.38440714672441795,
      "grad_norm": 9.3389892578125,
      "learning_rate": 2.7695722793719545e-05,
      "loss": 0.7741,
      "step": 2130
    },
    {
      "epoch": 0.38621187511279553,
      "grad_norm": 6.9699320793151855,
      "learning_rate": 2.7684894423389283e-05,
      "loss": 0.8651,
      "step": 2140
    },
    {
      "epoch": 0.38801660350117306,
      "grad_norm": 5.17404842376709,
      "learning_rate": 2.7674066053059015e-05,
      "loss": 0.7779,
      "step": 2150
    },
    {
      "epoch": 0.38982133188955065,
      "grad_norm": 10.910941123962402,
      "learning_rate": 2.766323768272875e-05,
      "loss": 0.8304,
      "step": 2160
    },
    {
      "epoch": 0.3916260602779282,
      "grad_norm": 6.22056245803833,
      "learning_rate": 2.7652409312398485e-05,
      "loss": 0.8093,
      "step": 2170
    },
    {
      "epoch": 0.3934307886663057,
      "grad_norm": 9.313888549804688,
      "learning_rate": 2.764158094206822e-05,
      "loss": 0.6748,
      "step": 2180
    },
    {
      "epoch": 0.3952355170546833,
      "grad_norm": 10.047723770141602,
      "learning_rate": 2.7630752571737954e-05,
      "loss": 0.7506,
      "step": 2190
    },
    {
      "epoch": 0.3970402454430608,
      "grad_norm": 10.805456161499023,
      "learning_rate": 2.761992420140769e-05,
      "loss": 0.8239,
      "step": 2200
    },
    {
      "epoch": 0.39884497383143835,
      "grad_norm": 9.935823440551758,
      "learning_rate": 2.7609095831077424e-05,
      "loss": 0.749,
      "step": 2210
    },
    {
      "epoch": 0.40064970221981594,
      "grad_norm": 8.837501525878906,
      "learning_rate": 2.759826746074716e-05,
      "loss": 0.8216,
      "step": 2220
    },
    {
      "epoch": 0.40245443060819347,
      "grad_norm": 8.171041488647461,
      "learning_rate": 2.7587439090416894e-05,
      "loss": 0.7428,
      "step": 2230
    },
    {
      "epoch": 0.404259158996571,
      "grad_norm": 12.610612869262695,
      "learning_rate": 2.7576610720086626e-05,
      "loss": 0.7972,
      "step": 2240
    },
    {
      "epoch": 0.4060638873849486,
      "grad_norm": 14.896540641784668,
      "learning_rate": 2.756578234975636e-05,
      "loss": 0.8061,
      "step": 2250
    },
    {
      "epoch": 0.4078686157733261,
      "grad_norm": 10.249619483947754,
      "learning_rate": 2.75549539794261e-05,
      "loss": 0.6583,
      "step": 2260
    },
    {
      "epoch": 0.40967334416170365,
      "grad_norm": 10.138212203979492,
      "learning_rate": 2.7544125609095834e-05,
      "loss": 0.8404,
      "step": 2270
    },
    {
      "epoch": 0.41147807255008123,
      "grad_norm": 7.667726516723633,
      "learning_rate": 2.7533297238765565e-05,
      "loss": 0.8769,
      "step": 2280
    },
    {
      "epoch": 0.41328280093845876,
      "grad_norm": 9.741416931152344,
      "learning_rate": 2.75224688684353e-05,
      "loss": 0.8997,
      "step": 2290
    },
    {
      "epoch": 0.4150875293268363,
      "grad_norm": 7.273045063018799,
      "learning_rate": 2.7511640498105035e-05,
      "loss": 0.8431,
      "step": 2300
    },
    {
      "epoch": 0.4168922577152139,
      "grad_norm": 7.634489059448242,
      "learning_rate": 2.750081212777477e-05,
      "loss": 0.7303,
      "step": 2310
    },
    {
      "epoch": 0.4186969861035914,
      "grad_norm": 7.511545658111572,
      "learning_rate": 2.7489983757444508e-05,
      "loss": 0.7917,
      "step": 2320
    },
    {
      "epoch": 0.42050171449196894,
      "grad_norm": 6.643487453460693,
      "learning_rate": 2.747915538711424e-05,
      "loss": 0.7174,
      "step": 2330
    },
    {
      "epoch": 0.4223064428803465,
      "grad_norm": 9.378440856933594,
      "learning_rate": 2.7468327016783975e-05,
      "loss": 0.7645,
      "step": 2340
    },
    {
      "epoch": 0.42411117126872405,
      "grad_norm": 5.53935432434082,
      "learning_rate": 2.745749864645371e-05,
      "loss": 0.7802,
      "step": 2350
    },
    {
      "epoch": 0.4259158996571016,
      "grad_norm": 7.967095375061035,
      "learning_rate": 2.7446670276123444e-05,
      "loss": 0.83,
      "step": 2360
    },
    {
      "epoch": 0.42772062804547917,
      "grad_norm": 14.265411376953125,
      "learning_rate": 2.7435841905793176e-05,
      "loss": 0.9235,
      "step": 2370
    },
    {
      "epoch": 0.4295253564338567,
      "grad_norm": 6.026987552642822,
      "learning_rate": 2.7425013535462914e-05,
      "loss": 0.7476,
      "step": 2380
    },
    {
      "epoch": 0.4313300848222342,
      "grad_norm": 7.980223655700684,
      "learning_rate": 2.741418516513265e-05,
      "loss": 0.6971,
      "step": 2390
    },
    {
      "epoch": 0.4331348132106118,
      "grad_norm": 9.775760650634766,
      "learning_rate": 2.7403356794802384e-05,
      "loss": 0.7988,
      "step": 2400
    },
    {
      "epoch": 0.43493954159898934,
      "grad_norm": 6.477345943450928,
      "learning_rate": 2.739252842447212e-05,
      "loss": 0.8532,
      "step": 2410
    },
    {
      "epoch": 0.43674426998736693,
      "grad_norm": 5.515121936798096,
      "learning_rate": 2.738170005414185e-05,
      "loss": 0.7823,
      "step": 2420
    },
    {
      "epoch": 0.43854899837574446,
      "grad_norm": 8.842146873474121,
      "learning_rate": 2.7370871683811585e-05,
      "loss": 0.741,
      "step": 2430
    },
    {
      "epoch": 0.440353726764122,
      "grad_norm": 9.816690444946289,
      "learning_rate": 2.7360043313481324e-05,
      "loss": 0.8111,
      "step": 2440
    },
    {
      "epoch": 0.4421584551524996,
      "grad_norm": 8.112114906311035,
      "learning_rate": 2.734921494315106e-05,
      "loss": 0.8206,
      "step": 2450
    },
    {
      "epoch": 0.4439631835408771,
      "grad_norm": 8.217535972595215,
      "learning_rate": 2.733838657282079e-05,
      "loss": 0.7341,
      "step": 2460
    },
    {
      "epoch": 0.44576791192925463,
      "grad_norm": 13.5017728805542,
      "learning_rate": 2.7327558202490525e-05,
      "loss": 0.6932,
      "step": 2470
    },
    {
      "epoch": 0.4475726403176322,
      "grad_norm": 14.021191596984863,
      "learning_rate": 2.731672983216026e-05,
      "loss": 0.9604,
      "step": 2480
    },
    {
      "epoch": 0.44937736870600975,
      "grad_norm": 10.743307113647461,
      "learning_rate": 2.7305901461829995e-05,
      "loss": 0.7535,
      "step": 2490
    },
    {
      "epoch": 0.4511820970943873,
      "grad_norm": 16.671123504638672,
      "learning_rate": 2.7295073091499733e-05,
      "loss": 0.8641,
      "step": 2500
    },
    {
      "epoch": 0.45298682548276487,
      "grad_norm": 7.607607364654541,
      "learning_rate": 2.7284244721169465e-05,
      "loss": 0.8615,
      "step": 2510
    },
    {
      "epoch": 0.4547915538711424,
      "grad_norm": 7.074763774871826,
      "learning_rate": 2.72734163508392e-05,
      "loss": 0.7663,
      "step": 2520
    },
    {
      "epoch": 0.4565962822595199,
      "grad_norm": 9.604735374450684,
      "learning_rate": 2.7262587980508934e-05,
      "loss": 0.6982,
      "step": 2530
    },
    {
      "epoch": 0.4584010106478975,
      "grad_norm": 7.086632251739502,
      "learning_rate": 2.725175961017867e-05,
      "loss": 0.699,
      "step": 2540
    },
    {
      "epoch": 0.46020573903627504,
      "grad_norm": 6.79312801361084,
      "learning_rate": 2.72409312398484e-05,
      "loss": 0.7504,
      "step": 2550
    },
    {
      "epoch": 0.46201046742465257,
      "grad_norm": 11.635018348693848,
      "learning_rate": 2.723010286951814e-05,
      "loss": 0.7792,
      "step": 2560
    },
    {
      "epoch": 0.46381519581303016,
      "grad_norm": 5.534504413604736,
      "learning_rate": 2.7219274499187874e-05,
      "loss": 0.8067,
      "step": 2570
    },
    {
      "epoch": 0.4656199242014077,
      "grad_norm": 8.902612686157227,
      "learning_rate": 2.720844612885761e-05,
      "loss": 0.7703,
      "step": 2580
    },
    {
      "epoch": 0.4674246525897852,
      "grad_norm": 10.317477226257324,
      "learning_rate": 2.7197617758527344e-05,
      "loss": 0.7608,
      "step": 2590
    },
    {
      "epoch": 0.4692293809781628,
      "grad_norm": 10.435001373291016,
      "learning_rate": 2.7186789388197075e-05,
      "loss": 0.9566,
      "step": 2600
    },
    {
      "epoch": 0.47103410936654033,
      "grad_norm": 7.927485942840576,
      "learning_rate": 2.717596101786681e-05,
      "loss": 0.7657,
      "step": 2610
    },
    {
      "epoch": 0.47283883775491786,
      "grad_norm": 8.657339096069336,
      "learning_rate": 2.716513264753655e-05,
      "loss": 0.7893,
      "step": 2620
    },
    {
      "epoch": 0.47464356614329545,
      "grad_norm": 12.09827995300293,
      "learning_rate": 2.7154304277206283e-05,
      "loss": 0.7742,
      "step": 2630
    },
    {
      "epoch": 0.476448294531673,
      "grad_norm": 14.928181648254395,
      "learning_rate": 2.7143475906876015e-05,
      "loss": 0.6546,
      "step": 2640
    },
    {
      "epoch": 0.4782530229200505,
      "grad_norm": 8.610074043273926,
      "learning_rate": 2.713264753654575e-05,
      "loss": 0.8209,
      "step": 2650
    },
    {
      "epoch": 0.4800577513084281,
      "grad_norm": 8.012679100036621,
      "learning_rate": 2.7121819166215485e-05,
      "loss": 0.7749,
      "step": 2660
    },
    {
      "epoch": 0.4818624796968056,
      "grad_norm": 10.697452545166016,
      "learning_rate": 2.711099079588522e-05,
      "loss": 0.8644,
      "step": 2670
    },
    {
      "epoch": 0.48366720808518315,
      "grad_norm": 7.185003280639648,
      "learning_rate": 2.7100162425554958e-05,
      "loss": 0.754,
      "step": 2680
    },
    {
      "epoch": 0.48547193647356074,
      "grad_norm": 6.952927589416504,
      "learning_rate": 2.708933405522469e-05,
      "loss": 0.7791,
      "step": 2690
    },
    {
      "epoch": 0.48727666486193827,
      "grad_norm": 9.23926067352295,
      "learning_rate": 2.7078505684894424e-05,
      "loss": 0.6897,
      "step": 2700
    },
    {
      "epoch": 0.48908139325031585,
      "grad_norm": 11.786918640136719,
      "learning_rate": 2.706767731456416e-05,
      "loss": 0.7115,
      "step": 2710
    },
    {
      "epoch": 0.4908861216386934,
      "grad_norm": 7.994476795196533,
      "learning_rate": 2.7056848944233894e-05,
      "loss": 0.7165,
      "step": 2720
    },
    {
      "epoch": 0.4926908500270709,
      "grad_norm": 8.464248657226562,
      "learning_rate": 2.7046020573903626e-05,
      "loss": 0.7047,
      "step": 2730
    },
    {
      "epoch": 0.4944955784154485,
      "grad_norm": 10.222169876098633,
      "learning_rate": 2.7035192203573364e-05,
      "loss": 0.8675,
      "step": 2740
    },
    {
      "epoch": 0.49630030680382603,
      "grad_norm": 8.1377592086792,
      "learning_rate": 2.70243638332431e-05,
      "loss": 0.7172,
      "step": 2750
    },
    {
      "epoch": 0.49810503519220356,
      "grad_norm": 6.484860897064209,
      "learning_rate": 2.7013535462912834e-05,
      "loss": 0.8155,
      "step": 2760
    },
    {
      "epoch": 0.49990976358058115,
      "grad_norm": 6.43185567855835,
      "learning_rate": 2.700270709258257e-05,
      "loss": 0.8834,
      "step": 2770
    },
    {
      "epoch": 0.5017144919689587,
      "grad_norm": 5.792159557342529,
      "learning_rate": 2.69918787222523e-05,
      "loss": 0.7143,
      "step": 2780
    },
    {
      "epoch": 0.5035192203573362,
      "grad_norm": 7.214229106903076,
      "learning_rate": 2.6981050351922035e-05,
      "loss": 0.6925,
      "step": 2790
    },
    {
      "epoch": 0.5053239487457137,
      "grad_norm": 9.320597648620605,
      "learning_rate": 2.6970221981591773e-05,
      "loss": 0.7526,
      "step": 2800
    },
    {
      "epoch": 0.5071286771340913,
      "grad_norm": 8.398590087890625,
      "learning_rate": 2.6959393611261508e-05,
      "loss": 0.8766,
      "step": 2810
    },
    {
      "epoch": 0.5089334055224689,
      "grad_norm": 8.153078079223633,
      "learning_rate": 2.694856524093124e-05,
      "loss": 0.7271,
      "step": 2820
    },
    {
      "epoch": 0.5107381339108464,
      "grad_norm": 9.759308815002441,
      "learning_rate": 2.6937736870600975e-05,
      "loss": 0.7906,
      "step": 2830
    },
    {
      "epoch": 0.512542862299224,
      "grad_norm": 5.999177932739258,
      "learning_rate": 2.692690850027071e-05,
      "loss": 0.8026,
      "step": 2840
    },
    {
      "epoch": 0.5143475906876015,
      "grad_norm": 11.437801361083984,
      "learning_rate": 2.6916080129940444e-05,
      "loss": 0.7431,
      "step": 2850
    },
    {
      "epoch": 0.516152319075979,
      "grad_norm": 6.0200653076171875,
      "learning_rate": 2.6905251759610176e-05,
      "loss": 0.8492,
      "step": 2860
    },
    {
      "epoch": 0.5179570474643567,
      "grad_norm": 8.38083267211914,
      "learning_rate": 2.6894423389279914e-05,
      "loss": 0.6776,
      "step": 2870
    },
    {
      "epoch": 0.5197617758527342,
      "grad_norm": 7.621946811676025,
      "learning_rate": 2.688359501894965e-05,
      "loss": 0.6591,
      "step": 2880
    },
    {
      "epoch": 0.5215665042411117,
      "grad_norm": 7.330513954162598,
      "learning_rate": 2.6872766648619384e-05,
      "loss": 0.7484,
      "step": 2890
    },
    {
      "epoch": 0.5233712326294893,
      "grad_norm": 9.167457580566406,
      "learning_rate": 2.686193827828912e-05,
      "loss": 0.6134,
      "step": 2900
    },
    {
      "epoch": 0.5251759610178668,
      "grad_norm": 10.415862083435059,
      "learning_rate": 2.685110990795885e-05,
      "loss": 0.8813,
      "step": 2910
    },
    {
      "epoch": 0.5269806894062443,
      "grad_norm": 7.400534152984619,
      "learning_rate": 2.6840281537628585e-05,
      "loss": 0.788,
      "step": 2920
    },
    {
      "epoch": 0.528785417794622,
      "grad_norm": 5.4141764640808105,
      "learning_rate": 2.6829453167298324e-05,
      "loss": 0.7293,
      "step": 2930
    },
    {
      "epoch": 0.5305901461829995,
      "grad_norm": 8.768172264099121,
      "learning_rate": 2.681862479696806e-05,
      "loss": 0.7756,
      "step": 2940
    },
    {
      "epoch": 0.532394874571377,
      "grad_norm": 8.333341598510742,
      "learning_rate": 2.680779642663779e-05,
      "loss": 0.7267,
      "step": 2950
    },
    {
      "epoch": 0.5341996029597545,
      "grad_norm": 5.309983253479004,
      "learning_rate": 2.6796968056307525e-05,
      "loss": 0.6581,
      "step": 2960
    },
    {
      "epoch": 0.5360043313481321,
      "grad_norm": 7.78687047958374,
      "learning_rate": 2.678613968597726e-05,
      "loss": 0.6667,
      "step": 2970
    },
    {
      "epoch": 0.5378090597365096,
      "grad_norm": 10.133614540100098,
      "learning_rate": 2.6775311315646995e-05,
      "loss": 0.6695,
      "step": 2980
    },
    {
      "epoch": 0.5396137881248873,
      "grad_norm": 11.702402114868164,
      "learning_rate": 2.6764482945316733e-05,
      "loss": 0.8045,
      "step": 2990
    },
    {
      "epoch": 0.5414185165132648,
      "grad_norm": 7.3579840660095215,
      "learning_rate": 2.6753654574986465e-05,
      "loss": 0.8487,
      "step": 3000
    },
    {
      "epoch": 0.5432232449016423,
      "grad_norm": 6.25709867477417,
      "learning_rate": 2.67428262046562e-05,
      "loss": 0.7662,
      "step": 3010
    },
    {
      "epoch": 0.5450279732900198,
      "grad_norm": 14.809127807617188,
      "learning_rate": 2.6731997834325934e-05,
      "loss": 0.7137,
      "step": 3020
    },
    {
      "epoch": 0.5468327016783974,
      "grad_norm": 8.892863273620605,
      "learning_rate": 2.672116946399567e-05,
      "loss": 0.7429,
      "step": 3030
    },
    {
      "epoch": 0.5486374300667749,
      "grad_norm": 9.077735900878906,
      "learning_rate": 2.67103410936654e-05,
      "loss": 0.7489,
      "step": 3040
    },
    {
      "epoch": 0.5504421584551525,
      "grad_norm": 8.489624977111816,
      "learning_rate": 2.669951272333514e-05,
      "loss": 0.6423,
      "step": 3050
    },
    {
      "epoch": 0.5522468868435301,
      "grad_norm": 11.811796188354492,
      "learning_rate": 2.6688684353004874e-05,
      "loss": 0.6324,
      "step": 3060
    },
    {
      "epoch": 0.5540516152319076,
      "grad_norm": 5.237399101257324,
      "learning_rate": 2.667785598267461e-05,
      "loss": 0.7525,
      "step": 3070
    },
    {
      "epoch": 0.5558563436202851,
      "grad_norm": 6.247732639312744,
      "learning_rate": 2.6667027612344344e-05,
      "loss": 0.7571,
      "step": 3080
    },
    {
      "epoch": 0.5576610720086627,
      "grad_norm": 15.442672729492188,
      "learning_rate": 2.6656199242014075e-05,
      "loss": 0.919,
      "step": 3090
    },
    {
      "epoch": 0.5594658003970402,
      "grad_norm": 7.806149959564209,
      "learning_rate": 2.664537087168381e-05,
      "loss": 0.7148,
      "step": 3100
    },
    {
      "epoch": 0.5612705287854178,
      "grad_norm": 8.789323806762695,
      "learning_rate": 2.663454250135355e-05,
      "loss": 0.7895,
      "step": 3110
    },
    {
      "epoch": 0.5630752571737954,
      "grad_norm": 17.234203338623047,
      "learning_rate": 2.6623714131023284e-05,
      "loss": 0.8315,
      "step": 3120
    },
    {
      "epoch": 0.5648799855621729,
      "grad_norm": 7.882549285888672,
      "learning_rate": 2.6612885760693015e-05,
      "loss": 0.8323,
      "step": 3130
    },
    {
      "epoch": 0.5666847139505504,
      "grad_norm": 6.176948547363281,
      "learning_rate": 2.660205739036275e-05,
      "loss": 0.8082,
      "step": 3140
    },
    {
      "epoch": 0.568489442338928,
      "grad_norm": 11.845314979553223,
      "learning_rate": 2.6591229020032485e-05,
      "loss": 0.7947,
      "step": 3150
    },
    {
      "epoch": 0.5702941707273056,
      "grad_norm": 7.437103271484375,
      "learning_rate": 2.658040064970222e-05,
      "loss": 0.717,
      "step": 3160
    },
    {
      "epoch": 0.5720988991156831,
      "grad_norm": 4.700821399688721,
      "learning_rate": 2.6569572279371958e-05,
      "loss": 0.9422,
      "step": 3170
    },
    {
      "epoch": 0.5739036275040607,
      "grad_norm": 7.3326520919799805,
      "learning_rate": 2.655874390904169e-05,
      "loss": 0.6753,
      "step": 3180
    },
    {
      "epoch": 0.5757083558924382,
      "grad_norm": 12.082571983337402,
      "learning_rate": 2.6547915538711424e-05,
      "loss": 0.7148,
      "step": 3190
    },
    {
      "epoch": 0.5775130842808157,
      "grad_norm": 8.484358787536621,
      "learning_rate": 2.653708716838116e-05,
      "loss": 0.8316,
      "step": 3200
    },
    {
      "epoch": 0.5793178126691932,
      "grad_norm": 9.15153694152832,
      "learning_rate": 2.6526258798050894e-05,
      "loss": 0.7447,
      "step": 3210
    },
    {
      "epoch": 0.5811225410575709,
      "grad_norm": 8.203887939453125,
      "learning_rate": 2.6515430427720626e-05,
      "loss": 0.8074,
      "step": 3220
    },
    {
      "epoch": 0.5829272694459484,
      "grad_norm": 9.210406303405762,
      "learning_rate": 2.6504602057390364e-05,
      "loss": 0.6659,
      "step": 3230
    },
    {
      "epoch": 0.584731997834326,
      "grad_norm": 6.786635398864746,
      "learning_rate": 2.64937736870601e-05,
      "loss": 0.7888,
      "step": 3240
    },
    {
      "epoch": 0.5865367262227035,
      "grad_norm": 9.164726257324219,
      "learning_rate": 2.6482945316729834e-05,
      "loss": 0.7393,
      "step": 3250
    },
    {
      "epoch": 0.588341454611081,
      "grad_norm": 10.166570663452148,
      "learning_rate": 2.647211694639957e-05,
      "loss": 0.9111,
      "step": 3260
    },
    {
      "epoch": 0.5901461829994585,
      "grad_norm": 9.672163963317871,
      "learning_rate": 2.64612885760693e-05,
      "loss": 0.6912,
      "step": 3270
    },
    {
      "epoch": 0.5919509113878362,
      "grad_norm": 8.587377548217773,
      "learning_rate": 2.6450460205739035e-05,
      "loss": 0.7407,
      "step": 3280
    },
    {
      "epoch": 0.5937556397762137,
      "grad_norm": 6.419749736785889,
      "learning_rate": 2.6439631835408773e-05,
      "loss": 0.6023,
      "step": 3290
    },
    {
      "epoch": 0.5955603681645912,
      "grad_norm": 12.281291961669922,
      "learning_rate": 2.642880346507851e-05,
      "loss": 0.7504,
      "step": 3300
    },
    {
      "epoch": 0.5973650965529688,
      "grad_norm": 19.967403411865234,
      "learning_rate": 2.641797509474824e-05,
      "loss": 0.9508,
      "step": 3310
    },
    {
      "epoch": 0.5991698249413463,
      "grad_norm": 6.525888919830322,
      "learning_rate": 2.6407146724417975e-05,
      "loss": 0.7899,
      "step": 3320
    },
    {
      "epoch": 0.6009745533297238,
      "grad_norm": 4.722810745239258,
      "learning_rate": 2.639631835408771e-05,
      "loss": 0.8539,
      "step": 3330
    },
    {
      "epoch": 0.6027792817181015,
      "grad_norm": 9.046305656433105,
      "learning_rate": 2.6385489983757445e-05,
      "loss": 0.7702,
      "step": 3340
    },
    {
      "epoch": 0.604584010106479,
      "grad_norm": 11.65903377532959,
      "learning_rate": 2.637466161342718e-05,
      "loss": 0.7574,
      "step": 3350
    },
    {
      "epoch": 0.6063887384948565,
      "grad_norm": 7.361898422241211,
      "learning_rate": 2.6363833243096914e-05,
      "loss": 0.8056,
      "step": 3360
    },
    {
      "epoch": 0.6081934668832341,
      "grad_norm": 7.185096263885498,
      "learning_rate": 2.635300487276665e-05,
      "loss": 0.8499,
      "step": 3370
    },
    {
      "epoch": 0.6099981952716116,
      "grad_norm": 7.204604625701904,
      "learning_rate": 2.6342176502436384e-05,
      "loss": 0.7228,
      "step": 3380
    },
    {
      "epoch": 0.6118029236599891,
      "grad_norm": 9.91758918762207,
      "learning_rate": 2.633134813210612e-05,
      "loss": 0.8806,
      "step": 3390
    },
    {
      "epoch": 0.6136076520483668,
      "grad_norm": 4.620715141296387,
      "learning_rate": 2.632051976177585e-05,
      "loss": 0.6788,
      "step": 3400
    },
    {
      "epoch": 0.6154123804367443,
      "grad_norm": 7.065613746643066,
      "learning_rate": 2.630969139144559e-05,
      "loss": 0.8138,
      "step": 3410
    },
    {
      "epoch": 0.6172171088251218,
      "grad_norm": 7.366941452026367,
      "learning_rate": 2.6298863021115324e-05,
      "loss": 0.6568,
      "step": 3420
    },
    {
      "epoch": 0.6190218372134993,
      "grad_norm": 10.12712287902832,
      "learning_rate": 2.628803465078506e-05,
      "loss": 0.8843,
      "step": 3430
    },
    {
      "epoch": 0.6208265656018769,
      "grad_norm": 9.215682029724121,
      "learning_rate": 2.627720628045479e-05,
      "loss": 0.7449,
      "step": 3440
    },
    {
      "epoch": 0.6226312939902545,
      "grad_norm": 6.949851989746094,
      "learning_rate": 2.6266377910124525e-05,
      "loss": 0.6779,
      "step": 3450
    },
    {
      "epoch": 0.624436022378632,
      "grad_norm": 4.784772872924805,
      "learning_rate": 2.625554953979426e-05,
      "loss": 0.807,
      "step": 3460
    },
    {
      "epoch": 0.6262407507670096,
      "grad_norm": 6.763701915740967,
      "learning_rate": 2.6244721169464e-05,
      "loss": 0.6854,
      "step": 3470
    },
    {
      "epoch": 0.6280454791553871,
      "grad_norm": 5.310186862945557,
      "learning_rate": 2.6233892799133733e-05,
      "loss": 0.6682,
      "step": 3480
    },
    {
      "epoch": 0.6298502075437646,
      "grad_norm": 7.705109596252441,
      "learning_rate": 2.6223064428803465e-05,
      "loss": 0.7042,
      "step": 3490
    },
    {
      "epoch": 0.6316549359321422,
      "grad_norm": 12.651671409606934,
      "learning_rate": 2.62122360584732e-05,
      "loss": 0.8038,
      "step": 3500
    },
    {
      "epoch": 0.6334596643205198,
      "grad_norm": 7.319899559020996,
      "learning_rate": 2.6201407688142935e-05,
      "loss": 0.7289,
      "step": 3510
    },
    {
      "epoch": 0.6352643927088973,
      "grad_norm": 8.265308380126953,
      "learning_rate": 2.619057931781267e-05,
      "loss": 0.91,
      "step": 3520
    },
    {
      "epoch": 0.6370691210972749,
      "grad_norm": 9.029424667358398,
      "learning_rate": 2.6179750947482404e-05,
      "loss": 0.668,
      "step": 3530
    },
    {
      "epoch": 0.6388738494856524,
      "grad_norm": 7.895513534545898,
      "learning_rate": 2.616892257715214e-05,
      "loss": 0.7299,
      "step": 3540
    },
    {
      "epoch": 0.6406785778740299,
      "grad_norm": 9.587658882141113,
      "learning_rate": 2.6158094206821874e-05,
      "loss": 0.653,
      "step": 3550
    },
    {
      "epoch": 0.6424833062624075,
      "grad_norm": 8.26709270477295,
      "learning_rate": 2.614726583649161e-05,
      "loss": 0.7815,
      "step": 3560
    },
    {
      "epoch": 0.6442880346507851,
      "grad_norm": 10.985397338867188,
      "learning_rate": 2.6136437466161344e-05,
      "loss": 0.717,
      "step": 3570
    },
    {
      "epoch": 0.6460927630391626,
      "grad_norm": 9.545870780944824,
      "learning_rate": 2.6125609095831076e-05,
      "loss": 0.8198,
      "step": 3580
    },
    {
      "epoch": 0.6478974914275402,
      "grad_norm": 7.102720260620117,
      "learning_rate": 2.6114780725500814e-05,
      "loss": 0.8337,
      "step": 3590
    },
    {
      "epoch": 0.6497022198159177,
      "grad_norm": 8.78720474243164,
      "learning_rate": 2.610395235517055e-05,
      "loss": 0.6986,
      "step": 3600
    },
    {
      "epoch": 0.6515069482042952,
      "grad_norm": 9.417670249938965,
      "learning_rate": 2.6093123984840284e-05,
      "loss": 0.7376,
      "step": 3610
    },
    {
      "epoch": 0.6533116765926728,
      "grad_norm": 14.357029914855957,
      "learning_rate": 2.6082295614510015e-05,
      "loss": 0.7535,
      "step": 3620
    },
    {
      "epoch": 0.6551164049810504,
      "grad_norm": 10.335810661315918,
      "learning_rate": 2.607146724417975e-05,
      "loss": 0.7353,
      "step": 3630
    },
    {
      "epoch": 0.6569211333694279,
      "grad_norm": 11.669583320617676,
      "learning_rate": 2.6060638873849485e-05,
      "loss": 0.8193,
      "step": 3640
    },
    {
      "epoch": 0.6587258617578055,
      "grad_norm": 12.501605987548828,
      "learning_rate": 2.6049810503519223e-05,
      "loss": 0.7424,
      "step": 3650
    },
    {
      "epoch": 0.660530590146183,
      "grad_norm": 9.0409517288208,
      "learning_rate": 2.6038982133188958e-05,
      "loss": 0.7866,
      "step": 3660
    },
    {
      "epoch": 0.6623353185345605,
      "grad_norm": 7.57557487487793,
      "learning_rate": 2.602815376285869e-05,
      "loss": 0.7392,
      "step": 3670
    },
    {
      "epoch": 0.664140046922938,
      "grad_norm": 5.7820143699646,
      "learning_rate": 2.6017325392528425e-05,
      "loss": 0.7606,
      "step": 3680
    },
    {
      "epoch": 0.6659447753113157,
      "grad_norm": 7.6327409744262695,
      "learning_rate": 2.600649702219816e-05,
      "loss": 0.7381,
      "step": 3690
    },
    {
      "epoch": 0.6677495036996932,
      "grad_norm": 9.96275520324707,
      "learning_rate": 2.5995668651867894e-05,
      "loss": 0.7431,
      "step": 3700
    },
    {
      "epoch": 0.6695542320880707,
      "grad_norm": 6.899835109710693,
      "learning_rate": 2.598484028153763e-05,
      "loss": 0.7114,
      "step": 3710
    },
    {
      "epoch": 0.6713589604764483,
      "grad_norm": 9.55350399017334,
      "learning_rate": 2.5974011911207364e-05,
      "loss": 0.7811,
      "step": 3720
    },
    {
      "epoch": 0.6731636888648258,
      "grad_norm": 10.154436111450195,
      "learning_rate": 2.59631835408771e-05,
      "loss": 0.7425,
      "step": 3730
    },
    {
      "epoch": 0.6749684172532034,
      "grad_norm": 7.095241546630859,
      "learning_rate": 2.5952355170546834e-05,
      "loss": 0.7169,
      "step": 3740
    },
    {
      "epoch": 0.676773145641581,
      "grad_norm": 6.958930492401123,
      "learning_rate": 2.594152680021657e-05,
      "loss": 0.7766,
      "step": 3750
    },
    {
      "epoch": 0.6785778740299585,
      "grad_norm": 11.148942947387695,
      "learning_rate": 2.59306984298863e-05,
      "loss": 0.51,
      "step": 3760
    },
    {
      "epoch": 0.680382602418336,
      "grad_norm": 12.46295166015625,
      "learning_rate": 2.591987005955604e-05,
      "loss": 0.7182,
      "step": 3770
    },
    {
      "epoch": 0.6821873308067136,
      "grad_norm": 15.50915813446045,
      "learning_rate": 2.59101245262588e-05,
      "loss": 0.7831,
      "step": 3780
    },
    {
      "epoch": 0.6839920591950911,
      "grad_norm": 12.020820617675781,
      "learning_rate": 2.5899296155928534e-05,
      "loss": 0.7806,
      "step": 3790
    },
    {
      "epoch": 0.6857967875834687,
      "grad_norm": 6.441421031951904,
      "learning_rate": 2.588846778559827e-05,
      "loss": 0.7718,
      "step": 3800
    },
    {
      "epoch": 0.6876015159718463,
      "grad_norm": 6.735010147094727,
      "learning_rate": 2.5877639415268003e-05,
      "loss": 0.7941,
      "step": 3810
    },
    {
      "epoch": 0.6894062443602238,
      "grad_norm": 9.282218933105469,
      "learning_rate": 2.5866811044937735e-05,
      "loss": 0.6722,
      "step": 3820
    },
    {
      "epoch": 0.6912109727486013,
      "grad_norm": 4.8206987380981445,
      "learning_rate": 2.5855982674607473e-05,
      "loss": 0.7741,
      "step": 3830
    },
    {
      "epoch": 0.6930157011369789,
      "grad_norm": 9.607990264892578,
      "learning_rate": 2.5845154304277208e-05,
      "loss": 0.6976,
      "step": 3840
    },
    {
      "epoch": 0.6948204295253564,
      "grad_norm": 4.891042232513428,
      "learning_rate": 2.5834325933946943e-05,
      "loss": 0.8809,
      "step": 3850
    },
    {
      "epoch": 0.696625157913734,
      "grad_norm": 8.165307998657227,
      "learning_rate": 2.5823497563616675e-05,
      "loss": 0.7597,
      "step": 3860
    },
    {
      "epoch": 0.6984298863021116,
      "grad_norm": 16.599119186401367,
      "learning_rate": 2.581266919328641e-05,
      "loss": 0.7507,
      "step": 3870
    },
    {
      "epoch": 0.7002346146904891,
      "grad_norm": 10.324461936950684,
      "learning_rate": 2.5801840822956144e-05,
      "loss": 0.7071,
      "step": 3880
    },
    {
      "epoch": 0.7020393430788666,
      "grad_norm": 14.6680326461792,
      "learning_rate": 2.5791012452625883e-05,
      "loss": 0.7885,
      "step": 3890
    },
    {
      "epoch": 0.7038440714672441,
      "grad_norm": 8.42296028137207,
      "learning_rate": 2.5780184082295614e-05,
      "loss": 0.7381,
      "step": 3900
    },
    {
      "epoch": 0.7056487998556217,
      "grad_norm": 9.157175064086914,
      "learning_rate": 2.576935571196535e-05,
      "loss": 0.8896,
      "step": 3910
    },
    {
      "epoch": 0.7074535282439993,
      "grad_norm": 10.519505500793457,
      "learning_rate": 2.5758527341635084e-05,
      "loss": 0.8038,
      "step": 3920
    },
    {
      "epoch": 0.7092582566323768,
      "grad_norm": 5.651674270629883,
      "learning_rate": 2.574769897130482e-05,
      "loss": 0.7156,
      "step": 3930
    },
    {
      "epoch": 0.7110629850207544,
      "grad_norm": 8.3182954788208,
      "learning_rate": 2.5736870600974554e-05,
      "loss": 0.7983,
      "step": 3940
    },
    {
      "epoch": 0.7128677134091319,
      "grad_norm": 9.298192024230957,
      "learning_rate": 2.572604223064429e-05,
      "loss": 0.7703,
      "step": 3950
    },
    {
      "epoch": 0.7146724417975094,
      "grad_norm": 9.750996589660645,
      "learning_rate": 2.5715213860314024e-05,
      "loss": 0.8039,
      "step": 3960
    },
    {
      "epoch": 0.716477170185887,
      "grad_norm": 13.686196327209473,
      "learning_rate": 2.570438548998376e-05,
      "loss": 0.7083,
      "step": 3970
    },
    {
      "epoch": 0.7182818985742646,
      "grad_norm": 13.361090660095215,
      "learning_rate": 2.5693557119653493e-05,
      "loss": 0.6923,
      "step": 3980
    },
    {
      "epoch": 0.7200866269626421,
      "grad_norm": 5.86331033706665,
      "learning_rate": 2.5682728749323225e-05,
      "loss": 0.761,
      "step": 3990
    },
    {
      "epoch": 0.7218913553510197,
      "grad_norm": 9.135453224182129,
      "learning_rate": 2.567190037899296e-05,
      "loss": 0.769,
      "step": 4000
    },
    {
      "epoch": 0.7236960837393972,
      "grad_norm": 6.863408088684082,
      "learning_rate": 2.5661072008662698e-05,
      "loss": 0.7335,
      "step": 4010
    },
    {
      "epoch": 0.7255008121277747,
      "grad_norm": 9.16828727722168,
      "learning_rate": 2.5650243638332433e-05,
      "loss": 0.6814,
      "step": 4020
    },
    {
      "epoch": 0.7273055405161524,
      "grad_norm": 11.0270414352417,
      "learning_rate": 2.5639415268002168e-05,
      "loss": 0.9109,
      "step": 4030
    },
    {
      "epoch": 0.7291102689045299,
      "grad_norm": 10.244812965393066,
      "learning_rate": 2.56285868976719e-05,
      "loss": 0.7739,
      "step": 4040
    },
    {
      "epoch": 0.7309149972929074,
      "grad_norm": 7.596117973327637,
      "learning_rate": 2.5617758527341634e-05,
      "loss": 0.7113,
      "step": 4050
    },
    {
      "epoch": 0.732719725681285,
      "grad_norm": 10.950541496276855,
      "learning_rate": 2.560693015701137e-05,
      "loss": 0.6094,
      "step": 4060
    },
    {
      "epoch": 0.7345244540696625,
      "grad_norm": 7.017909049987793,
      "learning_rate": 2.5596101786681108e-05,
      "loss": 0.749,
      "step": 4070
    },
    {
      "epoch": 0.73632918245804,
      "grad_norm": 8.31564712524414,
      "learning_rate": 2.558527341635084e-05,
      "loss": 0.7167,
      "step": 4080
    },
    {
      "epoch": 0.7381339108464177,
      "grad_norm": 4.683080673217773,
      "learning_rate": 2.5574445046020574e-05,
      "loss": 0.8083,
      "step": 4090
    },
    {
      "epoch": 0.7399386392347952,
      "grad_norm": 8.568305969238281,
      "learning_rate": 2.556361667569031e-05,
      "loss": 0.7394,
      "step": 4100
    },
    {
      "epoch": 0.7417433676231727,
      "grad_norm": 7.4840617179870605,
      "learning_rate": 2.5552788305360044e-05,
      "loss": 0.7804,
      "step": 4110
    },
    {
      "epoch": 0.7435480960115503,
      "grad_norm": 6.224974155426025,
      "learning_rate": 2.554195993502978e-05,
      "loss": 0.7531,
      "step": 4120
    },
    {
      "epoch": 0.7453528243999278,
      "grad_norm": 10.60409927368164,
      "learning_rate": 2.5531131564699514e-05,
      "loss": 0.8081,
      "step": 4130
    },
    {
      "epoch": 0.7471575527883053,
      "grad_norm": 6.251443386077881,
      "learning_rate": 2.552030319436925e-05,
      "loss": 0.6778,
      "step": 4140
    },
    {
      "epoch": 0.748962281176683,
      "grad_norm": 9.111099243164062,
      "learning_rate": 2.5509474824038983e-05,
      "loss": 0.7517,
      "step": 4150
    },
    {
      "epoch": 0.7507670095650605,
      "grad_norm": 8.942795753479004,
      "learning_rate": 2.549864645370872e-05,
      "loss": 0.9119,
      "step": 4160
    },
    {
      "epoch": 0.752571737953438,
      "grad_norm": 13.207716941833496,
      "learning_rate": 2.548781808337845e-05,
      "loss": 0.7653,
      "step": 4170
    },
    {
      "epoch": 0.7543764663418155,
      "grad_norm": 7.475887775421143,
      "learning_rate": 2.5476989713048185e-05,
      "loss": 0.8034,
      "step": 4180
    },
    {
      "epoch": 0.7561811947301931,
      "grad_norm": 4.987125396728516,
      "learning_rate": 2.5466161342717923e-05,
      "loss": 0.8244,
      "step": 4190
    },
    {
      "epoch": 0.7579859231185706,
      "grad_norm": 10.364448547363281,
      "learning_rate": 2.5455332972387658e-05,
      "loss": 0.726,
      "step": 4200
    },
    {
      "epoch": 0.7597906515069482,
      "grad_norm": 9.17105770111084,
      "learning_rate": 2.5444504602057393e-05,
      "loss": 0.6836,
      "step": 4210
    },
    {
      "epoch": 0.7615953798953258,
      "grad_norm": 8.288670539855957,
      "learning_rate": 2.5433676231727124e-05,
      "loss": 0.7557,
      "step": 4220
    },
    {
      "epoch": 0.7634001082837033,
      "grad_norm": 10.071915626525879,
      "learning_rate": 2.542284786139686e-05,
      "loss": 0.6275,
      "step": 4230
    },
    {
      "epoch": 0.7652048366720808,
      "grad_norm": 8.596997261047363,
      "learning_rate": 2.5412019491066594e-05,
      "loss": 0.7678,
      "step": 4240
    },
    {
      "epoch": 0.7670095650604584,
      "grad_norm": 9.65373420715332,
      "learning_rate": 2.5401191120736332e-05,
      "loss": 0.7891,
      "step": 4250
    },
    {
      "epoch": 0.7688142934488359,
      "grad_norm": 18.412166595458984,
      "learning_rate": 2.5390362750406064e-05,
      "loss": 0.6877,
      "step": 4260
    },
    {
      "epoch": 0.7706190218372135,
      "grad_norm": 6.257960319519043,
      "learning_rate": 2.53795343800758e-05,
      "loss": 0.6687,
      "step": 4270
    },
    {
      "epoch": 0.7724237502255911,
      "grad_norm": 8.344488143920898,
      "learning_rate": 2.5368706009745534e-05,
      "loss": 0.8517,
      "step": 4280
    },
    {
      "epoch": 0.7742284786139686,
      "grad_norm": 12.963759422302246,
      "learning_rate": 2.535787763941527e-05,
      "loss": 0.7149,
      "step": 4290
    },
    {
      "epoch": 0.7760332070023461,
      "grad_norm": 17.382625579833984,
      "learning_rate": 2.5347049269085004e-05,
      "loss": 0.7852,
      "step": 4300
    },
    {
      "epoch": 0.7778379353907237,
      "grad_norm": 11.113003730773926,
      "learning_rate": 2.533622089875474e-05,
      "loss": 0.697,
      "step": 4310
    },
    {
      "epoch": 0.7796426637791013,
      "grad_norm": 6.098171710968018,
      "learning_rate": 2.5325392528424473e-05,
      "loss": 0.6709,
      "step": 4320
    },
    {
      "epoch": 0.7814473921674788,
      "grad_norm": 8.25799560546875,
      "learning_rate": 2.531456415809421e-05,
      "loss": 0.7696,
      "step": 4330
    },
    {
      "epoch": 0.7832521205558564,
      "grad_norm": 7.266390800476074,
      "learning_rate": 2.5303735787763943e-05,
      "loss": 0.7938,
      "step": 4340
    },
    {
      "epoch": 0.7850568489442339,
      "grad_norm": 14.5884428024292,
      "learning_rate": 2.5292907417433675e-05,
      "loss": 0.7006,
      "step": 4350
    },
    {
      "epoch": 0.7868615773326114,
      "grad_norm": 14.463285446166992,
      "learning_rate": 2.528207904710341e-05,
      "loss": 0.7188,
      "step": 4360
    },
    {
      "epoch": 0.788666305720989,
      "grad_norm": 13.041033744812012,
      "learning_rate": 2.5271250676773148e-05,
      "loss": 0.8106,
      "step": 4370
    },
    {
      "epoch": 0.7904710341093666,
      "grad_norm": 12.61405086517334,
      "learning_rate": 2.5260422306442883e-05,
      "loss": 0.7048,
      "step": 4380
    },
    {
      "epoch": 0.7922757624977441,
      "grad_norm": 6.520750999450684,
      "learning_rate": 2.5249593936112614e-05,
      "loss": 0.8534,
      "step": 4390
    },
    {
      "epoch": 0.7940804908861216,
      "grad_norm": 4.7418622970581055,
      "learning_rate": 2.523876556578235e-05,
      "loss": 0.7433,
      "step": 4400
    },
    {
      "epoch": 0.7958852192744992,
      "grad_norm": 5.121192932128906,
      "learning_rate": 2.5227937195452084e-05,
      "loss": 0.6713,
      "step": 4410
    },
    {
      "epoch": 0.7976899476628767,
      "grad_norm": 9.748208045959473,
      "learning_rate": 2.521710882512182e-05,
      "loss": 0.759,
      "step": 4420
    },
    {
      "epoch": 0.7994946760512542,
      "grad_norm": 10.289338111877441,
      "learning_rate": 2.5206280454791557e-05,
      "loss": 0.7946,
      "step": 4430
    },
    {
      "epoch": 0.8012994044396319,
      "grad_norm": 9.47620964050293,
      "learning_rate": 2.519545208446129e-05,
      "loss": 0.742,
      "step": 4440
    },
    {
      "epoch": 0.8031041328280094,
      "grad_norm": 10.020523071289062,
      "learning_rate": 2.5184623714131024e-05,
      "loss": 0.7013,
      "step": 4450
    },
    {
      "epoch": 0.8049088612163869,
      "grad_norm": 8.82628059387207,
      "learning_rate": 2.517379534380076e-05,
      "loss": 0.7762,
      "step": 4460
    },
    {
      "epoch": 0.8067135896047645,
      "grad_norm": 7.974745750427246,
      "learning_rate": 2.5162966973470494e-05,
      "loss": 0.7,
      "step": 4470
    },
    {
      "epoch": 0.808518317993142,
      "grad_norm": 15.294554710388184,
      "learning_rate": 2.5152138603140225e-05,
      "loss": 0.7217,
      "step": 4480
    },
    {
      "epoch": 0.8103230463815195,
      "grad_norm": 11.471675872802734,
      "learning_rate": 2.5141310232809963e-05,
      "loss": 0.7951,
      "step": 4490
    },
    {
      "epoch": 0.8121277747698972,
      "grad_norm": 8.91431713104248,
      "learning_rate": 2.51304818624797e-05,
      "loss": 0.6927,
      "step": 4500
    },
    {
      "epoch": 0.8139325031582747,
      "grad_norm": 7.6869587898254395,
      "learning_rate": 2.5119653492149433e-05,
      "loss": 0.806,
      "step": 4510
    },
    {
      "epoch": 0.8157372315466522,
      "grad_norm": 9.140411376953125,
      "learning_rate": 2.5108825121819168e-05,
      "loss": 0.7496,
      "step": 4520
    },
    {
      "epoch": 0.8175419599350298,
      "grad_norm": 7.161017894744873,
      "learning_rate": 2.50979967514889e-05,
      "loss": 0.7391,
      "step": 4530
    },
    {
      "epoch": 0.8193466883234073,
      "grad_norm": 12.590459823608398,
      "learning_rate": 2.5087168381158635e-05,
      "loss": 0.6902,
      "step": 4540
    },
    {
      "epoch": 0.8211514167117849,
      "grad_norm": 9.15931510925293,
      "learning_rate": 2.5076340010828373e-05,
      "loss": 0.8513,
      "step": 4550
    },
    {
      "epoch": 0.8229561451001625,
      "grad_norm": 8.233531951904297,
      "learning_rate": 2.5065511640498108e-05,
      "loss": 0.6987,
      "step": 4560
    },
    {
      "epoch": 0.82476087348854,
      "grad_norm": 10.76944351196289,
      "learning_rate": 2.505468327016784e-05,
      "loss": 0.7033,
      "step": 4570
    },
    {
      "epoch": 0.8265656018769175,
      "grad_norm": 9.578181266784668,
      "learning_rate": 2.5043854899837574e-05,
      "loss": 0.8117,
      "step": 4580
    },
    {
      "epoch": 0.828370330265295,
      "grad_norm": 8.52833366394043,
      "learning_rate": 2.503302652950731e-05,
      "loss": 0.798,
      "step": 4590
    },
    {
      "epoch": 0.8301750586536726,
      "grad_norm": 12.781330108642578,
      "learning_rate": 2.5022198159177044e-05,
      "loss": 0.7479,
      "step": 4600
    },
    {
      "epoch": 0.8319797870420502,
      "grad_norm": 13.078160285949707,
      "learning_rate": 2.5011369788846782e-05,
      "loss": 0.8283,
      "step": 4610
    },
    {
      "epoch": 0.8337845154304278,
      "grad_norm": 4.645646095275879,
      "learning_rate": 2.5000541418516514e-05,
      "loss": 0.769,
      "step": 4620
    },
    {
      "epoch": 0.8355892438188053,
      "grad_norm": 10.365513801574707,
      "learning_rate": 2.498971304818625e-05,
      "loss": 0.787,
      "step": 4630
    },
    {
      "epoch": 0.8373939722071828,
      "grad_norm": 8.254326820373535,
      "learning_rate": 2.4978884677855984e-05,
      "loss": 0.766,
      "step": 4640
    },
    {
      "epoch": 0.8391987005955603,
      "grad_norm": 5.442291736602783,
      "learning_rate": 2.496805630752572e-05,
      "loss": 0.7658,
      "step": 4650
    },
    {
      "epoch": 0.8410034289839379,
      "grad_norm": 6.922250270843506,
      "learning_rate": 2.495722793719545e-05,
      "loss": 0.774,
      "step": 4660
    },
    {
      "epoch": 0.8428081573723155,
      "grad_norm": 8.73119068145752,
      "learning_rate": 2.4946399566865188e-05,
      "loss": 0.6596,
      "step": 4670
    },
    {
      "epoch": 0.844612885760693,
      "grad_norm": 10.535835266113281,
      "learning_rate": 2.4935571196534923e-05,
      "loss": 0.779,
      "step": 4680
    },
    {
      "epoch": 0.8464176141490706,
      "grad_norm": 4.341630458831787,
      "learning_rate": 2.4924742826204658e-05,
      "loss": 0.7573,
      "step": 4690
    },
    {
      "epoch": 0.8482223425374481,
      "grad_norm": 8.347651481628418,
      "learning_rate": 2.4913914455874393e-05,
      "loss": 0.767,
      "step": 4700
    },
    {
      "epoch": 0.8500270709258256,
      "grad_norm": 5.250551223754883,
      "learning_rate": 2.4903086085544125e-05,
      "loss": 0.718,
      "step": 4710
    },
    {
      "epoch": 0.8518317993142032,
      "grad_norm": 9.24071216583252,
      "learning_rate": 2.489225771521386e-05,
      "loss": 0.7391,
      "step": 4720
    },
    {
      "epoch": 0.8536365277025808,
      "grad_norm": 7.9449849128723145,
      "learning_rate": 2.4881429344883598e-05,
      "loss": 0.6168,
      "step": 4730
    },
    {
      "epoch": 0.8554412560909583,
      "grad_norm": 6.04209041595459,
      "learning_rate": 2.4870600974553333e-05,
      "loss": 0.7558,
      "step": 4740
    },
    {
      "epoch": 0.8572459844793359,
      "grad_norm": 4.886850357055664,
      "learning_rate": 2.4859772604223064e-05,
      "loss": 0.6431,
      "step": 4750
    },
    {
      "epoch": 0.8590507128677134,
      "grad_norm": 9.251333236694336,
      "learning_rate": 2.48489442338928e-05,
      "loss": 0.7261,
      "step": 4760
    },
    {
      "epoch": 0.8608554412560909,
      "grad_norm": 12.565115928649902,
      "learning_rate": 2.4838115863562534e-05,
      "loss": 0.7838,
      "step": 4770
    },
    {
      "epoch": 0.8626601696444685,
      "grad_norm": 14.484676361083984,
      "learning_rate": 2.482728749323227e-05,
      "loss": 0.7417,
      "step": 4780
    },
    {
      "epoch": 0.8644648980328461,
      "grad_norm": 12.008367538452148,
      "learning_rate": 2.4816459122902007e-05,
      "loss": 0.7039,
      "step": 4790
    },
    {
      "epoch": 0.8662696264212236,
      "grad_norm": 9.922196388244629,
      "learning_rate": 2.480563075257174e-05,
      "loss": 0.7895,
      "step": 4800
    },
    {
      "epoch": 0.8680743548096012,
      "grad_norm": 6.5982136726379395,
      "learning_rate": 2.4794802382241474e-05,
      "loss": 0.7967,
      "step": 4810
    },
    {
      "epoch": 0.8698790831979787,
      "grad_norm": 5.910696983337402,
      "learning_rate": 2.478397401191121e-05,
      "loss": 0.7314,
      "step": 4820
    },
    {
      "epoch": 0.8716838115863562,
      "grad_norm": 5.789559841156006,
      "learning_rate": 2.4773145641580943e-05,
      "loss": 0.6567,
      "step": 4830
    },
    {
      "epoch": 0.8734885399747339,
      "grad_norm": 8.612029075622559,
      "learning_rate": 2.4762317271250675e-05,
      "loss": 0.7299,
      "step": 4840
    },
    {
      "epoch": 0.8752932683631114,
      "grad_norm": 12.890583992004395,
      "learning_rate": 2.4751488900920413e-05,
      "loss": 0.8363,
      "step": 4850
    },
    {
      "epoch": 0.8770979967514889,
      "grad_norm": 7.97794771194458,
      "learning_rate": 2.4740660530590148e-05,
      "loss": 0.7347,
      "step": 4860
    },
    {
      "epoch": 0.8789027251398664,
      "grad_norm": 6.446364879608154,
      "learning_rate": 2.4729832160259883e-05,
      "loss": 0.7268,
      "step": 4870
    },
    {
      "epoch": 0.880707453528244,
      "grad_norm": 6.330154895782471,
      "learning_rate": 2.4719003789929615e-05,
      "loss": 0.6723,
      "step": 4880
    },
    {
      "epoch": 0.8825121819166215,
      "grad_norm": 6.439594268798828,
      "learning_rate": 2.470817541959935e-05,
      "loss": 0.6961,
      "step": 4890
    },
    {
      "epoch": 0.8843169103049991,
      "grad_norm": 9.2645902633667,
      "learning_rate": 2.4697347049269084e-05,
      "loss": 0.83,
      "step": 4900
    },
    {
      "epoch": 0.8861216386933767,
      "grad_norm": 5.624060153961182,
      "learning_rate": 2.4686518678938823e-05,
      "loss": 0.7896,
      "step": 4910
    },
    {
      "epoch": 0.8879263670817542,
      "grad_norm": 12.431868553161621,
      "learning_rate": 2.4675690308608558e-05,
      "loss": 0.7675,
      "step": 4920
    },
    {
      "epoch": 0.8897310954701317,
      "grad_norm": 9.345364570617676,
      "learning_rate": 2.466486193827829e-05,
      "loss": 0.6949,
      "step": 4930
    },
    {
      "epoch": 0.8915358238585093,
      "grad_norm": 7.995479583740234,
      "learning_rate": 2.4654033567948024e-05,
      "loss": 0.7692,
      "step": 4940
    },
    {
      "epoch": 0.8933405522468868,
      "grad_norm": 5.911910533905029,
      "learning_rate": 2.464320519761776e-05,
      "loss": 0.7359,
      "step": 4950
    },
    {
      "epoch": 0.8951452806352644,
      "grad_norm": 7.065942287445068,
      "learning_rate": 2.4632376827287494e-05,
      "loss": 0.7948,
      "step": 4960
    },
    {
      "epoch": 0.896950009023642,
      "grad_norm": 9.539867401123047,
      "learning_rate": 2.462154845695723e-05,
      "loss": 0.737,
      "step": 4970
    },
    {
      "epoch": 0.8987547374120195,
      "grad_norm": 6.89813756942749,
      "learning_rate": 2.4610720086626964e-05,
      "loss": 0.6864,
      "step": 4980
    },
    {
      "epoch": 0.900559465800397,
      "grad_norm": 8.022411346435547,
      "learning_rate": 2.45998917162967e-05,
      "loss": 0.6946,
      "step": 4990
    },
    {
      "epoch": 0.9023641941887746,
      "grad_norm": 9.872493743896484,
      "learning_rate": 2.4589063345966433e-05,
      "loss": 0.6794,
      "step": 5000
    },
    {
      "epoch": 0.9041689225771521,
      "grad_norm": 10.012012481689453,
      "learning_rate": 2.4578234975636168e-05,
      "loss": 0.7285,
      "step": 5010
    },
    {
      "epoch": 0.9059736509655297,
      "grad_norm": 9.046306610107422,
      "learning_rate": 2.45674066053059e-05,
      "loss": 0.8023,
      "step": 5020
    },
    {
      "epoch": 0.9077783793539073,
      "grad_norm": 4.39716911315918,
      "learning_rate": 2.4556578234975635e-05,
      "loss": 0.7212,
      "step": 5030
    },
    {
      "epoch": 0.9095831077422848,
      "grad_norm": 10.226670265197754,
      "learning_rate": 2.4545749864645373e-05,
      "loss": 0.6452,
      "step": 5040
    },
    {
      "epoch": 0.9113878361306623,
      "grad_norm": 4.474891185760498,
      "learning_rate": 2.4534921494315108e-05,
      "loss": 0.7542,
      "step": 5050
    },
    {
      "epoch": 0.9131925645190399,
      "grad_norm": 8.925117492675781,
      "learning_rate": 2.452409312398484e-05,
      "loss": 0.703,
      "step": 5060
    },
    {
      "epoch": 0.9149972929074174,
      "grad_norm": 12.647566795349121,
      "learning_rate": 2.4513264753654574e-05,
      "loss": 0.6264,
      "step": 5070
    },
    {
      "epoch": 0.916802021295795,
      "grad_norm": 5.542916774749756,
      "learning_rate": 2.450243638332431e-05,
      "loss": 0.6304,
      "step": 5080
    },
    {
      "epoch": 0.9186067496841726,
      "grad_norm": 9.009048461914062,
      "learning_rate": 2.4491608012994044e-05,
      "loss": 0.8236,
      "step": 5090
    },
    {
      "epoch": 0.9204114780725501,
      "grad_norm": 8.946503639221191,
      "learning_rate": 2.4480779642663782e-05,
      "loss": 0.7859,
      "step": 5100
    },
    {
      "epoch": 0.9222162064609276,
      "grad_norm": 13.77550220489502,
      "learning_rate": 2.4469951272333514e-05,
      "loss": 0.6844,
      "step": 5110
    },
    {
      "epoch": 0.9240209348493051,
      "grad_norm": 15.655166625976562,
      "learning_rate": 2.445912290200325e-05,
      "loss": 0.5564,
      "step": 5120
    },
    {
      "epoch": 0.9258256632376828,
      "grad_norm": 9.166245460510254,
      "learning_rate": 2.4448294531672984e-05,
      "loss": 0.7543,
      "step": 5130
    },
    {
      "epoch": 0.9276303916260603,
      "grad_norm": 13.108254432678223,
      "learning_rate": 2.443746616134272e-05,
      "loss": 0.7097,
      "step": 5140
    },
    {
      "epoch": 0.9294351200144378,
      "grad_norm": 6.207575798034668,
      "learning_rate": 2.442663779101245e-05,
      "loss": 0.7118,
      "step": 5150
    },
    {
      "epoch": 0.9312398484028154,
      "grad_norm": 6.212305068969727,
      "learning_rate": 2.441580942068219e-05,
      "loss": 0.59,
      "step": 5160
    },
    {
      "epoch": 0.9330445767911929,
      "grad_norm": 8.163171768188477,
      "learning_rate": 2.4404981050351923e-05,
      "loss": 0.5722,
      "step": 5170
    },
    {
      "epoch": 0.9348493051795704,
      "grad_norm": 14.844874382019043,
      "learning_rate": 2.4394152680021658e-05,
      "loss": 0.7408,
      "step": 5180
    },
    {
      "epoch": 0.9366540335679481,
      "grad_norm": 10.883769035339355,
      "learning_rate": 2.4383324309691393e-05,
      "loss": 0.7777,
      "step": 5190
    },
    {
      "epoch": 0.9384587619563256,
      "grad_norm": 8.173059463500977,
      "learning_rate": 2.4372495939361125e-05,
      "loss": 0.6985,
      "step": 5200
    },
    {
      "epoch": 0.9402634903447031,
      "grad_norm": 13.117321014404297,
      "learning_rate": 2.436166756903086e-05,
      "loss": 0.7444,
      "step": 5210
    },
    {
      "epoch": 0.9420682187330807,
      "grad_norm": 7.4876017570495605,
      "learning_rate": 2.4350839198700598e-05,
      "loss": 0.6711,
      "step": 5220
    },
    {
      "epoch": 0.9438729471214582,
      "grad_norm": 4.1956963539123535,
      "learning_rate": 2.4340010828370333e-05,
      "loss": 0.6688,
      "step": 5230
    },
    {
      "epoch": 0.9456776755098357,
      "grad_norm": 9.180681228637695,
      "learning_rate": 2.4329182458040064e-05,
      "loss": 0.757,
      "step": 5240
    },
    {
      "epoch": 0.9474824038982134,
      "grad_norm": 6.562953472137451,
      "learning_rate": 2.43183540877098e-05,
      "loss": 0.6744,
      "step": 5250
    },
    {
      "epoch": 0.9492871322865909,
      "grad_norm": 11.570395469665527,
      "learning_rate": 2.4307525717379534e-05,
      "loss": 0.8688,
      "step": 5260
    },
    {
      "epoch": 0.9510918606749684,
      "grad_norm": 7.071556568145752,
      "learning_rate": 2.429669734704927e-05,
      "loss": 0.8025,
      "step": 5270
    },
    {
      "epoch": 0.952896589063346,
      "grad_norm": 7.306174278259277,
      "learning_rate": 2.4285868976719007e-05,
      "loss": 0.77,
      "step": 5280
    },
    {
      "epoch": 0.9547013174517235,
      "grad_norm": 10.966747283935547,
      "learning_rate": 2.427504060638874e-05,
      "loss": 0.8069,
      "step": 5290
    },
    {
      "epoch": 0.956506045840101,
      "grad_norm": 9.465357780456543,
      "learning_rate": 2.4264212236058474e-05,
      "loss": 0.7867,
      "step": 5300
    },
    {
      "epoch": 0.9583107742284787,
      "grad_norm": 6.356661796569824,
      "learning_rate": 2.425338386572821e-05,
      "loss": 0.7316,
      "step": 5310
    },
    {
      "epoch": 0.9601155026168562,
      "grad_norm": 7.05842924118042,
      "learning_rate": 2.4242555495397944e-05,
      "loss": 0.7828,
      "step": 5320
    },
    {
      "epoch": 0.9619202310052337,
      "grad_norm": 9.469194412231445,
      "learning_rate": 2.4231727125067675e-05,
      "loss": 0.7912,
      "step": 5330
    },
    {
      "epoch": 0.9637249593936112,
      "grad_norm": 6.830784797668457,
      "learning_rate": 2.4220898754737413e-05,
      "loss": 0.6798,
      "step": 5340
    },
    {
      "epoch": 0.9655296877819888,
      "grad_norm": 11.4749174118042,
      "learning_rate": 2.4210070384407148e-05,
      "loss": 0.6541,
      "step": 5350
    },
    {
      "epoch": 0.9673344161703663,
      "grad_norm": 6.463920593261719,
      "learning_rate": 2.4199242014076883e-05,
      "loss": 0.7807,
      "step": 5360
    },
    {
      "epoch": 0.969139144558744,
      "grad_norm": 13.686917304992676,
      "learning_rate": 2.4188413643746618e-05,
      "loss": 0.7349,
      "step": 5370
    },
    {
      "epoch": 0.9709438729471215,
      "grad_norm": 11.958252906799316,
      "learning_rate": 2.417758527341635e-05,
      "loss": 0.7248,
      "step": 5380
    },
    {
      "epoch": 0.972748601335499,
      "grad_norm": 9.618008613586426,
      "learning_rate": 2.4166756903086084e-05,
      "loss": 0.714,
      "step": 5390
    },
    {
      "epoch": 0.9745533297238765,
      "grad_norm": 17.344196319580078,
      "learning_rate": 2.4155928532755823e-05,
      "loss": 0.6283,
      "step": 5400
    },
    {
      "epoch": 0.9763580581122541,
      "grad_norm": 5.630173683166504,
      "learning_rate": 2.4145100162425558e-05,
      "loss": 0.7932,
      "step": 5410
    },
    {
      "epoch": 0.9781627865006317,
      "grad_norm": 8.346635818481445,
      "learning_rate": 2.413427179209529e-05,
      "loss": 0.7887,
      "step": 5420
    },
    {
      "epoch": 0.9799675148890092,
      "grad_norm": 15.4551362991333,
      "learning_rate": 2.4123443421765024e-05,
      "loss": 0.6738,
      "step": 5430
    },
    {
      "epoch": 0.9817722432773868,
      "grad_norm": 9.622000694274902,
      "learning_rate": 2.411261505143476e-05,
      "loss": 0.6646,
      "step": 5440
    },
    {
      "epoch": 0.9835769716657643,
      "grad_norm": 8.819452285766602,
      "learning_rate": 2.4101786681104494e-05,
      "loss": 0.7307,
      "step": 5450
    },
    {
      "epoch": 0.9853817000541418,
      "grad_norm": 7.146517276763916,
      "learning_rate": 2.409095831077423e-05,
      "loss": 0.684,
      "step": 5460
    },
    {
      "epoch": 0.9871864284425194,
      "grad_norm": 6.560965061187744,
      "learning_rate": 2.4080129940443964e-05,
      "loss": 0.773,
      "step": 5470
    },
    {
      "epoch": 0.988991156830897,
      "grad_norm": 8.660265922546387,
      "learning_rate": 2.40693015701137e-05,
      "loss": 0.6445,
      "step": 5480
    },
    {
      "epoch": 0.9907958852192745,
      "grad_norm": 12.145088195800781,
      "learning_rate": 2.4058473199783433e-05,
      "loss": 0.7005,
      "step": 5490
    },
    {
      "epoch": 0.9926006136076521,
      "grad_norm": 4.650135040283203,
      "learning_rate": 2.404764482945317e-05,
      "loss": 0.6526,
      "step": 5500
    },
    {
      "epoch": 0.9944053419960296,
      "grad_norm": 10.205984115600586,
      "learning_rate": 2.40368164591229e-05,
      "loss": 0.6856,
      "step": 5510
    },
    {
      "epoch": 0.9962100703844071,
      "grad_norm": 6.18156623840332,
      "learning_rate": 2.4025988088792638e-05,
      "loss": 0.7826,
      "step": 5520
    },
    {
      "epoch": 0.9980147987727847,
      "grad_norm": 5.964878082275391,
      "learning_rate": 2.4015159718462373e-05,
      "loss": 0.6563,
      "step": 5530
    },
    {
      "epoch": 0.9998195271611623,
      "grad_norm": 10.99072265625,
      "learning_rate": 2.4004331348132108e-05,
      "loss": 0.6938,
      "step": 5540
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7190813518025538,
      "eval_f1": 0.6670921245334966,
      "eval_loss": 0.6901546716690063,
      "eval_precision": 0.6728443196196304,
      "eval_recall": 0.6954711465563871,
      "eval_runtime": 160.4294,
      "eval_samples_per_second": 138.148,
      "eval_steps_per_second": 8.639,
      "step": 5541
    },
    {
      "epoch": 1.0016242555495398,
      "grad_norm": 7.4997878074646,
      "learning_rate": 2.399350297780184e-05,
      "loss": 0.6688,
      "step": 5550
    },
    {
      "epoch": 1.0034289839379174,
      "grad_norm": 8.51596736907959,
      "learning_rate": 2.3982674607471574e-05,
      "loss": 0.6359,
      "step": 5560
    },
    {
      "epoch": 1.0052337123262949,
      "grad_norm": 10.917862892150879,
      "learning_rate": 2.397184623714131e-05,
      "loss": 0.6433,
      "step": 5570
    },
    {
      "epoch": 1.0070384407146724,
      "grad_norm": 6.516542911529541,
      "learning_rate": 2.3961017866811048e-05,
      "loss": 0.6788,
      "step": 5580
    },
    {
      "epoch": 1.00884316910305,
      "grad_norm": 5.942925930023193,
      "learning_rate": 2.3950189496480783e-05,
      "loss": 0.5686,
      "step": 5590
    },
    {
      "epoch": 1.0106478974914275,
      "grad_norm": 18.208707809448242,
      "learning_rate": 2.3939361126150514e-05,
      "loss": 0.5595,
      "step": 5600
    },
    {
      "epoch": 1.012452625879805,
      "grad_norm": 12.586977005004883,
      "learning_rate": 2.392853275582025e-05,
      "loss": 0.6721,
      "step": 5610
    },
    {
      "epoch": 1.0142573542681825,
      "grad_norm": 10.698039054870605,
      "learning_rate": 2.3917704385489984e-05,
      "loss": 0.689,
      "step": 5620
    },
    {
      "epoch": 1.0160620826565603,
      "grad_norm": 17.194726943969727,
      "learning_rate": 2.390687601515972e-05,
      "loss": 0.6572,
      "step": 5630
    },
    {
      "epoch": 1.0178668110449378,
      "grad_norm": 9.405759811401367,
      "learning_rate": 2.3896047644829454e-05,
      "loss": 0.5795,
      "step": 5640
    },
    {
      "epoch": 1.0196715394333153,
      "grad_norm": 10.743915557861328,
      "learning_rate": 2.388521927449919e-05,
      "loss": 0.5708,
      "step": 5650
    },
    {
      "epoch": 1.0214762678216929,
      "grad_norm": 14.30970287322998,
      "learning_rate": 2.3874390904168923e-05,
      "loss": 0.604,
      "step": 5660
    },
    {
      "epoch": 1.0232809962100704,
      "grad_norm": 14.581184387207031,
      "learning_rate": 2.386356253383866e-05,
      "loss": 0.6685,
      "step": 5670
    },
    {
      "epoch": 1.025085724598448,
      "grad_norm": 6.375946044921875,
      "learning_rate": 2.3852734163508393e-05,
      "loss": 0.7098,
      "step": 5680
    },
    {
      "epoch": 1.0268904529868255,
      "grad_norm": 6.963015079498291,
      "learning_rate": 2.3841905793178125e-05,
      "loss": 0.62,
      "step": 5690
    },
    {
      "epoch": 1.028695181375203,
      "grad_norm": 8.055559158325195,
      "learning_rate": 2.3831077422847863e-05,
      "loss": 0.6935,
      "step": 5700
    },
    {
      "epoch": 1.0304999097635805,
      "grad_norm": 9.513371467590332,
      "learning_rate": 2.3820249052517598e-05,
      "loss": 0.6415,
      "step": 5710
    },
    {
      "epoch": 1.032304638151958,
      "grad_norm": 8.542570114135742,
      "learning_rate": 2.3809420682187333e-05,
      "loss": 0.5866,
      "step": 5720
    },
    {
      "epoch": 1.0341093665403356,
      "grad_norm": 9.95473575592041,
      "learning_rate": 2.3798592311857064e-05,
      "loss": 0.7217,
      "step": 5730
    },
    {
      "epoch": 1.0359140949287133,
      "grad_norm": 7.785891056060791,
      "learning_rate": 2.37877639415268e-05,
      "loss": 0.5531,
      "step": 5740
    },
    {
      "epoch": 1.0377188233170909,
      "grad_norm": 5.856265544891357,
      "learning_rate": 2.3776935571196534e-05,
      "loss": 0.6119,
      "step": 5750
    },
    {
      "epoch": 1.0395235517054684,
      "grad_norm": 12.188841819763184,
      "learning_rate": 2.3766107200866273e-05,
      "loss": 0.6169,
      "step": 5760
    },
    {
      "epoch": 1.041328280093846,
      "grad_norm": 5.729201793670654,
      "learning_rate": 2.3755278830536007e-05,
      "loss": 0.5825,
      "step": 5770
    },
    {
      "epoch": 1.0431330084822235,
      "grad_norm": 7.188645839691162,
      "learning_rate": 2.374445046020574e-05,
      "loss": 0.5958,
      "step": 5780
    },
    {
      "epoch": 1.044937736870601,
      "grad_norm": 4.983626365661621,
      "learning_rate": 2.3733622089875474e-05,
      "loss": 0.6919,
      "step": 5790
    },
    {
      "epoch": 1.0467424652589785,
      "grad_norm": 12.956276893615723,
      "learning_rate": 2.372279371954521e-05,
      "loss": 0.6704,
      "step": 5800
    },
    {
      "epoch": 1.048547193647356,
      "grad_norm": 9.443437576293945,
      "learning_rate": 2.3711965349214944e-05,
      "loss": 0.6322,
      "step": 5810
    },
    {
      "epoch": 1.0503519220357336,
      "grad_norm": 12.211381912231445,
      "learning_rate": 2.370113697888468e-05,
      "loss": 0.5498,
      "step": 5820
    },
    {
      "epoch": 1.052156650424111,
      "grad_norm": 8.222676277160645,
      "learning_rate": 2.3690308608554413e-05,
      "loss": 0.5389,
      "step": 5830
    },
    {
      "epoch": 1.0539613788124886,
      "grad_norm": 13.926950454711914,
      "learning_rate": 2.367948023822415e-05,
      "loss": 0.6559,
      "step": 5840
    },
    {
      "epoch": 1.0557661072008662,
      "grad_norm": 7.368180751800537,
      "learning_rate": 2.3668651867893883e-05,
      "loss": 0.5764,
      "step": 5850
    },
    {
      "epoch": 1.057570835589244,
      "grad_norm": 11.769477844238281,
      "learning_rate": 2.3657823497563618e-05,
      "loss": 0.6983,
      "step": 5860
    },
    {
      "epoch": 1.0593755639776214,
      "grad_norm": 21.257793426513672,
      "learning_rate": 2.364699512723335e-05,
      "loss": 0.6221,
      "step": 5870
    },
    {
      "epoch": 1.061180292365999,
      "grad_norm": 8.457779884338379,
      "learning_rate": 2.3636166756903088e-05,
      "loss": 0.6048,
      "step": 5880
    },
    {
      "epoch": 1.0629850207543765,
      "grad_norm": 17.122222900390625,
      "learning_rate": 2.3625338386572823e-05,
      "loss": 0.5943,
      "step": 5890
    },
    {
      "epoch": 1.064789749142754,
      "grad_norm": 11.914726257324219,
      "learning_rate": 2.3614510016242558e-05,
      "loss": 0.6214,
      "step": 5900
    },
    {
      "epoch": 1.0665944775311316,
      "grad_norm": 14.315245628356934,
      "learning_rate": 2.360368164591229e-05,
      "loss": 0.5397,
      "step": 5910
    },
    {
      "epoch": 1.068399205919509,
      "grad_norm": 7.549063682556152,
      "learning_rate": 2.3592853275582024e-05,
      "loss": 0.6114,
      "step": 5920
    },
    {
      "epoch": 1.0702039343078866,
      "grad_norm": 5.431629657745361,
      "learning_rate": 2.358202490525176e-05,
      "loss": 0.6565,
      "step": 5930
    },
    {
      "epoch": 1.0720086626962642,
      "grad_norm": 17.499969482421875,
      "learning_rate": 2.3571196534921497e-05,
      "loss": 0.6249,
      "step": 5940
    },
    {
      "epoch": 1.0738133910846417,
      "grad_norm": 5.239292144775391,
      "learning_rate": 2.356036816459123e-05,
      "loss": 0.6477,
      "step": 5950
    },
    {
      "epoch": 1.0756181194730192,
      "grad_norm": 6.497111797332764,
      "learning_rate": 2.3549539794260964e-05,
      "loss": 0.7586,
      "step": 5960
    },
    {
      "epoch": 1.077422847861397,
      "grad_norm": 8.210709571838379,
      "learning_rate": 2.35387114239307e-05,
      "loss": 0.6189,
      "step": 5970
    },
    {
      "epoch": 1.0792275762497745,
      "grad_norm": 12.315034866333008,
      "learning_rate": 2.3527883053600434e-05,
      "loss": 0.5299,
      "step": 5980
    },
    {
      "epoch": 1.081032304638152,
      "grad_norm": 7.316166877746582,
      "learning_rate": 2.351705468327017e-05,
      "loss": 0.5215,
      "step": 5990
    },
    {
      "epoch": 1.0828370330265296,
      "grad_norm": 7.936036586761475,
      "learning_rate": 2.3506226312939903e-05,
      "loss": 0.7519,
      "step": 6000
    },
    {
      "epoch": 1.084641761414907,
      "grad_norm": 10.651389122009277,
      "learning_rate": 2.349539794260964e-05,
      "loss": 0.6328,
      "step": 6010
    },
    {
      "epoch": 1.0864464898032846,
      "grad_norm": 11.113183975219727,
      "learning_rate": 2.3484569572279373e-05,
      "loss": 0.6633,
      "step": 6020
    },
    {
      "epoch": 1.0882512181916622,
      "grad_norm": 8.780152320861816,
      "learning_rate": 2.3473741201949108e-05,
      "loss": 0.5478,
      "step": 6030
    },
    {
      "epoch": 1.0900559465800397,
      "grad_norm": 17.095327377319336,
      "learning_rate": 2.346291283161884e-05,
      "loss": 0.7682,
      "step": 6040
    },
    {
      "epoch": 1.0918606749684172,
      "grad_norm": 9.016971588134766,
      "learning_rate": 2.3452084461288575e-05,
      "loss": 0.6013,
      "step": 6050
    },
    {
      "epoch": 1.0936654033567947,
      "grad_norm": 15.780477523803711,
      "learning_rate": 2.3441256090958313e-05,
      "loss": 0.5956,
      "step": 6060
    },
    {
      "epoch": 1.0954701317451723,
      "grad_norm": 14.019770622253418,
      "learning_rate": 2.3430427720628048e-05,
      "loss": 0.4905,
      "step": 6070
    },
    {
      "epoch": 1.0972748601335498,
      "grad_norm": 8.83520221710205,
      "learning_rate": 2.3419599350297783e-05,
      "loss": 0.6604,
      "step": 6080
    },
    {
      "epoch": 1.0990795885219276,
      "grad_norm": 5.012866497039795,
      "learning_rate": 2.3408770979967514e-05,
      "loss": 0.5544,
      "step": 6090
    },
    {
      "epoch": 1.100884316910305,
      "grad_norm": 18.369487762451172,
      "learning_rate": 2.339794260963725e-05,
      "loss": 0.6122,
      "step": 6100
    },
    {
      "epoch": 1.1026890452986826,
      "grad_norm": 15.271881103515625,
      "learning_rate": 2.3387114239306984e-05,
      "loss": 0.6015,
      "step": 6110
    },
    {
      "epoch": 1.1044937736870601,
      "grad_norm": 7.2700395584106445,
      "learning_rate": 2.3376285868976722e-05,
      "loss": 0.6658,
      "step": 6120
    },
    {
      "epoch": 1.1062985020754377,
      "grad_norm": 12.016700744628906,
      "learning_rate": 2.3365457498646454e-05,
      "loss": 0.6409,
      "step": 6130
    },
    {
      "epoch": 1.1081032304638152,
      "grad_norm": 20.168973922729492,
      "learning_rate": 2.335462912831619e-05,
      "loss": 0.5565,
      "step": 6140
    },
    {
      "epoch": 1.1099079588521927,
      "grad_norm": 13.13112735748291,
      "learning_rate": 2.3343800757985924e-05,
      "loss": 0.6204,
      "step": 6150
    },
    {
      "epoch": 1.1117126872405703,
      "grad_norm": 12.369002342224121,
      "learning_rate": 2.333297238765566e-05,
      "loss": 0.6312,
      "step": 6160
    },
    {
      "epoch": 1.1135174156289478,
      "grad_norm": 15.442694664001465,
      "learning_rate": 2.3322144017325393e-05,
      "loss": 0.6103,
      "step": 6170
    },
    {
      "epoch": 1.1153221440173253,
      "grad_norm": 9.858444213867188,
      "learning_rate": 2.3311315646995128e-05,
      "loss": 0.6641,
      "step": 6180
    },
    {
      "epoch": 1.1171268724057029,
      "grad_norm": 10.199231147766113,
      "learning_rate": 2.3300487276664863e-05,
      "loss": 0.5596,
      "step": 6190
    },
    {
      "epoch": 1.1189316007940806,
      "grad_norm": 13.1950044631958,
      "learning_rate": 2.3289658906334598e-05,
      "loss": 0.5303,
      "step": 6200
    },
    {
      "epoch": 1.1207363291824581,
      "grad_norm": 8.505019187927246,
      "learning_rate": 2.3278830536004333e-05,
      "loss": 0.6863,
      "step": 6210
    },
    {
      "epoch": 1.1225410575708357,
      "grad_norm": 19.50261116027832,
      "learning_rate": 2.3268002165674065e-05,
      "loss": 0.6397,
      "step": 6220
    },
    {
      "epoch": 1.1243457859592132,
      "grad_norm": 6.0290021896362305,
      "learning_rate": 2.32571737953438e-05,
      "loss": 0.4827,
      "step": 6230
    },
    {
      "epoch": 1.1261505143475907,
      "grad_norm": 9.318227767944336,
      "learning_rate": 2.3246345425013538e-05,
      "loss": 0.6017,
      "step": 6240
    },
    {
      "epoch": 1.1279552427359683,
      "grad_norm": 11.486775398254395,
      "learning_rate": 2.3235517054683273e-05,
      "loss": 0.55,
      "step": 6250
    },
    {
      "epoch": 1.1297599711243458,
      "grad_norm": 15.897258758544922,
      "learning_rate": 2.3224688684353008e-05,
      "loss": 0.5932,
      "step": 6260
    },
    {
      "epoch": 1.1315646995127233,
      "grad_norm": 20.86455726623535,
      "learning_rate": 2.321386031402274e-05,
      "loss": 0.6168,
      "step": 6270
    },
    {
      "epoch": 1.1333694279011008,
      "grad_norm": 16.82143211364746,
      "learning_rate": 2.3203031943692474e-05,
      "loss": 0.4478,
      "step": 6280
    },
    {
      "epoch": 1.1351741562894784,
      "grad_norm": 11.711892127990723,
      "learning_rate": 2.319220357336221e-05,
      "loss": 0.4994,
      "step": 6290
    },
    {
      "epoch": 1.136978884677856,
      "grad_norm": 10.028494834899902,
      "learning_rate": 2.3181375203031947e-05,
      "loss": 0.777,
      "step": 6300
    },
    {
      "epoch": 1.1387836130662334,
      "grad_norm": 4.310519695281982,
      "learning_rate": 2.317054683270168e-05,
      "loss": 0.4882,
      "step": 6310
    },
    {
      "epoch": 1.140588341454611,
      "grad_norm": 9.91346263885498,
      "learning_rate": 2.3160801299404442e-05,
      "loss": 0.7994,
      "step": 6320
    },
    {
      "epoch": 1.1423930698429887,
      "grad_norm": 9.988494873046875,
      "learning_rate": 2.3149972929074174e-05,
      "loss": 0.7937,
      "step": 6330
    },
    {
      "epoch": 1.1441977982313662,
      "grad_norm": 13.592138290405273,
      "learning_rate": 2.313914455874391e-05,
      "loss": 0.5624,
      "step": 6340
    },
    {
      "epoch": 1.1460025266197438,
      "grad_norm": 10.320317268371582,
      "learning_rate": 2.3128316188413643e-05,
      "loss": 0.6811,
      "step": 6350
    },
    {
      "epoch": 1.1478072550081213,
      "grad_norm": 7.773855209350586,
      "learning_rate": 2.3117487818083382e-05,
      "loss": 0.6684,
      "step": 6360
    },
    {
      "epoch": 1.1496119833964988,
      "grad_norm": 3.7952935695648193,
      "learning_rate": 2.3106659447753113e-05,
      "loss": 0.6103,
      "step": 6370
    },
    {
      "epoch": 1.1514167117848764,
      "grad_norm": 9.653714179992676,
      "learning_rate": 2.3095831077422848e-05,
      "loss": 0.5419,
      "step": 6380
    },
    {
      "epoch": 1.153221440173254,
      "grad_norm": 6.515361785888672,
      "learning_rate": 2.3085002707092583e-05,
      "loss": 0.5984,
      "step": 6390
    },
    {
      "epoch": 1.1550261685616314,
      "grad_norm": 13.190948486328125,
      "learning_rate": 2.3074174336762318e-05,
      "loss": 0.7661,
      "step": 6400
    },
    {
      "epoch": 1.156830896950009,
      "grad_norm": 7.037515640258789,
      "learning_rate": 2.3063345966432053e-05,
      "loss": 0.5902,
      "step": 6410
    },
    {
      "epoch": 1.1586356253383865,
      "grad_norm": 13.527021408081055,
      "learning_rate": 2.3052517596101788e-05,
      "loss": 0.6406,
      "step": 6420
    },
    {
      "epoch": 1.1604403537267642,
      "grad_norm": 10.570398330688477,
      "learning_rate": 2.3041689225771523e-05,
      "loss": 0.7499,
      "step": 6430
    },
    {
      "epoch": 1.1622450821151418,
      "grad_norm": 11.912323951721191,
      "learning_rate": 2.3030860855441258e-05,
      "loss": 0.6857,
      "step": 6440
    },
    {
      "epoch": 1.1640498105035193,
      "grad_norm": 9.215794563293457,
      "learning_rate": 2.3020032485110992e-05,
      "loss": 0.6366,
      "step": 6450
    },
    {
      "epoch": 1.1658545388918968,
      "grad_norm": 8.88694953918457,
      "learning_rate": 2.3009204114780724e-05,
      "loss": 0.6463,
      "step": 6460
    },
    {
      "epoch": 1.1676592672802744,
      "grad_norm": 9.732819557189941,
      "learning_rate": 2.299837574445046e-05,
      "loss": 0.6049,
      "step": 6470
    },
    {
      "epoch": 1.169463995668652,
      "grad_norm": 13.757821083068848,
      "learning_rate": 2.2987547374120197e-05,
      "loss": 0.6174,
      "step": 6480
    },
    {
      "epoch": 1.1712687240570294,
      "grad_norm": 7.61509370803833,
      "learning_rate": 2.2976719003789932e-05,
      "loss": 0.5897,
      "step": 6490
    },
    {
      "epoch": 1.173073452445407,
      "grad_norm": 8.401864051818848,
      "learning_rate": 2.2965890633459664e-05,
      "loss": 0.7129,
      "step": 6500
    },
    {
      "epoch": 1.1748781808337845,
      "grad_norm": 12.552331924438477,
      "learning_rate": 2.29550622631294e-05,
      "loss": 0.5507,
      "step": 6510
    },
    {
      "epoch": 1.176682909222162,
      "grad_norm": 11.171587944030762,
      "learning_rate": 2.2944233892799133e-05,
      "loss": 0.4946,
      "step": 6520
    },
    {
      "epoch": 1.1784876376105395,
      "grad_norm": 9.778641700744629,
      "learning_rate": 2.293340552246887e-05,
      "loss": 0.5955,
      "step": 6530
    },
    {
      "epoch": 1.180292365998917,
      "grad_norm": 10.986379623413086,
      "learning_rate": 2.2922577152138607e-05,
      "loss": 0.5786,
      "step": 6540
    },
    {
      "epoch": 1.1820970943872946,
      "grad_norm": 10.251882553100586,
      "learning_rate": 2.2911748781808338e-05,
      "loss": 0.7609,
      "step": 6550
    },
    {
      "epoch": 1.1839018227756724,
      "grad_norm": 28.999698638916016,
      "learning_rate": 2.2900920411478073e-05,
      "loss": 0.5539,
      "step": 6560
    },
    {
      "epoch": 1.1857065511640499,
      "grad_norm": 11.21397590637207,
      "learning_rate": 2.2890092041147808e-05,
      "loss": 0.6324,
      "step": 6570
    },
    {
      "epoch": 1.1875112795524274,
      "grad_norm": 6.0484700202941895,
      "learning_rate": 2.2879263670817543e-05,
      "loss": 0.6154,
      "step": 6580
    },
    {
      "epoch": 1.189316007940805,
      "grad_norm": 12.143841743469238,
      "learning_rate": 2.2868435300487274e-05,
      "loss": 0.5645,
      "step": 6590
    },
    {
      "epoch": 1.1911207363291825,
      "grad_norm": 14.39420223236084,
      "learning_rate": 2.2857606930157013e-05,
      "loss": 0.5841,
      "step": 6600
    },
    {
      "epoch": 1.19292546471756,
      "grad_norm": 10.80527400970459,
      "learning_rate": 2.2846778559826748e-05,
      "loss": 0.5821,
      "step": 6610
    },
    {
      "epoch": 1.1947301931059375,
      "grad_norm": 8.208698272705078,
      "learning_rate": 2.2835950189496482e-05,
      "loss": 0.5759,
      "step": 6620
    },
    {
      "epoch": 1.196534921494315,
      "grad_norm": 12.664030075073242,
      "learning_rate": 2.2825121819166217e-05,
      "loss": 0.6334,
      "step": 6630
    },
    {
      "epoch": 1.1983396498826926,
      "grad_norm": 8.516213417053223,
      "learning_rate": 2.281429344883595e-05,
      "loss": 0.6047,
      "step": 6640
    },
    {
      "epoch": 1.2001443782710701,
      "grad_norm": 20.412776947021484,
      "learning_rate": 2.2803465078505684e-05,
      "loss": 0.6744,
      "step": 6650
    },
    {
      "epoch": 1.2019491066594479,
      "grad_norm": 14.785597801208496,
      "learning_rate": 2.2792636708175422e-05,
      "loss": 0.5895,
      "step": 6660
    },
    {
      "epoch": 1.2037538350478254,
      "grad_norm": 13.457650184631348,
      "learning_rate": 2.2781808337845157e-05,
      "loss": 0.6257,
      "step": 6670
    },
    {
      "epoch": 1.205558563436203,
      "grad_norm": 15.014097213745117,
      "learning_rate": 2.277097996751489e-05,
      "loss": 0.767,
      "step": 6680
    },
    {
      "epoch": 1.2073632918245805,
      "grad_norm": 8.281535148620605,
      "learning_rate": 2.2760151597184623e-05,
      "loss": 0.7564,
      "step": 6690
    },
    {
      "epoch": 1.209168020212958,
      "grad_norm": 11.941576957702637,
      "learning_rate": 2.2749323226854358e-05,
      "loss": 0.68,
      "step": 6700
    },
    {
      "epoch": 1.2109727486013355,
      "grad_norm": 6.378127574920654,
      "learning_rate": 2.2738494856524093e-05,
      "loss": 0.5939,
      "step": 6710
    },
    {
      "epoch": 1.212777476989713,
      "grad_norm": 7.474274635314941,
      "learning_rate": 2.272766648619383e-05,
      "loss": 0.6571,
      "step": 6720
    },
    {
      "epoch": 1.2145822053780906,
      "grad_norm": 9.89262580871582,
      "learning_rate": 2.2716838115863563e-05,
      "loss": 0.6465,
      "step": 6730
    },
    {
      "epoch": 1.2163869337664681,
      "grad_norm": 12.040514945983887,
      "learning_rate": 2.2706009745533298e-05,
      "loss": 0.5413,
      "step": 6740
    },
    {
      "epoch": 1.2181916621548456,
      "grad_norm": 18.006694793701172,
      "learning_rate": 2.2695181375203033e-05,
      "loss": 0.6316,
      "step": 6750
    },
    {
      "epoch": 1.2199963905432232,
      "grad_norm": 13.023351669311523,
      "learning_rate": 2.2684353004872768e-05,
      "loss": 0.5463,
      "step": 6760
    },
    {
      "epoch": 1.2218011189316007,
      "grad_norm": 13.566488265991211,
      "learning_rate": 2.26735246345425e-05,
      "loss": 0.5048,
      "step": 6770
    },
    {
      "epoch": 1.2236058473199782,
      "grad_norm": 18.585769653320312,
      "learning_rate": 2.2662696264212238e-05,
      "loss": 0.5791,
      "step": 6780
    },
    {
      "epoch": 1.225410575708356,
      "grad_norm": 16.72098159790039,
      "learning_rate": 2.2651867893881972e-05,
      "loss": 0.6621,
      "step": 6790
    },
    {
      "epoch": 1.2272153040967335,
      "grad_norm": 6.744295120239258,
      "learning_rate": 2.2641039523551707e-05,
      "loss": 0.575,
      "step": 6800
    },
    {
      "epoch": 1.229020032485111,
      "grad_norm": 11.456888198852539,
      "learning_rate": 2.2630211153221442e-05,
      "loss": 0.5936,
      "step": 6810
    },
    {
      "epoch": 1.2308247608734886,
      "grad_norm": 14.116236686706543,
      "learning_rate": 2.2619382782891174e-05,
      "loss": 0.7383,
      "step": 6820
    },
    {
      "epoch": 1.232629489261866,
      "grad_norm": 16.678890228271484,
      "learning_rate": 2.260855441256091e-05,
      "loss": 0.755,
      "step": 6830
    },
    {
      "epoch": 1.2344342176502436,
      "grad_norm": 11.39245319366455,
      "learning_rate": 2.2597726042230647e-05,
      "loss": 0.7183,
      "step": 6840
    },
    {
      "epoch": 1.2362389460386212,
      "grad_norm": 9.491570472717285,
      "learning_rate": 2.2586897671900382e-05,
      "loss": 0.5658,
      "step": 6850
    },
    {
      "epoch": 1.2380436744269987,
      "grad_norm": 13.767967224121094,
      "learning_rate": 2.2576069301570113e-05,
      "loss": 0.62,
      "step": 6860
    },
    {
      "epoch": 1.2398484028153762,
      "grad_norm": 8.964959144592285,
      "learning_rate": 2.2565240931239848e-05,
      "loss": 0.6314,
      "step": 6870
    },
    {
      "epoch": 1.2416531312037538,
      "grad_norm": 14.366436958312988,
      "learning_rate": 2.2554412560909583e-05,
      "loss": 0.6003,
      "step": 6880
    },
    {
      "epoch": 1.2434578595921313,
      "grad_norm": 9.065902709960938,
      "learning_rate": 2.2543584190579318e-05,
      "loss": 0.6906,
      "step": 6890
    },
    {
      "epoch": 1.245262587980509,
      "grad_norm": 6.458346366882324,
      "learning_rate": 2.2532755820249053e-05,
      "loss": 0.5788,
      "step": 6900
    },
    {
      "epoch": 1.2470673163688866,
      "grad_norm": 12.403169631958008,
      "learning_rate": 2.2521927449918788e-05,
      "loss": 0.634,
      "step": 6910
    },
    {
      "epoch": 1.248872044757264,
      "grad_norm": 12.188035011291504,
      "learning_rate": 2.2511099079588523e-05,
      "loss": 0.586,
      "step": 6920
    },
    {
      "epoch": 1.2506767731456416,
      "grad_norm": 8.411941528320312,
      "learning_rate": 2.2500270709258258e-05,
      "loss": 0.4951,
      "step": 6930
    },
    {
      "epoch": 1.2524815015340192,
      "grad_norm": 14.034229278564453,
      "learning_rate": 2.2489442338927993e-05,
      "loss": 0.6214,
      "step": 6940
    },
    {
      "epoch": 1.2542862299223967,
      "grad_norm": 16.152881622314453,
      "learning_rate": 2.2478613968597724e-05,
      "loss": 0.612,
      "step": 6950
    },
    {
      "epoch": 1.2560909583107742,
      "grad_norm": 7.739408493041992,
      "learning_rate": 2.2467785598267462e-05,
      "loss": 0.566,
      "step": 6960
    },
    {
      "epoch": 1.2578956866991517,
      "grad_norm": 5.883763790130615,
      "learning_rate": 2.2456957227937197e-05,
      "loss": 0.5733,
      "step": 6970
    },
    {
      "epoch": 1.2597004150875293,
      "grad_norm": 8.383526802062988,
      "learning_rate": 2.2446128857606932e-05,
      "loss": 0.7116,
      "step": 6980
    },
    {
      "epoch": 1.2615051434759068,
      "grad_norm": 13.902519226074219,
      "learning_rate": 2.2435300487276664e-05,
      "loss": 0.5871,
      "step": 6990
    },
    {
      "epoch": 1.2633098718642843,
      "grad_norm": 15.4509916305542,
      "learning_rate": 2.24244721169464e-05,
      "loss": 0.5829,
      "step": 7000
    },
    {
      "epoch": 1.2651146002526619,
      "grad_norm": 8.59496021270752,
      "learning_rate": 2.2413643746616134e-05,
      "loss": 0.5808,
      "step": 7010
    },
    {
      "epoch": 1.2669193286410394,
      "grad_norm": 10.790924072265625,
      "learning_rate": 2.2402815376285872e-05,
      "loss": 0.6218,
      "step": 7020
    },
    {
      "epoch": 1.2687240570294172,
      "grad_norm": 12.347400665283203,
      "learning_rate": 2.2391987005955607e-05,
      "loss": 0.5879,
      "step": 7030
    },
    {
      "epoch": 1.2705287854177947,
      "grad_norm": 5.3914055824279785,
      "learning_rate": 2.2381158635625338e-05,
      "loss": 0.6136,
      "step": 7040
    },
    {
      "epoch": 1.2723335138061722,
      "grad_norm": 4.603033542633057,
      "learning_rate": 2.2370330265295073e-05,
      "loss": 0.5763,
      "step": 7050
    },
    {
      "epoch": 1.2741382421945497,
      "grad_norm": 7.25995397567749,
      "learning_rate": 2.2359501894964808e-05,
      "loss": 0.6427,
      "step": 7060
    },
    {
      "epoch": 1.2759429705829273,
      "grad_norm": 7.541897773742676,
      "learning_rate": 2.2348673524634543e-05,
      "loss": 0.5177,
      "step": 7070
    },
    {
      "epoch": 1.2777476989713048,
      "grad_norm": 13.894547462463379,
      "learning_rate": 2.2337845154304278e-05,
      "loss": 0.6211,
      "step": 7080
    },
    {
      "epoch": 1.2795524273596823,
      "grad_norm": 15.2687349319458,
      "learning_rate": 2.2327016783974013e-05,
      "loss": 0.6619,
      "step": 7090
    },
    {
      "epoch": 1.2813571557480599,
      "grad_norm": 9.383647918701172,
      "learning_rate": 2.2316188413643748e-05,
      "loss": 0.5254,
      "step": 7100
    },
    {
      "epoch": 1.2831618841364374,
      "grad_norm": 12.608994483947754,
      "learning_rate": 2.2305360043313483e-05,
      "loss": 0.6834,
      "step": 7110
    },
    {
      "epoch": 1.2849666125248151,
      "grad_norm": 14.741049766540527,
      "learning_rate": 2.2294531672983217e-05,
      "loss": 0.6843,
      "step": 7120
    },
    {
      "epoch": 1.2867713409131927,
      "grad_norm": 10.486527442932129,
      "learning_rate": 2.228370330265295e-05,
      "loss": 0.6505,
      "step": 7130
    },
    {
      "epoch": 1.2885760693015702,
      "grad_norm": 12.138701438903809,
      "learning_rate": 2.2272874932322684e-05,
      "loss": 0.6318,
      "step": 7140
    },
    {
      "epoch": 1.2903807976899477,
      "grad_norm": 8.721048355102539,
      "learning_rate": 2.2262046561992422e-05,
      "loss": 0.4541,
      "step": 7150
    },
    {
      "epoch": 1.2921855260783253,
      "grad_norm": 20.97931480407715,
      "learning_rate": 2.2251218191662157e-05,
      "loss": 0.633,
      "step": 7160
    },
    {
      "epoch": 1.2939902544667028,
      "grad_norm": 4.227215766906738,
      "learning_rate": 2.224038982133189e-05,
      "loss": 0.5961,
      "step": 7170
    },
    {
      "epoch": 1.2957949828550803,
      "grad_norm": 19.455219268798828,
      "learning_rate": 2.2229561451001624e-05,
      "loss": 0.6484,
      "step": 7180
    },
    {
      "epoch": 1.2975997112434579,
      "grad_norm": 13.105448722839355,
      "learning_rate": 2.221873308067136e-05,
      "loss": 0.484,
      "step": 7190
    },
    {
      "epoch": 1.2994044396318354,
      "grad_norm": 12.530311584472656,
      "learning_rate": 2.2207904710341093e-05,
      "loss": 0.5482,
      "step": 7200
    },
    {
      "epoch": 1.301209168020213,
      "grad_norm": 18.264373779296875,
      "learning_rate": 2.219707634001083e-05,
      "loss": 0.5638,
      "step": 7210
    },
    {
      "epoch": 1.3030138964085904,
      "grad_norm": 17.95819854736328,
      "learning_rate": 2.2186247969680563e-05,
      "loss": 0.7433,
      "step": 7220
    },
    {
      "epoch": 1.304818624796968,
      "grad_norm": 17.615768432617188,
      "learning_rate": 2.2175419599350298e-05,
      "loss": 0.6073,
      "step": 7230
    },
    {
      "epoch": 1.3066233531853455,
      "grad_norm": 15.366074562072754,
      "learning_rate": 2.2164591229020033e-05,
      "loss": 0.596,
      "step": 7240
    },
    {
      "epoch": 1.308428081573723,
      "grad_norm": 9.287339210510254,
      "learning_rate": 2.2153762858689768e-05,
      "loss": 0.7028,
      "step": 7250
    },
    {
      "epoch": 1.3102328099621008,
      "grad_norm": 10.504122734069824,
      "learning_rate": 2.21429344883595e-05,
      "loss": 0.6922,
      "step": 7260
    },
    {
      "epoch": 1.3120375383504783,
      "grad_norm": 8.891361236572266,
      "learning_rate": 2.2132106118029238e-05,
      "loss": 0.5943,
      "step": 7270
    },
    {
      "epoch": 1.3138422667388558,
      "grad_norm": 15.98179817199707,
      "learning_rate": 2.2121277747698973e-05,
      "loss": 0.5392,
      "step": 7280
    },
    {
      "epoch": 1.3156469951272334,
      "grad_norm": 29.404666900634766,
      "learning_rate": 2.2110449377368707e-05,
      "loss": 0.7384,
      "step": 7290
    },
    {
      "epoch": 1.317451723515611,
      "grad_norm": 6.660526752471924,
      "learning_rate": 2.2099621007038442e-05,
      "loss": 0.6427,
      "step": 7300
    },
    {
      "epoch": 1.3192564519039884,
      "grad_norm": 5.172798156738281,
      "learning_rate": 2.2088792636708174e-05,
      "loss": 0.586,
      "step": 7310
    },
    {
      "epoch": 1.321061180292366,
      "grad_norm": 12.505265235900879,
      "learning_rate": 2.207796426637791e-05,
      "loss": 0.5804,
      "step": 7320
    },
    {
      "epoch": 1.3228659086807435,
      "grad_norm": 16.1933536529541,
      "learning_rate": 2.2067135896047647e-05,
      "loss": 0.6537,
      "step": 7330
    },
    {
      "epoch": 1.324670637069121,
      "grad_norm": 13.010088920593262,
      "learning_rate": 2.2056307525717382e-05,
      "loss": 0.5266,
      "step": 7340
    },
    {
      "epoch": 1.3264753654574988,
      "grad_norm": 5.140311241149902,
      "learning_rate": 2.2045479155387114e-05,
      "loss": 0.5749,
      "step": 7350
    },
    {
      "epoch": 1.3282800938458763,
      "grad_norm": 8.864130020141602,
      "learning_rate": 2.203465078505685e-05,
      "loss": 0.5262,
      "step": 7360
    },
    {
      "epoch": 1.3300848222342538,
      "grad_norm": 8.69775390625,
      "learning_rate": 2.2023822414726583e-05,
      "loss": 0.4643,
      "step": 7370
    },
    {
      "epoch": 1.3318895506226314,
      "grad_norm": 12.071950912475586,
      "learning_rate": 2.2012994044396318e-05,
      "loss": 0.6728,
      "step": 7380
    },
    {
      "epoch": 1.333694279011009,
      "grad_norm": 23.053321838378906,
      "learning_rate": 2.2002165674066057e-05,
      "loss": 0.776,
      "step": 7390
    },
    {
      "epoch": 1.3354990073993864,
      "grad_norm": 12.062603950500488,
      "learning_rate": 2.1991337303735788e-05,
      "loss": 0.5963,
      "step": 7400
    },
    {
      "epoch": 1.337303735787764,
      "grad_norm": 13.466935157775879,
      "learning_rate": 2.1980508933405523e-05,
      "loss": 0.6729,
      "step": 7410
    },
    {
      "epoch": 1.3391084641761415,
      "grad_norm": 16.922279357910156,
      "learning_rate": 2.1969680563075258e-05,
      "loss": 0.5573,
      "step": 7420
    },
    {
      "epoch": 1.340913192564519,
      "grad_norm": 13.91783618927002,
      "learning_rate": 2.1958852192744993e-05,
      "loss": 0.745,
      "step": 7430
    },
    {
      "epoch": 1.3427179209528965,
      "grad_norm": 6.335987567901611,
      "learning_rate": 2.1948023822414724e-05,
      "loss": 0.6065,
      "step": 7440
    },
    {
      "epoch": 1.344522649341274,
      "grad_norm": 9.061503410339355,
      "learning_rate": 2.1937195452084463e-05,
      "loss": 0.5732,
      "step": 7450
    },
    {
      "epoch": 1.3463273777296516,
      "grad_norm": 14.150078773498535,
      "learning_rate": 2.1926367081754197e-05,
      "loss": 0.6558,
      "step": 7460
    },
    {
      "epoch": 1.3481321061180291,
      "grad_norm": 5.777167320251465,
      "learning_rate": 2.1915538711423932e-05,
      "loss": 0.6566,
      "step": 7470
    },
    {
      "epoch": 1.3499368345064067,
      "grad_norm": 11.502315521240234,
      "learning_rate": 2.1904710341093664e-05,
      "loss": 0.517,
      "step": 7480
    },
    {
      "epoch": 1.3517415628947842,
      "grad_norm": 12.799423217773438,
      "learning_rate": 2.18938819707634e-05,
      "loss": 0.6561,
      "step": 7490
    },
    {
      "epoch": 1.353546291283162,
      "grad_norm": 10.908431053161621,
      "learning_rate": 2.1883053600433134e-05,
      "loss": 0.5344,
      "step": 7500
    },
    {
      "epoch": 1.3553510196715395,
      "grad_norm": 15.136712074279785,
      "learning_rate": 2.1872225230102872e-05,
      "loss": 0.5355,
      "step": 7510
    },
    {
      "epoch": 1.357155748059917,
      "grad_norm": 19.988555908203125,
      "learning_rate": 2.1861396859772607e-05,
      "loss": 0.5394,
      "step": 7520
    },
    {
      "epoch": 1.3589604764482945,
      "grad_norm": 14.132540702819824,
      "learning_rate": 2.185056848944234e-05,
      "loss": 0.5298,
      "step": 7530
    },
    {
      "epoch": 1.360765204836672,
      "grad_norm": 10.375861167907715,
      "learning_rate": 2.1839740119112073e-05,
      "loss": 0.555,
      "step": 7540
    },
    {
      "epoch": 1.3625699332250496,
      "grad_norm": 6.464095115661621,
      "learning_rate": 2.1828911748781808e-05,
      "loss": 0.4368,
      "step": 7550
    },
    {
      "epoch": 1.3643746616134271,
      "grad_norm": 23.151309967041016,
      "learning_rate": 2.1818083378451543e-05,
      "loss": 0.6416,
      "step": 7560
    },
    {
      "epoch": 1.3661793900018047,
      "grad_norm": 22.13646125793457,
      "learning_rate": 2.1807255008121278e-05,
      "loss": 0.7883,
      "step": 7570
    },
    {
      "epoch": 1.3679841183901824,
      "grad_norm": 19.35406494140625,
      "learning_rate": 2.1796426637791013e-05,
      "loss": 0.6965,
      "step": 7580
    },
    {
      "epoch": 1.36978884677856,
      "grad_norm": 8.288683891296387,
      "learning_rate": 2.1785598267460748e-05,
      "loss": 0.7512,
      "step": 7590
    },
    {
      "epoch": 1.3715935751669375,
      "grad_norm": 6.603683948516846,
      "learning_rate": 2.1774769897130483e-05,
      "loss": 0.6166,
      "step": 7600
    },
    {
      "epoch": 1.373398303555315,
      "grad_norm": 7.792265892028809,
      "learning_rate": 2.1763941526800218e-05,
      "loss": 0.5056,
      "step": 7610
    },
    {
      "epoch": 1.3752030319436925,
      "grad_norm": 7.192683696746826,
      "learning_rate": 2.175311315646995e-05,
      "loss": 0.5889,
      "step": 7620
    },
    {
      "epoch": 1.37700776033207,
      "grad_norm": 11.688879013061523,
      "learning_rate": 2.1742284786139687e-05,
      "loss": 0.8948,
      "step": 7630
    },
    {
      "epoch": 1.3788124887204476,
      "grad_norm": 17.286149978637695,
      "learning_rate": 2.1731456415809422e-05,
      "loss": 0.7372,
      "step": 7640
    },
    {
      "epoch": 1.3806172171088251,
      "grad_norm": 8.052020072937012,
      "learning_rate": 2.1720628045479157e-05,
      "loss": 0.6894,
      "step": 7650
    },
    {
      "epoch": 1.3824219454972027,
      "grad_norm": 9.566810607910156,
      "learning_rate": 2.170979967514889e-05,
      "loss": 0.58,
      "step": 7660
    },
    {
      "epoch": 1.3842266738855802,
      "grad_norm": 2.9800124168395996,
      "learning_rate": 2.1698971304818624e-05,
      "loss": 0.541,
      "step": 7670
    },
    {
      "epoch": 1.3860314022739577,
      "grad_norm": 13.85472583770752,
      "learning_rate": 2.168814293448836e-05,
      "loss": 0.5451,
      "step": 7680
    },
    {
      "epoch": 1.3878361306623352,
      "grad_norm": 12.690093040466309,
      "learning_rate": 2.1677314564158097e-05,
      "loss": 0.6024,
      "step": 7690
    },
    {
      "epoch": 1.3896408590507128,
      "grad_norm": 8.876220703125,
      "learning_rate": 2.1666486193827832e-05,
      "loss": 0.4832,
      "step": 7700
    },
    {
      "epoch": 1.3914455874390903,
      "grad_norm": 8.924596786499023,
      "learning_rate": 2.1655657823497563e-05,
      "loss": 0.5036,
      "step": 7710
    },
    {
      "epoch": 1.3932503158274678,
      "grad_norm": 11.317234992980957,
      "learning_rate": 2.1644829453167298e-05,
      "loss": 0.6191,
      "step": 7720
    },
    {
      "epoch": 1.3950550442158456,
      "grad_norm": 10.430802345275879,
      "learning_rate": 2.1634001082837033e-05,
      "loss": 0.534,
      "step": 7730
    },
    {
      "epoch": 1.3968597726042231,
      "grad_norm": 13.6421480178833,
      "learning_rate": 2.1623172712506768e-05,
      "loss": 0.666,
      "step": 7740
    },
    {
      "epoch": 1.3986645009926006,
      "grad_norm": 8.600759506225586,
      "learning_rate": 2.1612344342176503e-05,
      "loss": 0.602,
      "step": 7750
    },
    {
      "epoch": 1.4004692293809782,
      "grad_norm": 14.434863090515137,
      "learning_rate": 2.1601515971846238e-05,
      "loss": 0.7202,
      "step": 7760
    },
    {
      "epoch": 1.4022739577693557,
      "grad_norm": 20.790773391723633,
      "learning_rate": 2.1590687601515973e-05,
      "loss": 0.5453,
      "step": 7770
    },
    {
      "epoch": 1.4040786861577332,
      "grad_norm": 8.25813102722168,
      "learning_rate": 2.1579859231185708e-05,
      "loss": 0.606,
      "step": 7780
    },
    {
      "epoch": 1.4058834145461108,
      "grad_norm": 13.179244995117188,
      "learning_rate": 2.1569030860855443e-05,
      "loss": 0.6386,
      "step": 7790
    },
    {
      "epoch": 1.4076881429344883,
      "grad_norm": 13.589305877685547,
      "learning_rate": 2.1558202490525174e-05,
      "loss": 0.5708,
      "step": 7800
    },
    {
      "epoch": 1.4094928713228658,
      "grad_norm": 2.365792989730835,
      "learning_rate": 2.1547374120194912e-05,
      "loss": 0.5789,
      "step": 7810
    },
    {
      "epoch": 1.4112975997112436,
      "grad_norm": 7.604662895202637,
      "learning_rate": 2.1536545749864647e-05,
      "loss": 0.6349,
      "step": 7820
    },
    {
      "epoch": 1.413102328099621,
      "grad_norm": 7.585542678833008,
      "learning_rate": 2.1525717379534382e-05,
      "loss": 0.4216,
      "step": 7830
    },
    {
      "epoch": 1.4149070564879986,
      "grad_norm": 9.23923110961914,
      "learning_rate": 2.1514889009204114e-05,
      "loss": 0.6589,
      "step": 7840
    },
    {
      "epoch": 1.4167117848763762,
      "grad_norm": 9.418185234069824,
      "learning_rate": 2.150406063887385e-05,
      "loss": 0.4979,
      "step": 7850
    },
    {
      "epoch": 1.4185165132647537,
      "grad_norm": 12.268691062927246,
      "learning_rate": 2.1493232268543583e-05,
      "loss": 0.4524,
      "step": 7860
    },
    {
      "epoch": 1.4203212416531312,
      "grad_norm": 15.23015022277832,
      "learning_rate": 2.1482403898213322e-05,
      "loss": 0.6144,
      "step": 7870
    },
    {
      "epoch": 1.4221259700415088,
      "grad_norm": 10.62160873413086,
      "learning_rate": 2.1471575527883057e-05,
      "loss": 0.6559,
      "step": 7880
    },
    {
      "epoch": 1.4239306984298863,
      "grad_norm": 14.19282054901123,
      "learning_rate": 2.1460747157552788e-05,
      "loss": 0.6176,
      "step": 7890
    },
    {
      "epoch": 1.4257354268182638,
      "grad_norm": 10.147489547729492,
      "learning_rate": 2.1449918787222523e-05,
      "loss": 0.5214,
      "step": 7900
    },
    {
      "epoch": 1.4275401552066413,
      "grad_norm": 7.355388164520264,
      "learning_rate": 2.1439090416892258e-05,
      "loss": 0.7356,
      "step": 7910
    },
    {
      "epoch": 1.4293448835950189,
      "grad_norm": 19.68303871154785,
      "learning_rate": 2.1428262046561993e-05,
      "loss": 0.5204,
      "step": 7920
    },
    {
      "epoch": 1.4311496119833964,
      "grad_norm": 9.00692081451416,
      "learning_rate": 2.1417433676231728e-05,
      "loss": 0.5965,
      "step": 7930
    },
    {
      "epoch": 1.432954340371774,
      "grad_norm": 13.685291290283203,
      "learning_rate": 2.1406605305901463e-05,
      "loss": 0.5561,
      "step": 7940
    },
    {
      "epoch": 1.4347590687601515,
      "grad_norm": 14.686851501464844,
      "learning_rate": 2.1395776935571198e-05,
      "loss": 0.5578,
      "step": 7950
    },
    {
      "epoch": 1.4365637971485292,
      "grad_norm": 11.245469093322754,
      "learning_rate": 2.1384948565240932e-05,
      "loss": 0.5953,
      "step": 7960
    },
    {
      "epoch": 1.4383685255369068,
      "grad_norm": 10.748066902160645,
      "learning_rate": 2.1374120194910667e-05,
      "loss": 0.743,
      "step": 7970
    },
    {
      "epoch": 1.4401732539252843,
      "grad_norm": 13.804520606994629,
      "learning_rate": 2.13632918245804e-05,
      "loss": 0.593,
      "step": 7980
    },
    {
      "epoch": 1.4419779823136618,
      "grad_norm": 4.528295993804932,
      "learning_rate": 2.1352463454250137e-05,
      "loss": 0.5799,
      "step": 7990
    },
    {
      "epoch": 1.4437827107020393,
      "grad_norm": 8.986098289489746,
      "learning_rate": 2.1341635083919872e-05,
      "loss": 0.5156,
      "step": 8000
    },
    {
      "epoch": 1.4455874390904169,
      "grad_norm": 12.713688850402832,
      "learning_rate": 2.1330806713589607e-05,
      "loss": 0.5772,
      "step": 8010
    },
    {
      "epoch": 1.4473921674787944,
      "grad_norm": 10.537602424621582,
      "learning_rate": 2.131997834325934e-05,
      "loss": 0.5358,
      "step": 8020
    },
    {
      "epoch": 1.449196895867172,
      "grad_norm": 12.882370948791504,
      "learning_rate": 2.1309149972929073e-05,
      "loss": 0.5666,
      "step": 8030
    },
    {
      "epoch": 1.4510016242555495,
      "grad_norm": 7.6627702713012695,
      "learning_rate": 2.129832160259881e-05,
      "loss": 0.6244,
      "step": 8040
    },
    {
      "epoch": 1.4528063526439272,
      "grad_norm": 14.084413528442383,
      "learning_rate": 2.1287493232268547e-05,
      "loss": 0.6588,
      "step": 8050
    },
    {
      "epoch": 1.4546110810323047,
      "grad_norm": 14.185046195983887,
      "learning_rate": 2.1276664861938278e-05,
      "loss": 0.4918,
      "step": 8060
    },
    {
      "epoch": 1.4564158094206823,
      "grad_norm": 15.192763328552246,
      "learning_rate": 2.1265836491608013e-05,
      "loss": 0.5496,
      "step": 8070
    },
    {
      "epoch": 1.4582205378090598,
      "grad_norm": 3.7476253509521484,
      "learning_rate": 2.1255008121277748e-05,
      "loss": 0.6355,
      "step": 8080
    },
    {
      "epoch": 1.4600252661974373,
      "grad_norm": 7.975146293640137,
      "learning_rate": 2.1244179750947483e-05,
      "loss": 0.5267,
      "step": 8090
    },
    {
      "epoch": 1.4618299945858149,
      "grad_norm": 11.816664695739746,
      "learning_rate": 2.1233351380617218e-05,
      "loss": 0.6181,
      "step": 8100
    },
    {
      "epoch": 1.4636347229741924,
      "grad_norm": 11.991918563842773,
      "learning_rate": 2.1222523010286953e-05,
      "loss": 0.6897,
      "step": 8110
    },
    {
      "epoch": 1.46543945136257,
      "grad_norm": 8.473417282104492,
      "learning_rate": 2.1211694639956688e-05,
      "loss": 0.6029,
      "step": 8120
    },
    {
      "epoch": 1.4672441797509475,
      "grad_norm": 8.304655075073242,
      "learning_rate": 2.1200866269626422e-05,
      "loss": 0.59,
      "step": 8130
    },
    {
      "epoch": 1.469048908139325,
      "grad_norm": 16.523210525512695,
      "learning_rate": 2.1190037899296157e-05,
      "loss": 0.676,
      "step": 8140
    },
    {
      "epoch": 1.4708536365277025,
      "grad_norm": 6.815313339233398,
      "learning_rate": 2.117920952896589e-05,
      "loss": 0.6585,
      "step": 8150
    },
    {
      "epoch": 1.47265836491608,
      "grad_norm": 10.121003150939941,
      "learning_rate": 2.1168381158635624e-05,
      "loss": 0.7063,
      "step": 8160
    },
    {
      "epoch": 1.4744630933044576,
      "grad_norm": 9.943204879760742,
      "learning_rate": 2.1157552788305362e-05,
      "loss": 0.6074,
      "step": 8170
    },
    {
      "epoch": 1.476267821692835,
      "grad_norm": 9.553105354309082,
      "learning_rate": 2.1146724417975097e-05,
      "loss": 0.6336,
      "step": 8180
    },
    {
      "epoch": 1.4780725500812129,
      "grad_norm": 25.13271713256836,
      "learning_rate": 2.1135896047644832e-05,
      "loss": 0.6902,
      "step": 8190
    },
    {
      "epoch": 1.4798772784695904,
      "grad_norm": 23.714229583740234,
      "learning_rate": 2.1125067677314563e-05,
      "loss": 0.53,
      "step": 8200
    },
    {
      "epoch": 1.481682006857968,
      "grad_norm": 5.73724889755249,
      "learning_rate": 2.11142393069843e-05,
      "loss": 0.5511,
      "step": 8210
    },
    {
      "epoch": 1.4834867352463454,
      "grad_norm": 12.375028610229492,
      "learning_rate": 2.1103410936654033e-05,
      "loss": 0.5934,
      "step": 8220
    },
    {
      "epoch": 1.485291463634723,
      "grad_norm": 7.810361385345459,
      "learning_rate": 2.109258256632377e-05,
      "loss": 0.6173,
      "step": 8230
    },
    {
      "epoch": 1.4870961920231005,
      "grad_norm": 13.960867881774902,
      "learning_rate": 2.1081754195993503e-05,
      "loss": 0.6146,
      "step": 8240
    },
    {
      "epoch": 1.488900920411478,
      "grad_norm": 10.450282096862793,
      "learning_rate": 2.1070925825663238e-05,
      "loss": 0.6747,
      "step": 8250
    },
    {
      "epoch": 1.4907056487998556,
      "grad_norm": 12.9763765335083,
      "learning_rate": 2.1060097455332973e-05,
      "loss": 0.5218,
      "step": 8260
    },
    {
      "epoch": 1.492510377188233,
      "grad_norm": 11.363480567932129,
      "learning_rate": 2.1049269085002708e-05,
      "loss": 0.5849,
      "step": 8270
    },
    {
      "epoch": 1.4943151055766108,
      "grad_norm": 19.0554256439209,
      "learning_rate": 2.1038440714672443e-05,
      "loss": 0.6584,
      "step": 8280
    },
    {
      "epoch": 1.4961198339649884,
      "grad_norm": 8.56121826171875,
      "learning_rate": 2.1027612344342178e-05,
      "loss": 0.5883,
      "step": 8290
    },
    {
      "epoch": 1.497924562353366,
      "grad_norm": 7.701646327972412,
      "learning_rate": 2.1016783974011912e-05,
      "loss": 0.6458,
      "step": 8300
    },
    {
      "epoch": 1.4997292907417434,
      "grad_norm": 17.02938461303711,
      "learning_rate": 2.1005955603681647e-05,
      "loss": 0.5106,
      "step": 8310
    },
    {
      "epoch": 1.501534019130121,
      "grad_norm": 18.350261688232422,
      "learning_rate": 2.0995127233351382e-05,
      "loss": 0.647,
      "step": 8320
    },
    {
      "epoch": 1.5033387475184985,
      "grad_norm": 11.886680603027344,
      "learning_rate": 2.0984298863021114e-05,
      "loss": 0.5572,
      "step": 8330
    },
    {
      "epoch": 1.505143475906876,
      "grad_norm": 11.251039505004883,
      "learning_rate": 2.097347049269085e-05,
      "loss": 0.6687,
      "step": 8340
    },
    {
      "epoch": 1.5069482042952536,
      "grad_norm": 8.18588638305664,
      "learning_rate": 2.0962642122360587e-05,
      "loss": 0.6838,
      "step": 8350
    },
    {
      "epoch": 1.508752932683631,
      "grad_norm": 8.317716598510742,
      "learning_rate": 2.0951813752030322e-05,
      "loss": 0.557,
      "step": 8360
    },
    {
      "epoch": 1.5105576610720086,
      "grad_norm": 9.466727256774902,
      "learning_rate": 2.0940985381700057e-05,
      "loss": 0.624,
      "step": 8370
    },
    {
      "epoch": 1.5123623894603861,
      "grad_norm": 13.04416561126709,
      "learning_rate": 2.0930157011369788e-05,
      "loss": 0.5221,
      "step": 8380
    },
    {
      "epoch": 1.5141671178487637,
      "grad_norm": 12.4652681350708,
      "learning_rate": 2.0919328641039523e-05,
      "loss": 0.5809,
      "step": 8390
    },
    {
      "epoch": 1.5159718462371412,
      "grad_norm": 17.57645606994629,
      "learning_rate": 2.0908500270709258e-05,
      "loss": 0.6792,
      "step": 8400
    },
    {
      "epoch": 1.5177765746255187,
      "grad_norm": 6.56076192855835,
      "learning_rate": 2.0897671900378996e-05,
      "loss": 0.5583,
      "step": 8410
    },
    {
      "epoch": 1.5195813030138963,
      "grad_norm": 9.29477310180664,
      "learning_rate": 2.0886843530048728e-05,
      "loss": 0.5593,
      "step": 8420
    },
    {
      "epoch": 1.5213860314022738,
      "grad_norm": 13.0636568069458,
      "learning_rate": 2.0876015159718463e-05,
      "loss": 0.7011,
      "step": 8430
    },
    {
      "epoch": 1.5231907597906515,
      "grad_norm": 16.709091186523438,
      "learning_rate": 2.0865186789388198e-05,
      "loss": 0.6164,
      "step": 8440
    },
    {
      "epoch": 1.524995488179029,
      "grad_norm": 7.501031398773193,
      "learning_rate": 2.0854358419057933e-05,
      "loss": 0.5791,
      "step": 8450
    },
    {
      "epoch": 1.5268002165674066,
      "grad_norm": 13.037348747253418,
      "learning_rate": 2.0843530048727668e-05,
      "loss": 0.5714,
      "step": 8460
    },
    {
      "epoch": 1.5286049449557841,
      "grad_norm": 11.852566719055176,
      "learning_rate": 2.0832701678397402e-05,
      "loss": 0.4619,
      "step": 8470
    },
    {
      "epoch": 1.5304096733441617,
      "grad_norm": 11.643715858459473,
      "learning_rate": 2.0821873308067137e-05,
      "loss": 0.5687,
      "step": 8480
    },
    {
      "epoch": 1.5322144017325392,
      "grad_norm": 9.679201126098633,
      "learning_rate": 2.0811044937736872e-05,
      "loss": 0.6419,
      "step": 8490
    },
    {
      "epoch": 1.534019130120917,
      "grad_norm": 11.050642967224121,
      "learning_rate": 2.0800216567406607e-05,
      "loss": 0.5301,
      "step": 8500
    },
    {
      "epoch": 1.5358238585092945,
      "grad_norm": 12.604793548583984,
      "learning_rate": 2.078938819707634e-05,
      "loss": 0.5988,
      "step": 8510
    },
    {
      "epoch": 1.537628586897672,
      "grad_norm": 13.30410099029541,
      "learning_rate": 2.0778559826746074e-05,
      "loss": 0.6704,
      "step": 8520
    },
    {
      "epoch": 1.5394333152860495,
      "grad_norm": 9.109687805175781,
      "learning_rate": 2.076773145641581e-05,
      "loss": 0.5267,
      "step": 8530
    },
    {
      "epoch": 1.541238043674427,
      "grad_norm": 13.531502723693848,
      "learning_rate": 2.0756903086085547e-05,
      "loss": 0.5586,
      "step": 8540
    },
    {
      "epoch": 1.5430427720628046,
      "grad_norm": 16.419818878173828,
      "learning_rate": 2.0746074715755278e-05,
      "loss": 0.5219,
      "step": 8550
    },
    {
      "epoch": 1.5448475004511821,
      "grad_norm": 14.411920547485352,
      "learning_rate": 2.0735246345425013e-05,
      "loss": 0.5244,
      "step": 8560
    },
    {
      "epoch": 1.5466522288395597,
      "grad_norm": 13.68552303314209,
      "learning_rate": 2.0724417975094748e-05,
      "loss": 0.5278,
      "step": 8570
    },
    {
      "epoch": 1.5484569572279372,
      "grad_norm": 19.026371002197266,
      "learning_rate": 2.0713589604764483e-05,
      "loss": 0.5782,
      "step": 8580
    },
    {
      "epoch": 1.5502616856163147,
      "grad_norm": 12.849794387817383,
      "learning_rate": 2.0702761234434218e-05,
      "loss": 0.6336,
      "step": 8590
    },
    {
      "epoch": 1.5520664140046923,
      "grad_norm": 12.949807167053223,
      "learning_rate": 2.0691932864103953e-05,
      "loss": 0.4836,
      "step": 8600
    },
    {
      "epoch": 1.5538711423930698,
      "grad_norm": 14.169075012207031,
      "learning_rate": 2.0681104493773688e-05,
      "loss": 0.6765,
      "step": 8610
    },
    {
      "epoch": 1.5556758707814473,
      "grad_norm": 8.018895149230957,
      "learning_rate": 2.0670276123443423e-05,
      "loss": 0.433,
      "step": 8620
    },
    {
      "epoch": 1.5574805991698248,
      "grad_norm": 13.544278144836426,
      "learning_rate": 2.0659447753113158e-05,
      "loss": 0.6596,
      "step": 8630
    },
    {
      "epoch": 1.5592853275582024,
      "grad_norm": 15.438400268554688,
      "learning_rate": 2.064861938278289e-05,
      "loss": 0.4709,
      "step": 8640
    },
    {
      "epoch": 1.56109005594658,
      "grad_norm": 9.191094398498535,
      "learning_rate": 2.0637791012452624e-05,
      "loss": 0.6387,
      "step": 8650
    },
    {
      "epoch": 1.5628947843349574,
      "grad_norm": 10.715970039367676,
      "learning_rate": 2.0626962642122362e-05,
      "loss": 0.5598,
      "step": 8660
    },
    {
      "epoch": 1.5646995127233352,
      "grad_norm": 16.238191604614258,
      "learning_rate": 2.0616134271792097e-05,
      "loss": 0.6244,
      "step": 8670
    },
    {
      "epoch": 1.5665042411117127,
      "grad_norm": 17.522293090820312,
      "learning_rate": 2.0605305901461832e-05,
      "loss": 0.5817,
      "step": 8680
    },
    {
      "epoch": 1.5683089695000902,
      "grad_norm": 28.170202255249023,
      "learning_rate": 2.0594477531131564e-05,
      "loss": 0.6201,
      "step": 8690
    },
    {
      "epoch": 1.5701136978884678,
      "grad_norm": 9.501725196838379,
      "learning_rate": 2.05836491608013e-05,
      "loss": 0.6449,
      "step": 8700
    },
    {
      "epoch": 1.5719184262768453,
      "grad_norm": 13.803583145141602,
      "learning_rate": 2.0572820790471033e-05,
      "loss": 0.6357,
      "step": 8710
    },
    {
      "epoch": 1.5737231546652228,
      "grad_norm": 8.364251136779785,
      "learning_rate": 2.056199242014077e-05,
      "loss": 0.604,
      "step": 8720
    },
    {
      "epoch": 1.5755278830536006,
      "grad_norm": 9.650856971740723,
      "learning_rate": 2.0551164049810503e-05,
      "loss": 0.6536,
      "step": 8730
    },
    {
      "epoch": 1.5773326114419781,
      "grad_norm": 18.864898681640625,
      "learning_rate": 2.0540335679480238e-05,
      "loss": 0.7383,
      "step": 8740
    },
    {
      "epoch": 1.5791373398303556,
      "grad_norm": 10.783267974853516,
      "learning_rate": 2.0529507309149973e-05,
      "loss": 0.6681,
      "step": 8750
    },
    {
      "epoch": 1.5809420682187332,
      "grad_norm": 12.661023139953613,
      "learning_rate": 2.0518678938819708e-05,
      "loss": 0.5765,
      "step": 8760
    },
    {
      "epoch": 1.5827467966071107,
      "grad_norm": 5.155218124389648,
      "learning_rate": 2.0507850568489443e-05,
      "loss": 0.5483,
      "step": 8770
    },
    {
      "epoch": 1.5845515249954882,
      "grad_norm": 5.9485626220703125,
      "learning_rate": 2.0497022198159178e-05,
      "loss": 0.489,
      "step": 8780
    },
    {
      "epoch": 1.5863562533838658,
      "grad_norm": 15.048064231872559,
      "learning_rate": 2.0486193827828913e-05,
      "loss": 0.6975,
      "step": 8790
    },
    {
      "epoch": 1.5881609817722433,
      "grad_norm": 12.410877227783203,
      "learning_rate": 2.0475365457498647e-05,
      "loss": 0.6182,
      "step": 8800
    },
    {
      "epoch": 1.5899657101606208,
      "grad_norm": 10.668712615966797,
      "learning_rate": 2.0464537087168382e-05,
      "loss": 0.5769,
      "step": 8810
    },
    {
      "epoch": 1.5917704385489984,
      "grad_norm": 10.495417594909668,
      "learning_rate": 2.0453708716838114e-05,
      "loss": 0.5382,
      "step": 8820
    },
    {
      "epoch": 1.5935751669373759,
      "grad_norm": 14.661859512329102,
      "learning_rate": 2.044288034650785e-05,
      "loss": 0.5555,
      "step": 8830
    },
    {
      "epoch": 1.5953798953257534,
      "grad_norm": 18.124902725219727,
      "learning_rate": 2.0432051976177587e-05,
      "loss": 0.59,
      "step": 8840
    },
    {
      "epoch": 1.597184623714131,
      "grad_norm": 4.154000759124756,
      "learning_rate": 2.0421223605847322e-05,
      "loss": 0.4263,
      "step": 8850
    },
    {
      "epoch": 1.5989893521025085,
      "grad_norm": 14.274147987365723,
      "learning_rate": 2.0410395235517057e-05,
      "loss": 0.5703,
      "step": 8860
    },
    {
      "epoch": 1.600794080490886,
      "grad_norm": 5.749442100524902,
      "learning_rate": 2.039956686518679e-05,
      "loss": 0.6869,
      "step": 8870
    },
    {
      "epoch": 1.6025988088792635,
      "grad_norm": 12.77068042755127,
      "learning_rate": 2.0388738494856523e-05,
      "loss": 0.6413,
      "step": 8880
    },
    {
      "epoch": 1.604403537267641,
      "grad_norm": 10.55053424835205,
      "learning_rate": 2.0377910124526258e-05,
      "loss": 0.7018,
      "step": 8890
    },
    {
      "epoch": 1.6062082656560188,
      "grad_norm": 19.251161575317383,
      "learning_rate": 2.0367081754195997e-05,
      "loss": 0.6857,
      "step": 8900
    },
    {
      "epoch": 1.6080129940443963,
      "grad_norm": 13.426749229431152,
      "learning_rate": 2.0356253383865728e-05,
      "loss": 0.4848,
      "step": 8910
    },
    {
      "epoch": 1.6098177224327739,
      "grad_norm": 15.507543563842773,
      "learning_rate": 2.0345425013535463e-05,
      "loss": 0.5794,
      "step": 8920
    },
    {
      "epoch": 1.6116224508211514,
      "grad_norm": 23.196918487548828,
      "learning_rate": 2.0334596643205198e-05,
      "loss": 0.5085,
      "step": 8930
    },
    {
      "epoch": 1.613427179209529,
      "grad_norm": 14.040922164916992,
      "learning_rate": 2.0323768272874933e-05,
      "loss": 0.5594,
      "step": 8940
    },
    {
      "epoch": 1.6152319075979065,
      "grad_norm": 13.371670722961426,
      "learning_rate": 2.0312939902544668e-05,
      "loss": 0.6048,
      "step": 8950
    },
    {
      "epoch": 1.6170366359862842,
      "grad_norm": 13.41712474822998,
      "learning_rate": 2.0302111532214403e-05,
      "loss": 0.6383,
      "step": 8960
    },
    {
      "epoch": 1.6188413643746618,
      "grad_norm": 11.732207298278809,
      "learning_rate": 2.0291283161884137e-05,
      "loss": 0.4979,
      "step": 8970
    },
    {
      "epoch": 1.6206460927630393,
      "grad_norm": 7.396420478820801,
      "learning_rate": 2.0280454791553872e-05,
      "loss": 0.7166,
      "step": 8980
    },
    {
      "epoch": 1.6224508211514168,
      "grad_norm": 8.34699821472168,
      "learning_rate": 2.0269626421223607e-05,
      "loss": 0.3901,
      "step": 8990
    },
    {
      "epoch": 1.6242555495397943,
      "grad_norm": 6.696235179901123,
      "learning_rate": 2.025879805089334e-05,
      "loss": 0.6586,
      "step": 9000
    },
    {
      "epoch": 1.6260602779281719,
      "grad_norm": 8.845856666564941,
      "learning_rate": 2.0247969680563074e-05,
      "loss": 0.5906,
      "step": 9010
    },
    {
      "epoch": 1.6278650063165494,
      "grad_norm": 10.415521621704102,
      "learning_rate": 2.0237141310232812e-05,
      "loss": 0.6175,
      "step": 9020
    },
    {
      "epoch": 1.629669734704927,
      "grad_norm": 9.488472938537598,
      "learning_rate": 2.0226312939902547e-05,
      "loss": 0.4634,
      "step": 9030
    },
    {
      "epoch": 1.6314744630933045,
      "grad_norm": 12.574140548706055,
      "learning_rate": 2.021548456957228e-05,
      "loss": 0.3614,
      "step": 9040
    },
    {
      "epoch": 1.633279191481682,
      "grad_norm": 11.798185348510742,
      "learning_rate": 2.0204656199242013e-05,
      "loss": 0.7189,
      "step": 9050
    },
    {
      "epoch": 1.6350839198700595,
      "grad_norm": 13.905467987060547,
      "learning_rate": 2.0193827828911748e-05,
      "loss": 0.5675,
      "step": 9060
    },
    {
      "epoch": 1.636888648258437,
      "grad_norm": 9.042765617370605,
      "learning_rate": 2.0182999458581483e-05,
      "loss": 0.5484,
      "step": 9070
    },
    {
      "epoch": 1.6386933766468146,
      "grad_norm": 10.179503440856934,
      "learning_rate": 2.017217108825122e-05,
      "loss": 0.5727,
      "step": 9080
    },
    {
      "epoch": 1.6404981050351921,
      "grad_norm": 13.058321952819824,
      "learning_rate": 2.0161342717920953e-05,
      "loss": 0.4826,
      "step": 9090
    },
    {
      "epoch": 1.6423028334235696,
      "grad_norm": 9.307167053222656,
      "learning_rate": 2.0150514347590688e-05,
      "loss": 0.6067,
      "step": 9100
    },
    {
      "epoch": 1.6441075618119472,
      "grad_norm": 8.344489097595215,
      "learning_rate": 2.0139685977260423e-05,
      "loss": 0.6549,
      "step": 9110
    },
    {
      "epoch": 1.6459122902003247,
      "grad_norm": 11.184366226196289,
      "learning_rate": 2.0128857606930158e-05,
      "loss": 0.6569,
      "step": 9120
    },
    {
      "epoch": 1.6477170185887025,
      "grad_norm": 13.924400329589844,
      "learning_rate": 2.011802923659989e-05,
      "loss": 0.6472,
      "step": 9130
    },
    {
      "epoch": 1.64952174697708,
      "grad_norm": 13.395200729370117,
      "learning_rate": 2.0107200866269627e-05,
      "loss": 0.6145,
      "step": 9140
    },
    {
      "epoch": 1.6513264753654575,
      "grad_norm": 8.378299713134766,
      "learning_rate": 2.0096372495939362e-05,
      "loss": 0.4316,
      "step": 9150
    },
    {
      "epoch": 1.653131203753835,
      "grad_norm": 8.660252571105957,
      "learning_rate": 2.0085544125609097e-05,
      "loss": 0.5226,
      "step": 9160
    },
    {
      "epoch": 1.6549359321422126,
      "grad_norm": 23.034536361694336,
      "learning_rate": 2.0074715755278832e-05,
      "loss": 0.7058,
      "step": 9170
    },
    {
      "epoch": 1.65674066053059,
      "grad_norm": 10.695709228515625,
      "learning_rate": 2.0063887384948564e-05,
      "loss": 0.5562,
      "step": 9180
    },
    {
      "epoch": 1.6585453889189679,
      "grad_norm": 11.65256404876709,
      "learning_rate": 2.00530590146183e-05,
      "loss": 0.5323,
      "step": 9190
    },
    {
      "epoch": 1.6603501173073454,
      "grad_norm": 9.430832862854004,
      "learning_rate": 2.0042230644288037e-05,
      "loss": 0.5973,
      "step": 9200
    },
    {
      "epoch": 1.662154845695723,
      "grad_norm": 11.279107093811035,
      "learning_rate": 2.0031402273957772e-05,
      "loss": 0.5243,
      "step": 9210
    },
    {
      "epoch": 1.6639595740841004,
      "grad_norm": 4.563845634460449,
      "learning_rate": 2.0020573903627503e-05,
      "loss": 0.6546,
      "step": 9220
    },
    {
      "epoch": 1.665764302472478,
      "grad_norm": 13.468786239624023,
      "learning_rate": 2.0009745533297238e-05,
      "loss": 0.5307,
      "step": 9230
    },
    {
      "epoch": 1.6675690308608555,
      "grad_norm": 14.995895385742188,
      "learning_rate": 1.9998917162966973e-05,
      "loss": 0.5657,
      "step": 9240
    },
    {
      "epoch": 1.669373759249233,
      "grad_norm": 20.75823974609375,
      "learning_rate": 1.9988088792636708e-05,
      "loss": 0.497,
      "step": 9250
    },
    {
      "epoch": 1.6711784876376106,
      "grad_norm": 22.40656852722168,
      "learning_rate": 1.9977260422306446e-05,
      "loss": 0.5947,
      "step": 9260
    },
    {
      "epoch": 1.672983216025988,
      "grad_norm": 14.714599609375,
      "learning_rate": 1.9966432051976178e-05,
      "loss": 0.5551,
      "step": 9270
    },
    {
      "epoch": 1.6747879444143656,
      "grad_norm": 15.4361572265625,
      "learning_rate": 1.9955603681645913e-05,
      "loss": 0.6097,
      "step": 9280
    },
    {
      "epoch": 1.6765926728027432,
      "grad_norm": 10.965811729431152,
      "learning_rate": 1.9944775311315648e-05,
      "loss": 0.5278,
      "step": 9290
    },
    {
      "epoch": 1.6783974011911207,
      "grad_norm": 8.80572509765625,
      "learning_rate": 1.9933946940985383e-05,
      "loss": 0.5867,
      "step": 9300
    },
    {
      "epoch": 1.6802021295794982,
      "grad_norm": 11.360727310180664,
      "learning_rate": 1.9923118570655114e-05,
      "loss": 0.7195,
      "step": 9310
    },
    {
      "epoch": 1.6820068579678757,
      "grad_norm": 15.817604064941406,
      "learning_rate": 1.9912290200324852e-05,
      "loss": 0.6451,
      "step": 9320
    },
    {
      "epoch": 1.6838115863562533,
      "grad_norm": 27.74163055419922,
      "learning_rate": 1.9901461829994587e-05,
      "loss": 0.5807,
      "step": 9330
    },
    {
      "epoch": 1.6856163147446308,
      "grad_norm": 10.798196792602539,
      "learning_rate": 1.9890633459664322e-05,
      "loss": 0.7132,
      "step": 9340
    },
    {
      "epoch": 1.6874210431330083,
      "grad_norm": 7.605077743530273,
      "learning_rate": 1.9879805089334057e-05,
      "loss": 0.5781,
      "step": 9350
    },
    {
      "epoch": 1.6892257715213859,
      "grad_norm": 6.5233154296875,
      "learning_rate": 1.986897671900379e-05,
      "loss": 0.5786,
      "step": 9360
    },
    {
      "epoch": 1.6910304999097636,
      "grad_norm": 8.606542587280273,
      "learning_rate": 1.9858148348673523e-05,
      "loss": 0.5518,
      "step": 9370
    },
    {
      "epoch": 1.6928352282981411,
      "grad_norm": 12.514430046081543,
      "learning_rate": 1.9847319978343262e-05,
      "loss": 0.635,
      "step": 9380
    },
    {
      "epoch": 1.6946399566865187,
      "grad_norm": 12.528034210205078,
      "learning_rate": 1.9836491608012997e-05,
      "loss": 0.6278,
      "step": 9390
    },
    {
      "epoch": 1.6964446850748962,
      "grad_norm": 17.949460983276367,
      "learning_rate": 1.9825663237682728e-05,
      "loss": 0.6805,
      "step": 9400
    },
    {
      "epoch": 1.6982494134632737,
      "grad_norm": 30.03431510925293,
      "learning_rate": 1.9814834867352463e-05,
      "loss": 0.628,
      "step": 9410
    },
    {
      "epoch": 1.7000541418516515,
      "grad_norm": 24.697996139526367,
      "learning_rate": 1.9804006497022198e-05,
      "loss": 0.6523,
      "step": 9420
    },
    {
      "epoch": 1.701858870240029,
      "grad_norm": 5.749072551727295,
      "learning_rate": 1.9793178126691933e-05,
      "loss": 0.5827,
      "step": 9430
    },
    {
      "epoch": 1.7036635986284066,
      "grad_norm": 22.133203506469727,
      "learning_rate": 1.978234975636167e-05,
      "loss": 0.5297,
      "step": 9440
    },
    {
      "epoch": 1.705468327016784,
      "grad_norm": 19.00065040588379,
      "learning_rate": 1.9771521386031403e-05,
      "loss": 0.5426,
      "step": 9450
    },
    {
      "epoch": 1.7072730554051616,
      "grad_norm": 16.296667098999023,
      "learning_rate": 1.9760693015701138e-05,
      "loss": 0.7662,
      "step": 9460
    },
    {
      "epoch": 1.7090777837935391,
      "grad_norm": 10.608819007873535,
      "learning_rate": 1.9749864645370872e-05,
      "loss": 0.6846,
      "step": 9470
    },
    {
      "epoch": 1.7108825121819167,
      "grad_norm": 11.020751953125,
      "learning_rate": 1.9739036275040607e-05,
      "loss": 0.6665,
      "step": 9480
    },
    {
      "epoch": 1.7126872405702942,
      "grad_norm": 10.703481674194336,
      "learning_rate": 1.972820790471034e-05,
      "loss": 0.6626,
      "step": 9490
    },
    {
      "epoch": 1.7144919689586717,
      "grad_norm": 15.838227272033691,
      "learning_rate": 1.9717379534380077e-05,
      "loss": 0.5179,
      "step": 9500
    },
    {
      "epoch": 1.7162966973470493,
      "grad_norm": 10.466699600219727,
      "learning_rate": 1.9706551164049812e-05,
      "loss": 0.5971,
      "step": 9510
    },
    {
      "epoch": 1.7181014257354268,
      "grad_norm": 10.43808650970459,
      "learning_rate": 1.9695722793719547e-05,
      "loss": 0.6944,
      "step": 9520
    },
    {
      "epoch": 1.7199061541238043,
      "grad_norm": 11.947582244873047,
      "learning_rate": 1.9684894423389282e-05,
      "loss": 0.5731,
      "step": 9530
    },
    {
      "epoch": 1.7217108825121819,
      "grad_norm": 10.556563377380371,
      "learning_rate": 1.9674066053059013e-05,
      "loss": 0.5441,
      "step": 9540
    },
    {
      "epoch": 1.7235156109005594,
      "grad_norm": 9.691974639892578,
      "learning_rate": 1.966323768272875e-05,
      "loss": 0.6309,
      "step": 9550
    },
    {
      "epoch": 1.725320339288937,
      "grad_norm": 8.507993698120117,
      "learning_rate": 1.9652409312398487e-05,
      "loss": 0.5374,
      "step": 9560
    },
    {
      "epoch": 1.7271250676773144,
      "grad_norm": 36.32011795043945,
      "learning_rate": 1.964158094206822e-05,
      "loss": 0.6124,
      "step": 9570
    },
    {
      "epoch": 1.728929796065692,
      "grad_norm": 12.453530311584473,
      "learning_rate": 1.9630752571737953e-05,
      "loss": 0.5872,
      "step": 9580
    },
    {
      "epoch": 1.7307345244540695,
      "grad_norm": 5.901987552642822,
      "learning_rate": 1.9619924201407688e-05,
      "loss": 0.6083,
      "step": 9590
    },
    {
      "epoch": 1.7325392528424473,
      "grad_norm": 18.676624298095703,
      "learning_rate": 1.9609095831077423e-05,
      "loss": 0.6254,
      "step": 9600
    },
    {
      "epoch": 1.7343439812308248,
      "grad_norm": 20.68105697631836,
      "learning_rate": 1.9598267460747158e-05,
      "loss": 0.5931,
      "step": 9610
    },
    {
      "epoch": 1.7361487096192023,
      "grad_norm": 11.518879890441895,
      "learning_rate": 1.9587439090416893e-05,
      "loss": 0.6891,
      "step": 9620
    },
    {
      "epoch": 1.7379534380075798,
      "grad_norm": 4.930089473724365,
      "learning_rate": 1.9576610720086628e-05,
      "loss": 0.623,
      "step": 9630
    },
    {
      "epoch": 1.7397581663959574,
      "grad_norm": 9.14688491821289,
      "learning_rate": 1.9565782349756362e-05,
      "loss": 0.5661,
      "step": 9640
    },
    {
      "epoch": 1.741562894784335,
      "grad_norm": 19.140480041503906,
      "learning_rate": 1.9554953979426097e-05,
      "loss": 0.5875,
      "step": 9650
    },
    {
      "epoch": 1.7433676231727127,
      "grad_norm": 6.703503608703613,
      "learning_rate": 1.9544125609095832e-05,
      "loss": 0.6236,
      "step": 9660
    },
    {
      "epoch": 1.7451723515610902,
      "grad_norm": 12.49268913269043,
      "learning_rate": 1.9533297238765564e-05,
      "loss": 0.6093,
      "step": 9670
    },
    {
      "epoch": 1.7469770799494677,
      "grad_norm": 14.04822826385498,
      "learning_rate": 1.9522468868435302e-05,
      "loss": 0.574,
      "step": 9680
    },
    {
      "epoch": 1.7487818083378452,
      "grad_norm": 15.20715618133545,
      "learning_rate": 1.9511640498105037e-05,
      "loss": 0.5534,
      "step": 9690
    },
    {
      "epoch": 1.7505865367262228,
      "grad_norm": 13.765486717224121,
      "learning_rate": 1.9500812127774772e-05,
      "loss": 0.6209,
      "step": 9700
    },
    {
      "epoch": 1.7523912651146003,
      "grad_norm": 16.146787643432617,
      "learning_rate": 1.9489983757444503e-05,
      "loss": 0.6543,
      "step": 9710
    },
    {
      "epoch": 1.7541959935029778,
      "grad_norm": 6.577858924865723,
      "learning_rate": 1.947915538711424e-05,
      "loss": 0.5769,
      "step": 9720
    },
    {
      "epoch": 1.7560007218913554,
      "grad_norm": 17.63426399230957,
      "learning_rate": 1.9468327016783973e-05,
      "loss": 0.4894,
      "step": 9730
    },
    {
      "epoch": 1.757805450279733,
      "grad_norm": 10.987525939941406,
      "learning_rate": 1.945749864645371e-05,
      "loss": 0.578,
      "step": 9740
    },
    {
      "epoch": 1.7596101786681104,
      "grad_norm": 8.206835746765137,
      "learning_rate": 1.9446670276123446e-05,
      "loss": 0.7379,
      "step": 9750
    },
    {
      "epoch": 1.761414907056488,
      "grad_norm": 9.371650695800781,
      "learning_rate": 1.9435841905793178e-05,
      "loss": 0.6123,
      "step": 9760
    },
    {
      "epoch": 1.7632196354448655,
      "grad_norm": 10.97057819366455,
      "learning_rate": 1.9425013535462913e-05,
      "loss": 0.4599,
      "step": 9770
    },
    {
      "epoch": 1.765024363833243,
      "grad_norm": 10.502471923828125,
      "learning_rate": 1.9414185165132648e-05,
      "loss": 0.6209,
      "step": 9780
    },
    {
      "epoch": 1.7668290922216205,
      "grad_norm": 11.570528030395508,
      "learning_rate": 1.9403356794802383e-05,
      "loss": 0.5152,
      "step": 9790
    },
    {
      "epoch": 1.768633820609998,
      "grad_norm": 8.344690322875977,
      "learning_rate": 1.9392528424472118e-05,
      "loss": 0.5161,
      "step": 9800
    },
    {
      "epoch": 1.7704385489983756,
      "grad_norm": 14.871609687805176,
      "learning_rate": 1.9381700054141852e-05,
      "loss": 0.5869,
      "step": 9810
    },
    {
      "epoch": 1.7722432773867531,
      "grad_norm": 16.22027015686035,
      "learning_rate": 1.9370871683811587e-05,
      "loss": 0.5522,
      "step": 9820
    },
    {
      "epoch": 1.7740480057751309,
      "grad_norm": 8.802470207214355,
      "learning_rate": 1.9360043313481322e-05,
      "loss": 0.5855,
      "step": 9830
    },
    {
      "epoch": 1.7758527341635084,
      "grad_norm": 11.745285987854004,
      "learning_rate": 1.9349214943151057e-05,
      "loss": 0.6624,
      "step": 9840
    },
    {
      "epoch": 1.777657462551886,
      "grad_norm": 14.73862361907959,
      "learning_rate": 1.933838657282079e-05,
      "loss": 0.5601,
      "step": 9850
    },
    {
      "epoch": 1.7794621909402635,
      "grad_norm": 14.22424030303955,
      "learning_rate": 1.9327558202490527e-05,
      "loss": 0.439,
      "step": 9860
    },
    {
      "epoch": 1.781266919328641,
      "grad_norm": 14.364051818847656,
      "learning_rate": 1.9316729832160262e-05,
      "loss": 0.5638,
      "step": 9870
    },
    {
      "epoch": 1.7830716477170185,
      "grad_norm": 16.391345977783203,
      "learning_rate": 1.9305901461829997e-05,
      "loss": 0.4958,
      "step": 9880
    },
    {
      "epoch": 1.7848763761053963,
      "grad_norm": 6.830802917480469,
      "learning_rate": 1.9295073091499728e-05,
      "loss": 0.5111,
      "step": 9890
    },
    {
      "epoch": 1.7866811044937738,
      "grad_norm": 9.416266441345215,
      "learning_rate": 1.9284244721169463e-05,
      "loss": 0.6039,
      "step": 9900
    },
    {
      "epoch": 1.7884858328821513,
      "grad_norm": 8.077129364013672,
      "learning_rate": 1.9273416350839198e-05,
      "loss": 0.5221,
      "step": 9910
    },
    {
      "epoch": 1.7902905612705289,
      "grad_norm": 13.369168281555176,
      "learning_rate": 1.9262587980508936e-05,
      "loss": 0.6923,
      "step": 9920
    },
    {
      "epoch": 1.7920952896589064,
      "grad_norm": 15.725174903869629,
      "learning_rate": 1.925175961017867e-05,
      "loss": 0.553,
      "step": 9930
    },
    {
      "epoch": 1.793900018047284,
      "grad_norm": 17.847368240356445,
      "learning_rate": 1.9240931239848403e-05,
      "loss": 0.5611,
      "step": 9940
    },
    {
      "epoch": 1.7957047464356615,
      "grad_norm": 16.245817184448242,
      "learning_rate": 1.9230102869518138e-05,
      "loss": 0.7012,
      "step": 9950
    },
    {
      "epoch": 1.797509474824039,
      "grad_norm": 10.991676330566406,
      "learning_rate": 1.9219274499187873e-05,
      "loss": 0.6575,
      "step": 9960
    },
    {
      "epoch": 1.7993142032124165,
      "grad_norm": 8.193412780761719,
      "learning_rate": 1.9208446128857608e-05,
      "loss": 0.6001,
      "step": 9970
    },
    {
      "epoch": 1.801118931600794,
      "grad_norm": 21.92428207397461,
      "learning_rate": 1.919870059556037e-05,
      "loss": 0.5947,
      "step": 9980
    },
    {
      "epoch": 1.8029236599891716,
      "grad_norm": 11.060526847839355,
      "learning_rate": 1.9187872225230106e-05,
      "loss": 0.4934,
      "step": 9990
    },
    {
      "epoch": 1.8047283883775491,
      "grad_norm": 12.484105110168457,
      "learning_rate": 1.9177043854899837e-05,
      "loss": 0.4657,
      "step": 10000
    },
    {
      "epoch": 1.8065331167659266,
      "grad_norm": 17.30760955810547,
      "learning_rate": 1.9166215484569572e-05,
      "loss": 0.6734,
      "step": 10010
    },
    {
      "epoch": 1.8083378451543042,
      "grad_norm": 16.03730010986328,
      "learning_rate": 1.9155387114239307e-05,
      "loss": 0.5885,
      "step": 10020
    },
    {
      "epoch": 1.8101425735426817,
      "grad_norm": 20.476320266723633,
      "learning_rate": 1.9144558743909042e-05,
      "loss": 0.5252,
      "step": 10030
    },
    {
      "epoch": 1.8119473019310592,
      "grad_norm": 18.336612701416016,
      "learning_rate": 1.9133730373578777e-05,
      "loss": 0.5695,
      "step": 10040
    },
    {
      "epoch": 1.8137520303194368,
      "grad_norm": 13.227194786071777,
      "learning_rate": 1.9122902003248512e-05,
      "loss": 0.6563,
      "step": 10050
    },
    {
      "epoch": 1.8155567587078145,
      "grad_norm": 12.102712631225586,
      "learning_rate": 1.9112073632918247e-05,
      "loss": 0.522,
      "step": 10060
    },
    {
      "epoch": 1.817361487096192,
      "grad_norm": 18.499034881591797,
      "learning_rate": 1.9101245262587982e-05,
      "loss": 0.6833,
      "step": 10070
    },
    {
      "epoch": 1.8191662154845696,
      "grad_norm": 10.21777629852295,
      "learning_rate": 1.9090416892257713e-05,
      "loss": 0.6699,
      "step": 10080
    },
    {
      "epoch": 1.8209709438729471,
      "grad_norm": 7.167752265930176,
      "learning_rate": 1.9079588521927448e-05,
      "loss": 0.5064,
      "step": 10090
    },
    {
      "epoch": 1.8227756722613246,
      "grad_norm": 10.774567604064941,
      "learning_rate": 1.9068760151597186e-05,
      "loss": 0.6533,
      "step": 10100
    },
    {
      "epoch": 1.8245804006497022,
      "grad_norm": 8.39647102355957,
      "learning_rate": 1.905793178126692e-05,
      "loss": 0.5414,
      "step": 10110
    },
    {
      "epoch": 1.82638512903808,
      "grad_norm": 11.110129356384277,
      "learning_rate": 1.9047103410936656e-05,
      "loss": 0.589,
      "step": 10120
    },
    {
      "epoch": 1.8281898574264575,
      "grad_norm": 16.49246597290039,
      "learning_rate": 1.9036275040606388e-05,
      "loss": 0.6621,
      "step": 10130
    },
    {
      "epoch": 1.829994585814835,
      "grad_norm": 12.607954978942871,
      "learning_rate": 1.9025446670276123e-05,
      "loss": 0.5916,
      "step": 10140
    },
    {
      "epoch": 1.8317993142032125,
      "grad_norm": 9.895393371582031,
      "learning_rate": 1.9014618299945858e-05,
      "loss": 0.6849,
      "step": 10150
    },
    {
      "epoch": 1.83360404259159,
      "grad_norm": 13.668923377990723,
      "learning_rate": 1.9003789929615596e-05,
      "loss": 0.6409,
      "step": 10160
    },
    {
      "epoch": 1.8354087709799676,
      "grad_norm": 19.507999420166016,
      "learning_rate": 1.8992961559285327e-05,
      "loss": 0.7629,
      "step": 10170
    },
    {
      "epoch": 1.837213499368345,
      "grad_norm": 7.77689790725708,
      "learning_rate": 1.8982133188955062e-05,
      "loss": 0.4224,
      "step": 10180
    },
    {
      "epoch": 1.8390182277567226,
      "grad_norm": 11.626787185668945,
      "learning_rate": 1.8971304818624797e-05,
      "loss": 0.6348,
      "step": 10190
    },
    {
      "epoch": 1.8408229561451002,
      "grad_norm": 8.705649375915527,
      "learning_rate": 1.8960476448294532e-05,
      "loss": 0.5397,
      "step": 10200
    },
    {
      "epoch": 1.8426276845334777,
      "grad_norm": 5.986445903778076,
      "learning_rate": 1.8949648077964267e-05,
      "loss": 0.5903,
      "step": 10210
    },
    {
      "epoch": 1.8444324129218552,
      "grad_norm": 7.002461910247803,
      "learning_rate": 1.8938819707634002e-05,
      "loss": 0.6136,
      "step": 10220
    },
    {
      "epoch": 1.8462371413102328,
      "grad_norm": 14.874385833740234,
      "learning_rate": 1.8927991337303737e-05,
      "loss": 0.5683,
      "step": 10230
    },
    {
      "epoch": 1.8480418696986103,
      "grad_norm": 6.380557060241699,
      "learning_rate": 1.8917162966973472e-05,
      "loss": 0.6612,
      "step": 10240
    },
    {
      "epoch": 1.8498465980869878,
      "grad_norm": 19.70960807800293,
      "learning_rate": 1.8906334596643207e-05,
      "loss": 0.6594,
      "step": 10250
    },
    {
      "epoch": 1.8516513264753653,
      "grad_norm": 12.266456604003906,
      "learning_rate": 1.8895506226312938e-05,
      "loss": 0.5816,
      "step": 10260
    },
    {
      "epoch": 1.8534560548637429,
      "grad_norm": 14.76661491394043,
      "learning_rate": 1.8884677855982673e-05,
      "loss": 0.5487,
      "step": 10270
    },
    {
      "epoch": 1.8552607832521204,
      "grad_norm": 15.809244155883789,
      "learning_rate": 1.887384948565241e-05,
      "loss": 0.5025,
      "step": 10280
    },
    {
      "epoch": 1.8570655116404982,
      "grad_norm": 11.794473648071289,
      "learning_rate": 1.8863021115322146e-05,
      "loss": 0.5303,
      "step": 10290
    },
    {
      "epoch": 1.8588702400288757,
      "grad_norm": 11.106308937072754,
      "learning_rate": 1.885219274499188e-05,
      "loss": 0.5455,
      "step": 10300
    },
    {
      "epoch": 1.8606749684172532,
      "grad_norm": 12.884284973144531,
      "learning_rate": 1.8841364374661613e-05,
      "loss": 0.4823,
      "step": 10310
    },
    {
      "epoch": 1.8624796968056307,
      "grad_norm": 16.49993324279785,
      "learning_rate": 1.8830536004331348e-05,
      "loss": 0.5362,
      "step": 10320
    },
    {
      "epoch": 1.8642844251940083,
      "grad_norm": 10.03333854675293,
      "learning_rate": 1.8819707634001082e-05,
      "loss": 0.5741,
      "step": 10330
    },
    {
      "epoch": 1.8660891535823858,
      "grad_norm": 19.250743865966797,
      "learning_rate": 1.880887926367082e-05,
      "loss": 0.6896,
      "step": 10340
    },
    {
      "epoch": 1.8678938819707636,
      "grad_norm": 10.785037994384766,
      "learning_rate": 1.8798050893340552e-05,
      "loss": 0.6538,
      "step": 10350
    },
    {
      "epoch": 1.869698610359141,
      "grad_norm": 7.509262561798096,
      "learning_rate": 1.8787222523010287e-05,
      "loss": 0.5619,
      "step": 10360
    },
    {
      "epoch": 1.8715033387475186,
      "grad_norm": 12.4869384765625,
      "learning_rate": 1.8776394152680022e-05,
      "loss": 0.6247,
      "step": 10370
    },
    {
      "epoch": 1.8733080671358961,
      "grad_norm": 20.31329345703125,
      "learning_rate": 1.8765565782349757e-05,
      "loss": 0.5263,
      "step": 10380
    },
    {
      "epoch": 1.8751127955242737,
      "grad_norm": 12.991984367370605,
      "learning_rate": 1.8754737412019492e-05,
      "loss": 0.6606,
      "step": 10390
    },
    {
      "epoch": 1.8769175239126512,
      "grad_norm": 9.280177116394043,
      "learning_rate": 1.8743909041689227e-05,
      "loss": 0.5086,
      "step": 10400
    },
    {
      "epoch": 1.8787222523010287,
      "grad_norm": 16.36005210876465,
      "learning_rate": 1.873308067135896e-05,
      "loss": 0.6279,
      "step": 10410
    },
    {
      "epoch": 1.8805269806894063,
      "grad_norm": 34.05043029785156,
      "learning_rate": 1.8722252301028697e-05,
      "loss": 0.4515,
      "step": 10420
    },
    {
      "epoch": 1.8823317090777838,
      "grad_norm": 12.912113189697266,
      "learning_rate": 1.871142393069843e-05,
      "loss": 0.5488,
      "step": 10430
    },
    {
      "epoch": 1.8841364374661613,
      "grad_norm": 15.932597160339355,
      "learning_rate": 1.8700595560368163e-05,
      "loss": 0.593,
      "step": 10440
    },
    {
      "epoch": 1.8859411658545389,
      "grad_norm": 13.35715103149414,
      "learning_rate": 1.8689767190037898e-05,
      "loss": 0.4539,
      "step": 10450
    },
    {
      "epoch": 1.8877458942429164,
      "grad_norm": 12.608091354370117,
      "learning_rate": 1.8678938819707636e-05,
      "loss": 0.7964,
      "step": 10460
    },
    {
      "epoch": 1.889550622631294,
      "grad_norm": 14.285152435302734,
      "learning_rate": 1.866811044937737e-05,
      "loss": 0.5511,
      "step": 10470
    },
    {
      "epoch": 1.8913553510196714,
      "grad_norm": 11.513044357299805,
      "learning_rate": 1.8657282079047106e-05,
      "loss": 0.6285,
      "step": 10480
    },
    {
      "epoch": 1.893160079408049,
      "grad_norm": 20.63409423828125,
      "learning_rate": 1.8646453708716838e-05,
      "loss": 0.5109,
      "step": 10490
    },
    {
      "epoch": 1.8949648077964265,
      "grad_norm": 18.358200073242188,
      "learning_rate": 1.8635625338386572e-05,
      "loss": 0.5375,
      "step": 10500
    },
    {
      "epoch": 1.896769536184804,
      "grad_norm": 9.297993659973145,
      "learning_rate": 1.8624796968056307e-05,
      "loss": 0.7994,
      "step": 10510
    },
    {
      "epoch": 1.8985742645731818,
      "grad_norm": 9.006786346435547,
      "learning_rate": 1.8613968597726046e-05,
      "loss": 0.5961,
      "step": 10520
    },
    {
      "epoch": 1.9003789929615593,
      "grad_norm": 8.63279914855957,
      "learning_rate": 1.8603140227395777e-05,
      "loss": 0.4899,
      "step": 10530
    },
    {
      "epoch": 1.9021837213499369,
      "grad_norm": 13.082555770874023,
      "learning_rate": 1.8592311857065512e-05,
      "loss": 0.5273,
      "step": 10540
    },
    {
      "epoch": 1.9039884497383144,
      "grad_norm": 11.77424144744873,
      "learning_rate": 1.8581483486735247e-05,
      "loss": 0.4727,
      "step": 10550
    },
    {
      "epoch": 1.905793178126692,
      "grad_norm": 12.945537567138672,
      "learning_rate": 1.8570655116404982e-05,
      "loss": 0.5332,
      "step": 10560
    },
    {
      "epoch": 1.9075979065150694,
      "grad_norm": 15.131534576416016,
      "learning_rate": 1.8559826746074717e-05,
      "loss": 0.8088,
      "step": 10570
    },
    {
      "epoch": 1.9094026349034472,
      "grad_norm": 13.640767097473145,
      "learning_rate": 1.854899837574445e-05,
      "loss": 0.6066,
      "step": 10580
    },
    {
      "epoch": 1.9112073632918247,
      "grad_norm": 14.57863998413086,
      "learning_rate": 1.8538170005414187e-05,
      "loss": 0.4996,
      "step": 10590
    },
    {
      "epoch": 1.9130120916802023,
      "grad_norm": 4.8403706550598145,
      "learning_rate": 1.852734163508392e-05,
      "loss": 0.4823,
      "step": 10600
    },
    {
      "epoch": 1.9148168200685798,
      "grad_norm": 15.231427192687988,
      "learning_rate": 1.8516513264753656e-05,
      "loss": 0.5472,
      "step": 10610
    },
    {
      "epoch": 1.9166215484569573,
      "grad_norm": 14.97050952911377,
      "learning_rate": 1.8505684894423388e-05,
      "loss": 0.5501,
      "step": 10620
    },
    {
      "epoch": 1.9184262768453348,
      "grad_norm": 12.499173164367676,
      "learning_rate": 1.8494856524093123e-05,
      "loss": 0.6234,
      "step": 10630
    },
    {
      "epoch": 1.9202310052337124,
      "grad_norm": 12.163108825683594,
      "learning_rate": 1.848402815376286e-05,
      "loss": 0.4594,
      "step": 10640
    },
    {
      "epoch": 1.92203573362209,
      "grad_norm": 6.891327857971191,
      "learning_rate": 1.8473199783432596e-05,
      "loss": 0.6264,
      "step": 10650
    },
    {
      "epoch": 1.9238404620104674,
      "grad_norm": 16.253292083740234,
      "learning_rate": 1.8462371413102328e-05,
      "loss": 0.7121,
      "step": 10660
    },
    {
      "epoch": 1.925645190398845,
      "grad_norm": 13.41402816772461,
      "learning_rate": 1.8451543042772062e-05,
      "loss": 0.639,
      "step": 10670
    },
    {
      "epoch": 1.9274499187872225,
      "grad_norm": 9.503134727478027,
      "learning_rate": 1.8440714672441797e-05,
      "loss": 0.5173,
      "step": 10680
    },
    {
      "epoch": 1.9292546471756,
      "grad_norm": 5.931207656860352,
      "learning_rate": 1.8429886302111532e-05,
      "loss": 0.7031,
      "step": 10690
    },
    {
      "epoch": 1.9310593755639776,
      "grad_norm": 12.300132751464844,
      "learning_rate": 1.8419057931781267e-05,
      "loss": 0.5945,
      "step": 10700
    },
    {
      "epoch": 1.932864103952355,
      "grad_norm": 20.17831802368164,
      "learning_rate": 1.8408229561451002e-05,
      "loss": 0.6405,
      "step": 10710
    },
    {
      "epoch": 1.9346688323407326,
      "grad_norm": 10.16331672668457,
      "learning_rate": 1.8397401191120737e-05,
      "loss": 0.6796,
      "step": 10720
    },
    {
      "epoch": 1.9364735607291101,
      "grad_norm": 8.268224716186523,
      "learning_rate": 1.8386572820790472e-05,
      "loss": 0.593,
      "step": 10730
    },
    {
      "epoch": 1.9382782891174877,
      "grad_norm": 15.977437973022461,
      "learning_rate": 1.8375744450460207e-05,
      "loss": 0.471,
      "step": 10740
    },
    {
      "epoch": 1.9400830175058652,
      "grad_norm": 13.906164169311523,
      "learning_rate": 1.8364916080129938e-05,
      "loss": 0.6242,
      "step": 10750
    },
    {
      "epoch": 1.941887745894243,
      "grad_norm": 18.434843063354492,
      "learning_rate": 1.8354087709799673e-05,
      "loss": 0.4703,
      "step": 10760
    },
    {
      "epoch": 1.9436924742826205,
      "grad_norm": 10.123899459838867,
      "learning_rate": 1.834325933946941e-05,
      "loss": 0.4022,
      "step": 10770
    },
    {
      "epoch": 1.945497202670998,
      "grad_norm": 8.844649314880371,
      "learning_rate": 1.8332430969139146e-05,
      "loss": 0.6818,
      "step": 10780
    },
    {
      "epoch": 1.9473019310593755,
      "grad_norm": 17.131046295166016,
      "learning_rate": 1.832160259880888e-05,
      "loss": 0.4195,
      "step": 10790
    },
    {
      "epoch": 1.949106659447753,
      "grad_norm": 10.972811698913574,
      "learning_rate": 1.8310774228478613e-05,
      "loss": 0.5776,
      "step": 10800
    },
    {
      "epoch": 1.9509113878361308,
      "grad_norm": 9.149811744689941,
      "learning_rate": 1.8299945858148348e-05,
      "loss": 0.4827,
      "step": 10810
    },
    {
      "epoch": 1.9527161162245084,
      "grad_norm": 17.25541877746582,
      "learning_rate": 1.8289117487818083e-05,
      "loss": 0.7568,
      "step": 10820
    },
    {
      "epoch": 1.9545208446128859,
      "grad_norm": 13.883417129516602,
      "learning_rate": 1.827828911748782e-05,
      "loss": 0.5086,
      "step": 10830
    },
    {
      "epoch": 1.9563255730012634,
      "grad_norm": 7.8636860847473145,
      "learning_rate": 1.8267460747157552e-05,
      "loss": 0.7253,
      "step": 10840
    },
    {
      "epoch": 1.958130301389641,
      "grad_norm": 18.93462371826172,
      "learning_rate": 1.8256632376827287e-05,
      "loss": 0.499,
      "step": 10850
    },
    {
      "epoch": 1.9599350297780185,
      "grad_norm": 9.697979927062988,
      "learning_rate": 1.8245804006497022e-05,
      "loss": 0.6375,
      "step": 10860
    },
    {
      "epoch": 1.961739758166396,
      "grad_norm": 7.992811679840088,
      "learning_rate": 1.8234975636166757e-05,
      "loss": 0.5266,
      "step": 10870
    },
    {
      "epoch": 1.9635444865547735,
      "grad_norm": 14.742273330688477,
      "learning_rate": 1.8224147265836492e-05,
      "loss": 0.6237,
      "step": 10880
    },
    {
      "epoch": 1.965349214943151,
      "grad_norm": 9.530567169189453,
      "learning_rate": 1.8213318895506227e-05,
      "loss": 0.6447,
      "step": 10890
    },
    {
      "epoch": 1.9671539433315286,
      "grad_norm": 8.01412296295166,
      "learning_rate": 1.8202490525175962e-05,
      "loss": 0.629,
      "step": 10900
    },
    {
      "epoch": 1.9689586717199061,
      "grad_norm": 10.860625267028809,
      "learning_rate": 1.8191662154845697e-05,
      "loss": 0.6578,
      "step": 10910
    },
    {
      "epoch": 1.9707634001082837,
      "grad_norm": 12.270929336547852,
      "learning_rate": 1.818083378451543e-05,
      "loss": 0.7987,
      "step": 10920
    },
    {
      "epoch": 1.9725681284966612,
      "grad_norm": 5.59194803237915,
      "learning_rate": 1.8170005414185163e-05,
      "loss": 0.5414,
      "step": 10930
    },
    {
      "epoch": 1.9743728568850387,
      "grad_norm": 4.930305480957031,
      "learning_rate": 1.8159177043854898e-05,
      "loss": 0.5739,
      "step": 10940
    },
    {
      "epoch": 1.9761775852734162,
      "grad_norm": 10.935951232910156,
      "learning_rate": 1.8148348673524636e-05,
      "loss": 0.4927,
      "step": 10950
    },
    {
      "epoch": 1.9779823136617938,
      "grad_norm": 9.540507316589355,
      "learning_rate": 1.813752030319437e-05,
      "loss": 0.6672,
      "step": 10960
    },
    {
      "epoch": 1.9797870420501713,
      "grad_norm": 8.736026763916016,
      "learning_rate": 1.8126691932864106e-05,
      "loss": 0.593,
      "step": 10970
    },
    {
      "epoch": 1.9815917704385488,
      "grad_norm": 24.819780349731445,
      "learning_rate": 1.8115863562533838e-05,
      "loss": 0.6372,
      "step": 10980
    },
    {
      "epoch": 1.9833964988269266,
      "grad_norm": 10.057354927062988,
      "learning_rate": 1.8105035192203573e-05,
      "loss": 0.5539,
      "step": 10990
    },
    {
      "epoch": 1.9852012272153041,
      "grad_norm": 11.762527465820312,
      "learning_rate": 1.8094206821873307e-05,
      "loss": 0.5022,
      "step": 11000
    },
    {
      "epoch": 1.9870059556036817,
      "grad_norm": 8.454017639160156,
      "learning_rate": 1.8083378451543046e-05,
      "loss": 0.5735,
      "step": 11010
    },
    {
      "epoch": 1.9888106839920592,
      "grad_norm": 21.509700775146484,
      "learning_rate": 1.8072550081212777e-05,
      "loss": 0.6427,
      "step": 11020
    },
    {
      "epoch": 1.9906154123804367,
      "grad_norm": 6.788698196411133,
      "learning_rate": 1.8061721710882512e-05,
      "loss": 0.5273,
      "step": 11030
    },
    {
      "epoch": 1.9924201407688142,
      "grad_norm": 17.86650848388672,
      "learning_rate": 1.8050893340552247e-05,
      "loss": 0.5405,
      "step": 11040
    },
    {
      "epoch": 1.994224869157192,
      "grad_norm": 12.710417747497559,
      "learning_rate": 1.8040064970221982e-05,
      "loss": 0.5174,
      "step": 11050
    },
    {
      "epoch": 1.9960295975455695,
      "grad_norm": 12.440292358398438,
      "learning_rate": 1.8029236599891717e-05,
      "loss": 0.5778,
      "step": 11060
    },
    {
      "epoch": 1.997834325933947,
      "grad_norm": 14.405318260192871,
      "learning_rate": 1.8018408229561452e-05,
      "loss": 0.419,
      "step": 11070
    },
    {
      "epoch": 1.9996390543223246,
      "grad_norm": 23.410919189453125,
      "learning_rate": 1.8007579859231187e-05,
      "loss": 0.5601,
      "step": 11080
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.7854081126201327,
      "eval_f1": 0.714883567645202,
      "eval_loss": 0.6612387895584106,
      "eval_precision": 0.7050941258857794,
      "eval_recall": 0.7298758424735455,
      "eval_runtime": 160.0795,
      "eval_samples_per_second": 138.45,
      "eval_steps_per_second": 8.658,
      "step": 11082
    },
    {
      "epoch": 2.001443782710702,
      "grad_norm": 12.161978721618652,
      "learning_rate": 1.799675148890092e-05,
      "loss": 0.4259,
      "step": 11090
    },
    {
      "epoch": 2.0032485110990796,
      "grad_norm": 6.866259574890137,
      "learning_rate": 1.7985923118570657e-05,
      "loss": 0.5194,
      "step": 11100
    },
    {
      "epoch": 2.005053239487457,
      "grad_norm": 6.539632320404053,
      "learning_rate": 1.7975094748240388e-05,
      "loss": 0.3513,
      "step": 11110
    },
    {
      "epoch": 2.0068579678758347,
      "grad_norm": 7.067559719085693,
      "learning_rate": 1.7964266377910123e-05,
      "loss": 0.3646,
      "step": 11120
    },
    {
      "epoch": 2.0086626962642122,
      "grad_norm": 15.284363746643066,
      "learning_rate": 1.795343800757986e-05,
      "loss": 0.4302,
      "step": 11130
    },
    {
      "epoch": 2.0104674246525898,
      "grad_norm": 16.512544631958008,
      "learning_rate": 1.7942609637249596e-05,
      "loss": 0.5949,
      "step": 11140
    },
    {
      "epoch": 2.0122721530409673,
      "grad_norm": 12.261258125305176,
      "learning_rate": 1.7931781266919328e-05,
      "loss": 0.395,
      "step": 11150
    },
    {
      "epoch": 2.014076881429345,
      "grad_norm": 8.243025779724121,
      "learning_rate": 1.7920952896589063e-05,
      "loss": 0.4582,
      "step": 11160
    },
    {
      "epoch": 2.0158816098177224,
      "grad_norm": 1.8561936616897583,
      "learning_rate": 1.7910124526258797e-05,
      "loss": 0.2916,
      "step": 11170
    },
    {
      "epoch": 2.0176863382061,
      "grad_norm": 19.75870132446289,
      "learning_rate": 1.7899296155928532e-05,
      "loss": 0.5711,
      "step": 11180
    },
    {
      "epoch": 2.0194910665944774,
      "grad_norm": 8.112504959106445,
      "learning_rate": 1.788846778559827e-05,
      "loss": 0.6451,
      "step": 11190
    },
    {
      "epoch": 2.021295794982855,
      "grad_norm": 13.932286262512207,
      "learning_rate": 1.7877639415268002e-05,
      "loss": 0.3994,
      "step": 11200
    },
    {
      "epoch": 2.0231005233712325,
      "grad_norm": 16.758691787719727,
      "learning_rate": 1.7866811044937737e-05,
      "loss": 0.4513,
      "step": 11210
    },
    {
      "epoch": 2.02490525175961,
      "grad_norm": 7.172226428985596,
      "learning_rate": 1.7855982674607472e-05,
      "loss": 0.4998,
      "step": 11220
    },
    {
      "epoch": 2.0267099801479875,
      "grad_norm": 11.735307693481445,
      "learning_rate": 1.7845154304277207e-05,
      "loss": 0.4587,
      "step": 11230
    },
    {
      "epoch": 2.028514708536365,
      "grad_norm": 12.732857704162598,
      "learning_rate": 1.783432593394694e-05,
      "loss": 0.5582,
      "step": 11240
    },
    {
      "epoch": 2.030319436924743,
      "grad_norm": 15.212407112121582,
      "learning_rate": 1.7823497563616677e-05,
      "loss": 0.5298,
      "step": 11250
    },
    {
      "epoch": 2.0321241653131206,
      "grad_norm": 15.871028900146484,
      "learning_rate": 1.781266919328641e-05,
      "loss": 0.3299,
      "step": 11260
    },
    {
      "epoch": 2.033928893701498,
      "grad_norm": 3.471656322479248,
      "learning_rate": 1.7801840822956146e-05,
      "loss": 0.3535,
      "step": 11270
    },
    {
      "epoch": 2.0357336220898756,
      "grad_norm": 22.4830265045166,
      "learning_rate": 1.779101245262588e-05,
      "loss": 0.4405,
      "step": 11280
    },
    {
      "epoch": 2.037538350478253,
      "grad_norm": 10.067303657531738,
      "learning_rate": 1.7780184082295613e-05,
      "loss": 0.4467,
      "step": 11290
    },
    {
      "epoch": 2.0393430788666307,
      "grad_norm": 23.696643829345703,
      "learning_rate": 1.7769355711965348e-05,
      "loss": 0.4238,
      "step": 11300
    },
    {
      "epoch": 2.041147807255008,
      "grad_norm": 16.30623435974121,
      "learning_rate": 1.7758527341635086e-05,
      "loss": 0.5731,
      "step": 11310
    },
    {
      "epoch": 2.0429525356433857,
      "grad_norm": 13.190832138061523,
      "learning_rate": 1.774769897130482e-05,
      "loss": 0.3583,
      "step": 11320
    },
    {
      "epoch": 2.0447572640317633,
      "grad_norm": 28.935762405395508,
      "learning_rate": 1.7736870600974553e-05,
      "loss": 0.3996,
      "step": 11330
    },
    {
      "epoch": 2.046561992420141,
      "grad_norm": 20.054813385009766,
      "learning_rate": 1.7726042230644287e-05,
      "loss": 0.4122,
      "step": 11340
    },
    {
      "epoch": 2.0483667208085183,
      "grad_norm": 29.25743293762207,
      "learning_rate": 1.7715213860314022e-05,
      "loss": 0.4022,
      "step": 11350
    },
    {
      "epoch": 2.050171449196896,
      "grad_norm": 13.948742866516113,
      "learning_rate": 1.7704385489983757e-05,
      "loss": 0.4815,
      "step": 11360
    },
    {
      "epoch": 2.0519761775852734,
      "grad_norm": 22.019453048706055,
      "learning_rate": 1.7693557119653496e-05,
      "loss": 0.3963,
      "step": 11370
    },
    {
      "epoch": 2.053780905973651,
      "grad_norm": 17.994775772094727,
      "learning_rate": 1.7682728749323227e-05,
      "loss": 0.4113,
      "step": 11380
    },
    {
      "epoch": 2.0555856343620285,
      "grad_norm": 19.834678649902344,
      "learning_rate": 1.7671900378992962e-05,
      "loss": 0.4574,
      "step": 11390
    },
    {
      "epoch": 2.057390362750406,
      "grad_norm": 13.657617568969727,
      "learning_rate": 1.7661072008662697e-05,
      "loss": 0.4585,
      "step": 11400
    },
    {
      "epoch": 2.0591950911387835,
      "grad_norm": 20.047809600830078,
      "learning_rate": 1.7650243638332432e-05,
      "loss": 0.5444,
      "step": 11410
    },
    {
      "epoch": 2.060999819527161,
      "grad_norm": 19.763755798339844,
      "learning_rate": 1.7639415268002163e-05,
      "loss": 0.4705,
      "step": 11420
    },
    {
      "epoch": 2.0628045479155386,
      "grad_norm": 5.7845988273620605,
      "learning_rate": 1.76285868976719e-05,
      "loss": 0.3471,
      "step": 11430
    },
    {
      "epoch": 2.064609276303916,
      "grad_norm": 11.344403266906738,
      "learning_rate": 1.7617758527341636e-05,
      "loss": 0.3874,
      "step": 11440
    },
    {
      "epoch": 2.0664140046922936,
      "grad_norm": 4.804684638977051,
      "learning_rate": 1.760693015701137e-05,
      "loss": 0.4132,
      "step": 11450
    },
    {
      "epoch": 2.068218733080671,
      "grad_norm": 12.112603187561035,
      "learning_rate": 1.7596101786681106e-05,
      "loss": 0.2636,
      "step": 11460
    },
    {
      "epoch": 2.0700234614690487,
      "grad_norm": 2.9801254272460938,
      "learning_rate": 1.7585273416350838e-05,
      "loss": 0.3727,
      "step": 11470
    },
    {
      "epoch": 2.0718281898574267,
      "grad_norm": 25.15041732788086,
      "learning_rate": 1.7574445046020573e-05,
      "loss": 0.4191,
      "step": 11480
    },
    {
      "epoch": 2.073632918245804,
      "grad_norm": 21.393814086914062,
      "learning_rate": 1.756361667569031e-05,
      "loss": 0.4013,
      "step": 11490
    },
    {
      "epoch": 2.0754376466341817,
      "grad_norm": 5.020763397216797,
      "learning_rate": 1.7552788305360046e-05,
      "loss": 0.3062,
      "step": 11500
    },
    {
      "epoch": 2.0772423750225593,
      "grad_norm": 28.306615829467773,
      "learning_rate": 1.7541959935029777e-05,
      "loss": 0.4147,
      "step": 11510
    },
    {
      "epoch": 2.079047103410937,
      "grad_norm": 9.379783630371094,
      "learning_rate": 1.7531131564699512e-05,
      "loss": 0.4191,
      "step": 11520
    },
    {
      "epoch": 2.0808518317993143,
      "grad_norm": 21.435348510742188,
      "learning_rate": 1.7520303194369247e-05,
      "loss": 0.4235,
      "step": 11530
    },
    {
      "epoch": 2.082656560187692,
      "grad_norm": 24.075693130493164,
      "learning_rate": 1.7509474824038982e-05,
      "loss": 0.5037,
      "step": 11540
    },
    {
      "epoch": 2.0844612885760694,
      "grad_norm": 9.237195014953613,
      "learning_rate": 1.749864645370872e-05,
      "loss": 0.4992,
      "step": 11550
    },
    {
      "epoch": 2.086266016964447,
      "grad_norm": 15.805708885192871,
      "learning_rate": 1.7487818083378452e-05,
      "loss": 0.2265,
      "step": 11560
    },
    {
      "epoch": 2.0880707453528244,
      "grad_norm": 17.722314834594727,
      "learning_rate": 1.7476989713048187e-05,
      "loss": 0.4954,
      "step": 11570
    },
    {
      "epoch": 2.089875473741202,
      "grad_norm": 21.6475887298584,
      "learning_rate": 1.7466161342717922e-05,
      "loss": 0.4062,
      "step": 11580
    },
    {
      "epoch": 2.0916802021295795,
      "grad_norm": 26.660316467285156,
      "learning_rate": 1.7455332972387657e-05,
      "loss": 0.4289,
      "step": 11590
    },
    {
      "epoch": 2.093484930517957,
      "grad_norm": 19.38504409790039,
      "learning_rate": 1.7444504602057388e-05,
      "loss": 0.4539,
      "step": 11600
    },
    {
      "epoch": 2.0952896589063346,
      "grad_norm": 42.38254165649414,
      "learning_rate": 1.7433676231727126e-05,
      "loss": 0.4687,
      "step": 11610
    },
    {
      "epoch": 2.097094387294712,
      "grad_norm": 15.48721694946289,
      "learning_rate": 1.742284786139686e-05,
      "loss": 0.4821,
      "step": 11620
    },
    {
      "epoch": 2.0988991156830896,
      "grad_norm": 18.444692611694336,
      "learning_rate": 1.7412019491066596e-05,
      "loss": 0.4795,
      "step": 11630
    },
    {
      "epoch": 2.100703844071467,
      "grad_norm": 21.113197326660156,
      "learning_rate": 1.7401191120736328e-05,
      "loss": 0.3139,
      "step": 11640
    },
    {
      "epoch": 2.1025085724598447,
      "grad_norm": 33.73133850097656,
      "learning_rate": 1.7390362750406063e-05,
      "loss": 0.5118,
      "step": 11650
    },
    {
      "epoch": 2.104313300848222,
      "grad_norm": 10.589162826538086,
      "learning_rate": 1.7379534380075798e-05,
      "loss": 0.4158,
      "step": 11660
    },
    {
      "epoch": 2.1061180292365997,
      "grad_norm": 12.704195022583008,
      "learning_rate": 1.7368706009745536e-05,
      "loss": 0.3015,
      "step": 11670
    },
    {
      "epoch": 2.1079227576249773,
      "grad_norm": 24.18782615661621,
      "learning_rate": 1.735787763941527e-05,
      "loss": 0.334,
      "step": 11680
    },
    {
      "epoch": 2.109727486013355,
      "grad_norm": 17.267520904541016,
      "learning_rate": 1.7347049269085002e-05,
      "loss": 0.385,
      "step": 11690
    },
    {
      "epoch": 2.1115322144017323,
      "grad_norm": 25.340763092041016,
      "learning_rate": 1.7336220898754737e-05,
      "loss": 0.375,
      "step": 11700
    },
    {
      "epoch": 2.1133369427901103,
      "grad_norm": 5.580778121948242,
      "learning_rate": 1.7325392528424472e-05,
      "loss": 0.4949,
      "step": 11710
    },
    {
      "epoch": 2.115141671178488,
      "grad_norm": 25.877714157104492,
      "learning_rate": 1.7314564158094207e-05,
      "loss": 0.4432,
      "step": 11720
    },
    {
      "epoch": 2.1169463995668654,
      "grad_norm": 30.128591537475586,
      "learning_rate": 1.7303735787763942e-05,
      "loss": 0.4025,
      "step": 11730
    },
    {
      "epoch": 2.118751127955243,
      "grad_norm": 27.17441177368164,
      "learning_rate": 1.7292907417433677e-05,
      "loss": 0.4505,
      "step": 11740
    },
    {
      "epoch": 2.1205558563436204,
      "grad_norm": 10.2031888961792,
      "learning_rate": 1.7282079047103412e-05,
      "loss": 0.3738,
      "step": 11750
    },
    {
      "epoch": 2.122360584731998,
      "grad_norm": 46.392581939697266,
      "learning_rate": 1.7271250676773147e-05,
      "loss": 0.3563,
      "step": 11760
    },
    {
      "epoch": 2.1241653131203755,
      "grad_norm": 32.48044967651367,
      "learning_rate": 1.726042230644288e-05,
      "loss": 0.4586,
      "step": 11770
    },
    {
      "epoch": 2.125970041508753,
      "grad_norm": 19.55951499938965,
      "learning_rate": 1.7249593936112613e-05,
      "loss": 0.5101,
      "step": 11780
    },
    {
      "epoch": 2.1277747698971305,
      "grad_norm": 29.346282958984375,
      "learning_rate": 1.723876556578235e-05,
      "loss": 0.4147,
      "step": 11790
    },
    {
      "epoch": 2.129579498285508,
      "grad_norm": 19.116548538208008,
      "learning_rate": 1.7227937195452086e-05,
      "loss": 0.3828,
      "step": 11800
    },
    {
      "epoch": 2.1313842266738856,
      "grad_norm": 18.763792037963867,
      "learning_rate": 1.721710882512182e-05,
      "loss": 0.3705,
      "step": 11810
    },
    {
      "epoch": 2.133188955062263,
      "grad_norm": 26.625070571899414,
      "learning_rate": 1.7206280454791553e-05,
      "loss": 0.4535,
      "step": 11820
    },
    {
      "epoch": 2.1349936834506407,
      "grad_norm": 7.9756951332092285,
      "learning_rate": 1.7195452084461288e-05,
      "loss": 0.2578,
      "step": 11830
    },
    {
      "epoch": 2.136798411839018,
      "grad_norm": 20.318559646606445,
      "learning_rate": 1.7184623714131022e-05,
      "loss": 0.6611,
      "step": 11840
    },
    {
      "epoch": 2.1386031402273957,
      "grad_norm": 21.755874633789062,
      "learning_rate": 1.717379534380076e-05,
      "loss": 0.4681,
      "step": 11850
    },
    {
      "epoch": 2.1404078686157733,
      "grad_norm": 9.760802268981934,
      "learning_rate": 1.7162966973470496e-05,
      "loss": 0.4334,
      "step": 11860
    },
    {
      "epoch": 2.142212597004151,
      "grad_norm": 15.310195922851562,
      "learning_rate": 1.7152138603140227e-05,
      "loss": 0.4147,
      "step": 11870
    },
    {
      "epoch": 2.1440173253925283,
      "grad_norm": 28.530698776245117,
      "learning_rate": 1.7141310232809962e-05,
      "loss": 0.6026,
      "step": 11880
    },
    {
      "epoch": 2.145822053780906,
      "grad_norm": 12.418072700500488,
      "learning_rate": 1.7130481862479697e-05,
      "loss": 0.463,
      "step": 11890
    },
    {
      "epoch": 2.1476267821692834,
      "grad_norm": 14.892414093017578,
      "learning_rate": 1.7119653492149432e-05,
      "loss": 0.4884,
      "step": 11900
    },
    {
      "epoch": 2.149431510557661,
      "grad_norm": 2.9813339710235596,
      "learning_rate": 1.7108825121819167e-05,
      "loss": 0.3977,
      "step": 11910
    },
    {
      "epoch": 2.1512362389460384,
      "grad_norm": 23.051410675048828,
      "learning_rate": 1.7097996751488902e-05,
      "loss": 0.6201,
      "step": 11920
    },
    {
      "epoch": 2.153040967334416,
      "grad_norm": 5.694174766540527,
      "learning_rate": 1.7087168381158637e-05,
      "loss": 0.3054,
      "step": 11930
    },
    {
      "epoch": 2.154845695722794,
      "grad_norm": 6.714786529541016,
      "learning_rate": 1.707634001082837e-05,
      "loss": 0.5556,
      "step": 11940
    },
    {
      "epoch": 2.1566504241111715,
      "grad_norm": 14.766069412231445,
      "learning_rate": 1.7065511640498106e-05,
      "loss": 0.4605,
      "step": 11950
    },
    {
      "epoch": 2.158455152499549,
      "grad_norm": 13.590086936950684,
      "learning_rate": 1.7054683270167838e-05,
      "loss": 0.4182,
      "step": 11960
    },
    {
      "epoch": 2.1602598808879265,
      "grad_norm": 15.253758430480957,
      "learning_rate": 1.7043854899837576e-05,
      "loss": 0.4621,
      "step": 11970
    },
    {
      "epoch": 2.162064609276304,
      "grad_norm": 14.310348510742188,
      "learning_rate": 1.703302652950731e-05,
      "loss": 0.4525,
      "step": 11980
    },
    {
      "epoch": 2.1638693376646816,
      "grad_norm": 28.48181915283203,
      "learning_rate": 1.7022198159177046e-05,
      "loss": 0.4295,
      "step": 11990
    },
    {
      "epoch": 2.165674066053059,
      "grad_norm": 25.27149772644043,
      "learning_rate": 1.7011369788846778e-05,
      "loss": 0.399,
      "step": 12000
    },
    {
      "epoch": 2.1674787944414367,
      "grad_norm": 30.703584671020508,
      "learning_rate": 1.7000541418516512e-05,
      "loss": 0.3616,
      "step": 12010
    },
    {
      "epoch": 2.169283522829814,
      "grad_norm": 27.334274291992188,
      "learning_rate": 1.6989713048186247e-05,
      "loss": 0.3218,
      "step": 12020
    },
    {
      "epoch": 2.1710882512181917,
      "grad_norm": 13.740986824035645,
      "learning_rate": 1.6978884677855986e-05,
      "loss": 0.438,
      "step": 12030
    },
    {
      "epoch": 2.1728929796065692,
      "grad_norm": 12.01953411102295,
      "learning_rate": 1.696805630752572e-05,
      "loss": 0.3864,
      "step": 12040
    },
    {
      "epoch": 2.1746977079949468,
      "grad_norm": 14.776949882507324,
      "learning_rate": 1.6957227937195452e-05,
      "loss": 0.4238,
      "step": 12050
    },
    {
      "epoch": 2.1765024363833243,
      "grad_norm": 22.257892608642578,
      "learning_rate": 1.6946399566865187e-05,
      "loss": 0.4741,
      "step": 12060
    },
    {
      "epoch": 2.178307164771702,
      "grad_norm": 14.01569938659668,
      "learning_rate": 1.6935571196534922e-05,
      "loss": 0.5372,
      "step": 12070
    },
    {
      "epoch": 2.1801118931600794,
      "grad_norm": 12.324677467346191,
      "learning_rate": 1.6925825663237682e-05,
      "loss": 0.4807,
      "step": 12080
    },
    {
      "epoch": 2.181916621548457,
      "grad_norm": 16.984539031982422,
      "learning_rate": 1.691499729290742e-05,
      "loss": 0.4875,
      "step": 12090
    },
    {
      "epoch": 2.1837213499368344,
      "grad_norm": 14.65916919708252,
      "learning_rate": 1.6904168922577152e-05,
      "loss": 0.3631,
      "step": 12100
    },
    {
      "epoch": 2.185526078325212,
      "grad_norm": 27.26210594177246,
      "learning_rate": 1.6893340552246887e-05,
      "loss": 0.4494,
      "step": 12110
    },
    {
      "epoch": 2.1873308067135895,
      "grad_norm": 17.729537963867188,
      "learning_rate": 1.688251218191662e-05,
      "loss": 0.4394,
      "step": 12120
    },
    {
      "epoch": 2.189135535101967,
      "grad_norm": 16.95124053955078,
      "learning_rate": 1.6871683811586356e-05,
      "loss": 0.4851,
      "step": 12130
    },
    {
      "epoch": 2.1909402634903445,
      "grad_norm": 9.527938842773438,
      "learning_rate": 1.686085544125609e-05,
      "loss": 0.4852,
      "step": 12140
    },
    {
      "epoch": 2.192744991878722,
      "grad_norm": 13.802474021911621,
      "learning_rate": 1.6850027070925826e-05,
      "loss": 0.3643,
      "step": 12150
    },
    {
      "epoch": 2.1945497202670996,
      "grad_norm": 28.4749755859375,
      "learning_rate": 1.683919870059556e-05,
      "loss": 0.3769,
      "step": 12160
    },
    {
      "epoch": 2.1963544486554776,
      "grad_norm": 22.10318946838379,
      "learning_rate": 1.6828370330265296e-05,
      "loss": 0.3762,
      "step": 12170
    },
    {
      "epoch": 2.198159177043855,
      "grad_norm": 35.16987228393555,
      "learning_rate": 1.681754195993503e-05,
      "loss": 0.5273,
      "step": 12180
    },
    {
      "epoch": 2.1999639054322326,
      "grad_norm": 5.577988147735596,
      "learning_rate": 1.6806713589604762e-05,
      "loss": 0.4081,
      "step": 12190
    },
    {
      "epoch": 2.20176863382061,
      "grad_norm": 30.47016716003418,
      "learning_rate": 1.6795885219274497e-05,
      "loss": 0.451,
      "step": 12200
    },
    {
      "epoch": 2.2035733622089877,
      "grad_norm": 16.937591552734375,
      "learning_rate": 1.6785056848944236e-05,
      "loss": 0.4163,
      "step": 12210
    },
    {
      "epoch": 2.2053780905973652,
      "grad_norm": 6.782135486602783,
      "learning_rate": 1.677422847861397e-05,
      "loss": 0.4911,
      "step": 12220
    },
    {
      "epoch": 2.2071828189857428,
      "grad_norm": 15.306591033935547,
      "learning_rate": 1.6763400108283705e-05,
      "loss": 0.401,
      "step": 12230
    },
    {
      "epoch": 2.2089875473741203,
      "grad_norm": 18.754695892333984,
      "learning_rate": 1.6752571737953437e-05,
      "loss": 0.5467,
      "step": 12240
    },
    {
      "epoch": 2.210792275762498,
      "grad_norm": 24.108190536499023,
      "learning_rate": 1.6741743367623172e-05,
      "loss": 0.3706,
      "step": 12250
    },
    {
      "epoch": 2.2125970041508753,
      "grad_norm": 8.505778312683105,
      "learning_rate": 1.6730914997292907e-05,
      "loss": 0.3848,
      "step": 12260
    },
    {
      "epoch": 2.214401732539253,
      "grad_norm": 14.816394805908203,
      "learning_rate": 1.6720086626962645e-05,
      "loss": 0.5944,
      "step": 12270
    },
    {
      "epoch": 2.2162064609276304,
      "grad_norm": 43.7161979675293,
      "learning_rate": 1.6709258256632377e-05,
      "loss": 0.4655,
      "step": 12280
    },
    {
      "epoch": 2.218011189316008,
      "grad_norm": 15.391231536865234,
      "learning_rate": 1.669842988630211e-05,
      "loss": 0.516,
      "step": 12290
    },
    {
      "epoch": 2.2198159177043855,
      "grad_norm": 6.293023109436035,
      "learning_rate": 1.6687601515971846e-05,
      "loss": 0.4615,
      "step": 12300
    },
    {
      "epoch": 2.221620646092763,
      "grad_norm": 8.503894805908203,
      "learning_rate": 1.667677314564158e-05,
      "loss": 0.4113,
      "step": 12310
    },
    {
      "epoch": 2.2234253744811405,
      "grad_norm": 15.900177001953125,
      "learning_rate": 1.6665944775311316e-05,
      "loss": 0.4652,
      "step": 12320
    },
    {
      "epoch": 2.225230102869518,
      "grad_norm": 21.344873428344727,
      "learning_rate": 1.665511640498105e-05,
      "loss": 0.4418,
      "step": 12330
    },
    {
      "epoch": 2.2270348312578956,
      "grad_norm": 27.91869354248047,
      "learning_rate": 1.6644288034650786e-05,
      "loss": 0.3525,
      "step": 12340
    },
    {
      "epoch": 2.228839559646273,
      "grad_norm": 17.02073860168457,
      "learning_rate": 1.663345966432052e-05,
      "loss": 0.2955,
      "step": 12350
    },
    {
      "epoch": 2.2306442880346506,
      "grad_norm": 4.502109050750732,
      "learning_rate": 1.6622631293990256e-05,
      "loss": 0.2767,
      "step": 12360
    },
    {
      "epoch": 2.232449016423028,
      "grad_norm": 15.152627944946289,
      "learning_rate": 1.6611802923659987e-05,
      "loss": 0.7504,
      "step": 12370
    },
    {
      "epoch": 2.2342537448114057,
      "grad_norm": 16.123014450073242,
      "learning_rate": 1.6600974553329722e-05,
      "loss": 0.3612,
      "step": 12380
    },
    {
      "epoch": 2.2360584731997832,
      "grad_norm": 19.440282821655273,
      "learning_rate": 1.659014618299946e-05,
      "loss": 0.3022,
      "step": 12390
    },
    {
      "epoch": 2.237863201588161,
      "grad_norm": 22.391605377197266,
      "learning_rate": 1.6579317812669195e-05,
      "loss": 0.4981,
      "step": 12400
    },
    {
      "epoch": 2.2396679299765383,
      "grad_norm": 20.126201629638672,
      "learning_rate": 1.656848944233893e-05,
      "loss": 0.4847,
      "step": 12410
    },
    {
      "epoch": 2.2414726583649163,
      "grad_norm": 18.57595443725586,
      "learning_rate": 1.6557661072008662e-05,
      "loss": 0.3778,
      "step": 12420
    },
    {
      "epoch": 2.243277386753294,
      "grad_norm": 13.359482765197754,
      "learning_rate": 1.6546832701678397e-05,
      "loss": 0.4422,
      "step": 12430
    },
    {
      "epoch": 2.2450821151416713,
      "grad_norm": 9.917928695678711,
      "learning_rate": 1.653600433134813e-05,
      "loss": 0.4236,
      "step": 12440
    },
    {
      "epoch": 2.246886843530049,
      "grad_norm": 36.21866226196289,
      "learning_rate": 1.652517596101787e-05,
      "loss": 0.3604,
      "step": 12450
    },
    {
      "epoch": 2.2486915719184264,
      "grad_norm": 20.02781867980957,
      "learning_rate": 1.65143475906876e-05,
      "loss": 0.4956,
      "step": 12460
    },
    {
      "epoch": 2.250496300306804,
      "grad_norm": 9.328752517700195,
      "learning_rate": 1.6503519220357336e-05,
      "loss": 0.3325,
      "step": 12470
    },
    {
      "epoch": 2.2523010286951815,
      "grad_norm": 27.847654342651367,
      "learning_rate": 1.649269085002707e-05,
      "loss": 0.6176,
      "step": 12480
    },
    {
      "epoch": 2.254105757083559,
      "grad_norm": 9.623151779174805,
      "learning_rate": 1.6481862479696806e-05,
      "loss": 0.4352,
      "step": 12490
    },
    {
      "epoch": 2.2559104854719365,
      "grad_norm": 17.60556983947754,
      "learning_rate": 1.647103410936654e-05,
      "loss": 0.4391,
      "step": 12500
    },
    {
      "epoch": 2.257715213860314,
      "grad_norm": 28.915271759033203,
      "learning_rate": 1.6460205739036276e-05,
      "loss": 0.4365,
      "step": 12510
    },
    {
      "epoch": 2.2595199422486916,
      "grad_norm": 15.498740196228027,
      "learning_rate": 1.644937736870601e-05,
      "loss": 0.4965,
      "step": 12520
    },
    {
      "epoch": 2.261324670637069,
      "grad_norm": 17.74704933166504,
      "learning_rate": 1.6438548998375746e-05,
      "loss": 0.5864,
      "step": 12530
    },
    {
      "epoch": 2.2631293990254466,
      "grad_norm": 13.227788925170898,
      "learning_rate": 1.642772062804548e-05,
      "loss": 0.4095,
      "step": 12540
    },
    {
      "epoch": 2.264934127413824,
      "grad_norm": 16.53329086303711,
      "learning_rate": 1.6416892257715212e-05,
      "loss": 0.4586,
      "step": 12550
    },
    {
      "epoch": 2.2667388558022017,
      "grad_norm": 11.099370002746582,
      "learning_rate": 1.6406063887384947e-05,
      "loss": 0.3853,
      "step": 12560
    },
    {
      "epoch": 2.268543584190579,
      "grad_norm": 21.840654373168945,
      "learning_rate": 1.6395235517054685e-05,
      "loss": 0.4614,
      "step": 12570
    },
    {
      "epoch": 2.2703483125789568,
      "grad_norm": 16.714418411254883,
      "learning_rate": 1.638440714672442e-05,
      "loss": 0.3968,
      "step": 12580
    },
    {
      "epoch": 2.2721530409673343,
      "grad_norm": 7.7128472328186035,
      "learning_rate": 1.6373578776394155e-05,
      "loss": 0.4048,
      "step": 12590
    },
    {
      "epoch": 2.273957769355712,
      "grad_norm": 21.83818817138672,
      "learning_rate": 1.6362750406063887e-05,
      "loss": 0.4169,
      "step": 12600
    },
    {
      "epoch": 2.2757624977440893,
      "grad_norm": 11.315498352050781,
      "learning_rate": 1.635192203573362e-05,
      "loss": 0.3871,
      "step": 12610
    },
    {
      "epoch": 2.277567226132467,
      "grad_norm": 16.791595458984375,
      "learning_rate": 1.6341093665403357e-05,
      "loss": 0.5052,
      "step": 12620
    },
    {
      "epoch": 2.279371954520845,
      "grad_norm": 7.8860650062561035,
      "learning_rate": 1.6330265295073095e-05,
      "loss": 0.5354,
      "step": 12630
    },
    {
      "epoch": 2.281176682909222,
      "grad_norm": 11.618639945983887,
      "learning_rate": 1.6319436924742826e-05,
      "loss": 0.4138,
      "step": 12640
    },
    {
      "epoch": 2.2829814112976,
      "grad_norm": 1.7710542678833008,
      "learning_rate": 1.630860855441256e-05,
      "loss": 0.417,
      "step": 12650
    },
    {
      "epoch": 2.2847861396859774,
      "grad_norm": 22.9606876373291,
      "learning_rate": 1.6297780184082296e-05,
      "loss": 0.3348,
      "step": 12660
    },
    {
      "epoch": 2.286590868074355,
      "grad_norm": 23.7310848236084,
      "learning_rate": 1.628695181375203e-05,
      "loss": 0.4511,
      "step": 12670
    },
    {
      "epoch": 2.2883955964627325,
      "grad_norm": 15.509934425354004,
      "learning_rate": 1.6276123443421763e-05,
      "loss": 0.6324,
      "step": 12680
    },
    {
      "epoch": 2.29020032485111,
      "grad_norm": 13.907134056091309,
      "learning_rate": 1.62652950730915e-05,
      "loss": 0.3999,
      "step": 12690
    },
    {
      "epoch": 2.2920050532394876,
      "grad_norm": 21.999740600585938,
      "learning_rate": 1.6254466702761236e-05,
      "loss": 0.5095,
      "step": 12700
    },
    {
      "epoch": 2.293809781627865,
      "grad_norm": 19.043861389160156,
      "learning_rate": 1.624363833243097e-05,
      "loss": 0.403,
      "step": 12710
    },
    {
      "epoch": 2.2956145100162426,
      "grad_norm": 24.674348831176758,
      "learning_rate": 1.6232809962100706e-05,
      "loss": 0.304,
      "step": 12720
    },
    {
      "epoch": 2.29741923840462,
      "grad_norm": 14.516425132751465,
      "learning_rate": 1.6221981591770437e-05,
      "loss": 0.4207,
      "step": 12730
    },
    {
      "epoch": 2.2992239667929977,
      "grad_norm": 22.274702072143555,
      "learning_rate": 1.6211153221440172e-05,
      "loss": 0.4297,
      "step": 12740
    },
    {
      "epoch": 2.301028695181375,
      "grad_norm": 31.634958267211914,
      "learning_rate": 1.620032485110991e-05,
      "loss": 0.7301,
      "step": 12750
    },
    {
      "epoch": 2.3028334235697527,
      "grad_norm": 17.548322677612305,
      "learning_rate": 1.6189496480779645e-05,
      "loss": 0.3447,
      "step": 12760
    },
    {
      "epoch": 2.3046381519581303,
      "grad_norm": 17.925241470336914,
      "learning_rate": 1.6178668110449377e-05,
      "loss": 0.3725,
      "step": 12770
    },
    {
      "epoch": 2.306442880346508,
      "grad_norm": 17.87883949279785,
      "learning_rate": 1.616783974011911e-05,
      "loss": 0.5935,
      "step": 12780
    },
    {
      "epoch": 2.3082476087348853,
      "grad_norm": 5.15683126449585,
      "learning_rate": 1.6157011369788847e-05,
      "loss": 0.46,
      "step": 12790
    },
    {
      "epoch": 2.310052337123263,
      "grad_norm": 13.630788803100586,
      "learning_rate": 1.614618299945858e-05,
      "loss": 0.4374,
      "step": 12800
    },
    {
      "epoch": 2.3118570655116404,
      "grad_norm": 15.806626319885254,
      "learning_rate": 1.6135354629128316e-05,
      "loss": 0.3002,
      "step": 12810
    },
    {
      "epoch": 2.313661793900018,
      "grad_norm": 19.605443954467773,
      "learning_rate": 1.612452625879805e-05,
      "loss": 0.4577,
      "step": 12820
    },
    {
      "epoch": 2.3154665222883954,
      "grad_norm": 8.617570877075195,
      "learning_rate": 1.6113697888467786e-05,
      "loss": 0.3832,
      "step": 12830
    },
    {
      "epoch": 2.317271250676773,
      "grad_norm": 3.5592362880706787,
      "learning_rate": 1.610286951813752e-05,
      "loss": 0.4233,
      "step": 12840
    },
    {
      "epoch": 2.3190759790651505,
      "grad_norm": 40.04780578613281,
      "learning_rate": 1.6092041147807256e-05,
      "loss": 0.4784,
      "step": 12850
    },
    {
      "epoch": 2.3208807074535285,
      "grad_norm": 11.382280349731445,
      "learning_rate": 1.6081212777476987e-05,
      "loss": 0.3729,
      "step": 12860
    },
    {
      "epoch": 2.3226854358419056,
      "grad_norm": 11.949312210083008,
      "learning_rate": 1.6070384407146722e-05,
      "loss": 0.347,
      "step": 12870
    },
    {
      "epoch": 2.3244901642302835,
      "grad_norm": 11.742830276489258,
      "learning_rate": 1.605955603681646e-05,
      "loss": 0.3975,
      "step": 12880
    },
    {
      "epoch": 2.326294892618661,
      "grad_norm": 6.328669548034668,
      "learning_rate": 1.6048727666486196e-05,
      "loss": 0.4686,
      "step": 12890
    },
    {
      "epoch": 2.3280996210070386,
      "grad_norm": 12.760713577270508,
      "learning_rate": 1.603789929615593e-05,
      "loss": 0.4183,
      "step": 12900
    },
    {
      "epoch": 2.329904349395416,
      "grad_norm": 25.85171127319336,
      "learning_rate": 1.6027070925825662e-05,
      "loss": 0.3255,
      "step": 12910
    },
    {
      "epoch": 2.3317090777837937,
      "grad_norm": 2.6908247470855713,
      "learning_rate": 1.6016242555495397e-05,
      "loss": 0.441,
      "step": 12920
    },
    {
      "epoch": 2.333513806172171,
      "grad_norm": 1.7219207286834717,
      "learning_rate": 1.6005414185165132e-05,
      "loss": 0.601,
      "step": 12930
    },
    {
      "epoch": 2.3353185345605487,
      "grad_norm": 43.55421829223633,
      "learning_rate": 1.599458581483487e-05,
      "loss": 0.5825,
      "step": 12940
    },
    {
      "epoch": 2.3371232629489263,
      "grad_norm": 13.150837898254395,
      "learning_rate": 1.59837574445046e-05,
      "loss": 0.5911,
      "step": 12950
    },
    {
      "epoch": 2.338927991337304,
      "grad_norm": 19.60334587097168,
      "learning_rate": 1.5972929074174337e-05,
      "loss": 0.4941,
      "step": 12960
    },
    {
      "epoch": 2.3407327197256813,
      "grad_norm": 22.87009048461914,
      "learning_rate": 1.596210070384407e-05,
      "loss": 0.5172,
      "step": 12970
    },
    {
      "epoch": 2.342537448114059,
      "grad_norm": 17.575817108154297,
      "learning_rate": 1.5951272333513806e-05,
      "loss": 0.4429,
      "step": 12980
    },
    {
      "epoch": 2.3443421765024364,
      "grad_norm": 7.9871826171875,
      "learning_rate": 1.594044396318354e-05,
      "loss": 0.4886,
      "step": 12990
    },
    {
      "epoch": 2.346146904890814,
      "grad_norm": 14.605952262878418,
      "learning_rate": 1.5929615592853276e-05,
      "loss": 0.3582,
      "step": 13000
    },
    {
      "epoch": 2.3479516332791914,
      "grad_norm": 22.591577529907227,
      "learning_rate": 1.591878722252301e-05,
      "loss": 0.281,
      "step": 13010
    },
    {
      "epoch": 2.349756361667569,
      "grad_norm": 32.0587158203125,
      "learning_rate": 1.5907958852192746e-05,
      "loss": 0.5379,
      "step": 13020
    },
    {
      "epoch": 2.3515610900559465,
      "grad_norm": 20.6162109375,
      "learning_rate": 1.589713048186248e-05,
      "loss": 0.4276,
      "step": 13030
    },
    {
      "epoch": 2.353365818444324,
      "grad_norm": 8.935464859008789,
      "learning_rate": 1.5886302111532212e-05,
      "loss": 0.3135,
      "step": 13040
    },
    {
      "epoch": 2.3551705468327016,
      "grad_norm": 5.371670722961426,
      "learning_rate": 1.5875473741201947e-05,
      "loss": 0.3809,
      "step": 13050
    },
    {
      "epoch": 2.356975275221079,
      "grad_norm": 14.86967658996582,
      "learning_rate": 1.5864645370871686e-05,
      "loss": 0.4365,
      "step": 13060
    },
    {
      "epoch": 2.3587800036094566,
      "grad_norm": 26.67873191833496,
      "learning_rate": 1.585381700054142e-05,
      "loss": 0.448,
      "step": 13070
    },
    {
      "epoch": 2.360584731997834,
      "grad_norm": 18.271942138671875,
      "learning_rate": 1.5842988630211155e-05,
      "loss": 0.4657,
      "step": 13080
    },
    {
      "epoch": 2.362389460386212,
      "grad_norm": 34.901641845703125,
      "learning_rate": 1.5832160259880887e-05,
      "loss": 0.5083,
      "step": 13090
    },
    {
      "epoch": 2.364194188774589,
      "grad_norm": 26.523290634155273,
      "learning_rate": 1.5821331889550622e-05,
      "loss": 0.3896,
      "step": 13100
    },
    {
      "epoch": 2.365998917162967,
      "grad_norm": 8.355985641479492,
      "learning_rate": 1.5810503519220357e-05,
      "loss": 0.4231,
      "step": 13110
    },
    {
      "epoch": 2.3678036455513447,
      "grad_norm": 12.770087242126465,
      "learning_rate": 1.5799675148890095e-05,
      "loss": 0.5026,
      "step": 13120
    },
    {
      "epoch": 2.3696083739397222,
      "grad_norm": 7.024528503417969,
      "learning_rate": 1.5788846778559827e-05,
      "loss": 0.4607,
      "step": 13130
    },
    {
      "epoch": 2.3714131023280998,
      "grad_norm": 6.728365898132324,
      "learning_rate": 1.577801840822956e-05,
      "loss": 0.4036,
      "step": 13140
    },
    {
      "epoch": 2.3732178307164773,
      "grad_norm": 12.286992073059082,
      "learning_rate": 1.5767190037899296e-05,
      "loss": 0.4139,
      "step": 13150
    },
    {
      "epoch": 2.375022559104855,
      "grad_norm": 14.920876502990723,
      "learning_rate": 1.575636166756903e-05,
      "loss": 0.5114,
      "step": 13160
    },
    {
      "epoch": 2.3768272874932324,
      "grad_norm": 16.46546173095703,
      "learning_rate": 1.5745533297238766e-05,
      "loss": 0.3125,
      "step": 13170
    },
    {
      "epoch": 2.37863201588161,
      "grad_norm": 10.37370777130127,
      "learning_rate": 1.57347049269085e-05,
      "loss": 0.318,
      "step": 13180
    },
    {
      "epoch": 2.3804367442699874,
      "grad_norm": 19.383434295654297,
      "learning_rate": 1.5723876556578236e-05,
      "loss": 0.4927,
      "step": 13190
    },
    {
      "epoch": 2.382241472658365,
      "grad_norm": 15.789173126220703,
      "learning_rate": 1.571304818624797e-05,
      "loss": 0.4427,
      "step": 13200
    },
    {
      "epoch": 2.3840462010467425,
      "grad_norm": 11.702959060668945,
      "learning_rate": 1.5702219815917706e-05,
      "loss": 0.4913,
      "step": 13210
    },
    {
      "epoch": 2.38585092943512,
      "grad_norm": 23.71849250793457,
      "learning_rate": 1.5691391445587437e-05,
      "loss": 0.4073,
      "step": 13220
    },
    {
      "epoch": 2.3876556578234975,
      "grad_norm": 20.34639549255371,
      "learning_rate": 1.5680563075257172e-05,
      "loss": 0.5196,
      "step": 13230
    },
    {
      "epoch": 2.389460386211875,
      "grad_norm": 9.167223930358887,
      "learning_rate": 1.566973470492691e-05,
      "loss": 0.4269,
      "step": 13240
    },
    {
      "epoch": 2.3912651146002526,
      "grad_norm": 8.401190757751465,
      "learning_rate": 1.5658906334596645e-05,
      "loss": 0.3677,
      "step": 13250
    },
    {
      "epoch": 2.39306984298863,
      "grad_norm": 18.6173038482666,
      "learning_rate": 1.5648077964266377e-05,
      "loss": 0.4433,
      "step": 13260
    },
    {
      "epoch": 2.3948745713770077,
      "grad_norm": 20.620220184326172,
      "learning_rate": 1.5637249593936112e-05,
      "loss": 0.5016,
      "step": 13270
    },
    {
      "epoch": 2.396679299765385,
      "grad_norm": 20.539772033691406,
      "learning_rate": 1.5626421223605847e-05,
      "loss": 0.4325,
      "step": 13280
    },
    {
      "epoch": 2.3984840281537627,
      "grad_norm": 8.346041679382324,
      "learning_rate": 1.561559285327558e-05,
      "loss": 0.4331,
      "step": 13290
    },
    {
      "epoch": 2.4002887565421402,
      "grad_norm": 11.231104850769043,
      "learning_rate": 1.560476448294532e-05,
      "loss": 0.5063,
      "step": 13300
    },
    {
      "epoch": 2.4020934849305178,
      "grad_norm": 7.495666980743408,
      "learning_rate": 1.559393611261505e-05,
      "loss": 0.4368,
      "step": 13310
    },
    {
      "epoch": 2.4038982133188957,
      "grad_norm": 14.060503959655762,
      "learning_rate": 1.5583107742284786e-05,
      "loss": 0.4923,
      "step": 13320
    },
    {
      "epoch": 2.405702941707273,
      "grad_norm": 5.673432350158691,
      "learning_rate": 1.557227937195452e-05,
      "loss": 0.3783,
      "step": 13330
    },
    {
      "epoch": 2.407507670095651,
      "grad_norm": 8.319372177124023,
      "learning_rate": 1.5561451001624256e-05,
      "loss": 0.5158,
      "step": 13340
    },
    {
      "epoch": 2.4093123984840283,
      "grad_norm": 7.564870357513428,
      "learning_rate": 1.5550622631293988e-05,
      "loss": 0.3315,
      "step": 13350
    },
    {
      "epoch": 2.411117126872406,
      "grad_norm": 4.820587635040283,
      "learning_rate": 1.5539794260963726e-05,
      "loss": 0.2673,
      "step": 13360
    },
    {
      "epoch": 2.4129218552607834,
      "grad_norm": 23.89505958557129,
      "learning_rate": 1.552896589063346e-05,
      "loss": 0.4323,
      "step": 13370
    },
    {
      "epoch": 2.414726583649161,
      "grad_norm": 21.073654174804688,
      "learning_rate": 1.551922035733622e-05,
      "loss": 0.3441,
      "step": 13380
    },
    {
      "epoch": 2.4165313120375385,
      "grad_norm": 17.46959686279297,
      "learning_rate": 1.5508391987005956e-05,
      "loss": 0.4635,
      "step": 13390
    },
    {
      "epoch": 2.418336040425916,
      "grad_norm": 11.89266300201416,
      "learning_rate": 1.549756361667569e-05,
      "loss": 0.452,
      "step": 13400
    },
    {
      "epoch": 2.4201407688142935,
      "grad_norm": 23.633831024169922,
      "learning_rate": 1.5486735246345426e-05,
      "loss": 0.3201,
      "step": 13410
    },
    {
      "epoch": 2.421945497202671,
      "grad_norm": 20.171110153198242,
      "learning_rate": 1.547590687601516e-05,
      "loss": 0.3271,
      "step": 13420
    },
    {
      "epoch": 2.4237502255910486,
      "grad_norm": 29.28689193725586,
      "learning_rate": 1.5465078505684895e-05,
      "loss": 0.4972,
      "step": 13430
    },
    {
      "epoch": 2.425554953979426,
      "grad_norm": 23.741464614868164,
      "learning_rate": 1.545425013535463e-05,
      "loss": 0.4075,
      "step": 13440
    },
    {
      "epoch": 2.4273596823678036,
      "grad_norm": 15.057868957519531,
      "learning_rate": 1.5443421765024365e-05,
      "loss": 0.4488,
      "step": 13450
    },
    {
      "epoch": 2.429164410756181,
      "grad_norm": 9.155354499816895,
      "learning_rate": 1.5432593394694097e-05,
      "loss": 0.4161,
      "step": 13460
    },
    {
      "epoch": 2.4309691391445587,
      "grad_norm": 4.028213977813721,
      "learning_rate": 1.5421765024363835e-05,
      "loss": 0.4707,
      "step": 13470
    },
    {
      "epoch": 2.4327738675329362,
      "grad_norm": 13.030962944030762,
      "learning_rate": 1.541093665403357e-05,
      "loss": 0.3694,
      "step": 13480
    },
    {
      "epoch": 2.4345785959213138,
      "grad_norm": 19.29877281188965,
      "learning_rate": 1.5400108283703305e-05,
      "loss": 0.4787,
      "step": 13490
    },
    {
      "epoch": 2.4363833243096913,
      "grad_norm": 18.163354873657227,
      "learning_rate": 1.5389279913373036e-05,
      "loss": 0.4322,
      "step": 13500
    },
    {
      "epoch": 2.438188052698069,
      "grad_norm": 25.774477005004883,
      "learning_rate": 1.537845154304277e-05,
      "loss": 0.3148,
      "step": 13510
    },
    {
      "epoch": 2.4399927810864463,
      "grad_norm": 17.490076065063477,
      "learning_rate": 1.5367623172712506e-05,
      "loss": 0.316,
      "step": 13520
    },
    {
      "epoch": 2.441797509474824,
      "grad_norm": 19.405559539794922,
      "learning_rate": 1.535679480238224e-05,
      "loss": 0.4278,
      "step": 13530
    },
    {
      "epoch": 2.4436022378632014,
      "grad_norm": 15.323071479797363,
      "learning_rate": 1.534596643205198e-05,
      "loss": 0.4963,
      "step": 13540
    },
    {
      "epoch": 2.4454069662515794,
      "grad_norm": 13.829205513000488,
      "learning_rate": 1.533513806172171e-05,
      "loss": 0.5398,
      "step": 13550
    },
    {
      "epoch": 2.4472116946399565,
      "grad_norm": 30.702220916748047,
      "learning_rate": 1.5324309691391446e-05,
      "loss": 0.4158,
      "step": 13560
    },
    {
      "epoch": 2.4490164230283344,
      "grad_norm": 19.298831939697266,
      "learning_rate": 1.531348132106118e-05,
      "loss": 0.4902,
      "step": 13570
    },
    {
      "epoch": 2.450821151416712,
      "grad_norm": 12.650375366210938,
      "learning_rate": 1.5302652950730916e-05,
      "loss": 0.415,
      "step": 13580
    },
    {
      "epoch": 2.4526258798050895,
      "grad_norm": 11.518227577209473,
      "learning_rate": 1.5291824580400647e-05,
      "loss": 0.4265,
      "step": 13590
    },
    {
      "epoch": 2.454430608193467,
      "grad_norm": 19.20881462097168,
      "learning_rate": 1.5280996210070385e-05,
      "loss": 0.3933,
      "step": 13600
    },
    {
      "epoch": 2.4562353365818446,
      "grad_norm": 19.10744857788086,
      "learning_rate": 1.527016783974012e-05,
      "loss": 0.3747,
      "step": 13610
    },
    {
      "epoch": 2.458040064970222,
      "grad_norm": 16.744871139526367,
      "learning_rate": 1.5259339469409855e-05,
      "loss": 0.5606,
      "step": 13620
    },
    {
      "epoch": 2.4598447933585996,
      "grad_norm": 22.15485382080078,
      "learning_rate": 1.5248511099079588e-05,
      "loss": 0.3546,
      "step": 13630
    },
    {
      "epoch": 2.461649521746977,
      "grad_norm": 24.61246681213379,
      "learning_rate": 1.5237682728749323e-05,
      "loss": 0.5156,
      "step": 13640
    },
    {
      "epoch": 2.4634542501353547,
      "grad_norm": 12.147124290466309,
      "learning_rate": 1.5226854358419057e-05,
      "loss": 0.5309,
      "step": 13650
    },
    {
      "epoch": 2.465258978523732,
      "grad_norm": 15.803064346313477,
      "learning_rate": 1.5216025988088795e-05,
      "loss": 0.3563,
      "step": 13660
    },
    {
      "epoch": 2.4670637069121097,
      "grad_norm": 9.661043167114258,
      "learning_rate": 1.5205197617758528e-05,
      "loss": 0.3299,
      "step": 13670
    },
    {
      "epoch": 2.4688684353004873,
      "grad_norm": 30.170310974121094,
      "learning_rate": 1.5194369247428263e-05,
      "loss": 0.3913,
      "step": 13680
    },
    {
      "epoch": 2.470673163688865,
      "grad_norm": 19.154865264892578,
      "learning_rate": 1.5183540877097996e-05,
      "loss": 0.3014,
      "step": 13690
    },
    {
      "epoch": 2.4724778920772423,
      "grad_norm": 13.68649959564209,
      "learning_rate": 1.5172712506767731e-05,
      "loss": 0.3328,
      "step": 13700
    },
    {
      "epoch": 2.47428262046562,
      "grad_norm": 9.158174514770508,
      "learning_rate": 1.5161884136437464e-05,
      "loss": 0.3818,
      "step": 13710
    },
    {
      "epoch": 2.4760873488539974,
      "grad_norm": 37.04922103881836,
      "learning_rate": 1.5151055766107203e-05,
      "loss": 0.4034,
      "step": 13720
    },
    {
      "epoch": 2.477892077242375,
      "grad_norm": 22.930416107177734,
      "learning_rate": 1.5140227395776937e-05,
      "loss": 0.5665,
      "step": 13730
    },
    {
      "epoch": 2.4796968056307525,
      "grad_norm": 6.278538227081299,
      "learning_rate": 1.512939902544667e-05,
      "loss": 0.2323,
      "step": 13740
    },
    {
      "epoch": 2.48150153401913,
      "grad_norm": 26.952787399291992,
      "learning_rate": 1.5118570655116406e-05,
      "loss": 0.3653,
      "step": 13750
    },
    {
      "epoch": 2.4833062624075075,
      "grad_norm": 11.876333236694336,
      "learning_rate": 1.5107742284786139e-05,
      "loss": 0.4175,
      "step": 13760
    },
    {
      "epoch": 2.485110990795885,
      "grad_norm": 9.467744827270508,
      "learning_rate": 1.5096913914455874e-05,
      "loss": 0.252,
      "step": 13770
    },
    {
      "epoch": 2.4869157191842626,
      "grad_norm": 20.42301368713379,
      "learning_rate": 1.508608554412561e-05,
      "loss": 0.5834,
      "step": 13780
    },
    {
      "epoch": 2.48872044757264,
      "grad_norm": 12.948843002319336,
      "learning_rate": 1.5075257173795345e-05,
      "loss": 0.4306,
      "step": 13790
    },
    {
      "epoch": 2.490525175961018,
      "grad_norm": 20.90077781677246,
      "learning_rate": 1.5064428803465078e-05,
      "loss": 0.3559,
      "step": 13800
    },
    {
      "epoch": 2.4923299043493956,
      "grad_norm": 32.98133850097656,
      "learning_rate": 1.5053600433134813e-05,
      "loss": 0.509,
      "step": 13810
    },
    {
      "epoch": 2.494134632737773,
      "grad_norm": 12.362238883972168,
      "learning_rate": 1.5042772062804546e-05,
      "loss": 0.2779,
      "step": 13820
    },
    {
      "epoch": 2.4959393611261507,
      "grad_norm": 18.732717514038086,
      "learning_rate": 1.5031943692474281e-05,
      "loss": 0.4467,
      "step": 13830
    },
    {
      "epoch": 2.497744089514528,
      "grad_norm": 9.281327247619629,
      "learning_rate": 1.502111532214402e-05,
      "loss": 0.3742,
      "step": 13840
    },
    {
      "epoch": 2.4995488179029057,
      "grad_norm": 26.311111450195312,
      "learning_rate": 1.5010286951813753e-05,
      "loss": 0.5362,
      "step": 13850
    },
    {
      "epoch": 2.5013535462912833,
      "grad_norm": 6.385565280914307,
      "learning_rate": 1.4999458581483488e-05,
      "loss": 0.4995,
      "step": 13860
    },
    {
      "epoch": 2.503158274679661,
      "grad_norm": 16.624483108520508,
      "learning_rate": 1.4988630211153221e-05,
      "loss": 0.5533,
      "step": 13870
    },
    {
      "epoch": 2.5049630030680383,
      "grad_norm": 6.831815719604492,
      "learning_rate": 1.4977801840822958e-05,
      "loss": 0.5314,
      "step": 13880
    },
    {
      "epoch": 2.506767731456416,
      "grad_norm": 13.591545104980469,
      "learning_rate": 1.496697347049269e-05,
      "loss": 0.4636,
      "step": 13890
    },
    {
      "epoch": 2.5085724598447934,
      "grad_norm": 24.72947120666504,
      "learning_rate": 1.4956145100162426e-05,
      "loss": 0.5821,
      "step": 13900
    },
    {
      "epoch": 2.510377188233171,
      "grad_norm": 14.210892677307129,
      "learning_rate": 1.494531672983216e-05,
      "loss": 0.353,
      "step": 13910
    },
    {
      "epoch": 2.5121819166215484,
      "grad_norm": 16.813108444213867,
      "learning_rate": 1.4934488359501896e-05,
      "loss": 0.3705,
      "step": 13920
    },
    {
      "epoch": 2.513986645009926,
      "grad_norm": 3.7867493629455566,
      "learning_rate": 1.492365998917163e-05,
      "loss": 0.393,
      "step": 13930
    },
    {
      "epoch": 2.5157913733983035,
      "grad_norm": 9.903486251831055,
      "learning_rate": 1.4912831618841365e-05,
      "loss": 0.3868,
      "step": 13940
    },
    {
      "epoch": 2.517596101786681,
      "grad_norm": 31.737987518310547,
      "learning_rate": 1.49020032485111e-05,
      "loss": 0.505,
      "step": 13950
    },
    {
      "epoch": 2.5194008301750586,
      "grad_norm": 10.879637718200684,
      "learning_rate": 1.4891174878180833e-05,
      "loss": 0.3395,
      "step": 13960
    },
    {
      "epoch": 2.521205558563436,
      "grad_norm": 25.82683753967285,
      "learning_rate": 1.488034650785057e-05,
      "loss": 0.4877,
      "step": 13970
    },
    {
      "epoch": 2.5230102869518136,
      "grad_norm": 6.253637313842773,
      "learning_rate": 1.4869518137520303e-05,
      "loss": 0.4358,
      "step": 13980
    },
    {
      "epoch": 2.524815015340191,
      "grad_norm": 38.24626159667969,
      "learning_rate": 1.4858689767190038e-05,
      "loss": 0.4048,
      "step": 13990
    },
    {
      "epoch": 2.5266197437285687,
      "grad_norm": 9.988428115844727,
      "learning_rate": 1.4847861396859773e-05,
      "loss": 0.5213,
      "step": 14000
    },
    {
      "epoch": 2.5284244721169467,
      "grad_norm": 11.745634078979492,
      "learning_rate": 1.4837033026529508e-05,
      "loss": 0.4843,
      "step": 14010
    },
    {
      "epoch": 2.5302292005053237,
      "grad_norm": 15.596976280212402,
      "learning_rate": 1.4826204656199243e-05,
      "loss": 0.3493,
      "step": 14020
    },
    {
      "epoch": 2.5320339288937017,
      "grad_norm": 27.003746032714844,
      "learning_rate": 1.4815376285868978e-05,
      "loss": 0.393,
      "step": 14030
    },
    {
      "epoch": 2.533838657282079,
      "grad_norm": 11.031643867492676,
      "learning_rate": 1.4804547915538713e-05,
      "loss": 0.1823,
      "step": 14040
    },
    {
      "epoch": 2.5356433856704568,
      "grad_norm": 31.728586196899414,
      "learning_rate": 1.4793719545208446e-05,
      "loss": 0.5799,
      "step": 14050
    },
    {
      "epoch": 2.5374481140588343,
      "grad_norm": 27.832555770874023,
      "learning_rate": 1.4782891174878182e-05,
      "loss": 0.3127,
      "step": 14060
    },
    {
      "epoch": 2.539252842447212,
      "grad_norm": 8.44970989227295,
      "learning_rate": 1.4772062804547916e-05,
      "loss": 0.3599,
      "step": 14070
    },
    {
      "epoch": 2.5410575708355894,
      "grad_norm": 7.960921764373779,
      "learning_rate": 1.476123443421765e-05,
      "loss": 0.4425,
      "step": 14080
    },
    {
      "epoch": 2.542862299223967,
      "grad_norm": 6.485062122344971,
      "learning_rate": 1.4750406063887386e-05,
      "loss": 0.2594,
      "step": 14090
    },
    {
      "epoch": 2.5446670276123444,
      "grad_norm": 11.996045112609863,
      "learning_rate": 1.473957769355712e-05,
      "loss": 0.5931,
      "step": 14100
    },
    {
      "epoch": 2.546471756000722,
      "grad_norm": 0.9380146265029907,
      "learning_rate": 1.4728749323226854e-05,
      "loss": 0.4347,
      "step": 14110
    },
    {
      "epoch": 2.5482764843890995,
      "grad_norm": 16.088912963867188,
      "learning_rate": 1.471792095289659e-05,
      "loss": 0.3219,
      "step": 14120
    },
    {
      "epoch": 2.550081212777477,
      "grad_norm": 6.854053497314453,
      "learning_rate": 1.4707092582566325e-05,
      "loss": 0.3627,
      "step": 14130
    },
    {
      "epoch": 2.5518859411658545,
      "grad_norm": 23.437088012695312,
      "learning_rate": 1.4696264212236058e-05,
      "loss": 0.5569,
      "step": 14140
    },
    {
      "epoch": 2.553690669554232,
      "grad_norm": 47.3082389831543,
      "learning_rate": 1.4685435841905795e-05,
      "loss": 0.4204,
      "step": 14150
    },
    {
      "epoch": 2.5554953979426096,
      "grad_norm": 14.841504096984863,
      "learning_rate": 1.4674607471575528e-05,
      "loss": 0.3555,
      "step": 14160
    },
    {
      "epoch": 2.557300126330987,
      "grad_norm": 36.0869026184082,
      "learning_rate": 1.4663779101245263e-05,
      "loss": 0.394,
      "step": 14170
    },
    {
      "epoch": 2.5591048547193647,
      "grad_norm": 26.817358016967773,
      "learning_rate": 1.4652950730914998e-05,
      "loss": 0.3792,
      "step": 14180
    },
    {
      "epoch": 2.560909583107742,
      "grad_norm": 10.080792427062988,
      "learning_rate": 1.4642122360584733e-05,
      "loss": 0.266,
      "step": 14190
    },
    {
      "epoch": 2.5627143114961197,
      "grad_norm": 13.445186614990234,
      "learning_rate": 1.4631293990254466e-05,
      "loss": 0.4175,
      "step": 14200
    },
    {
      "epoch": 2.5645190398844973,
      "grad_norm": 20.701875686645508,
      "learning_rate": 1.4620465619924203e-05,
      "loss": 0.4052,
      "step": 14210
    },
    {
      "epoch": 2.566323768272875,
      "grad_norm": 5.072454929351807,
      "learning_rate": 1.4609637249593938e-05,
      "loss": 0.449,
      "step": 14220
    },
    {
      "epoch": 2.5681284966612523,
      "grad_norm": 6.682042121887207,
      "learning_rate": 1.459880887926367e-05,
      "loss": 0.547,
      "step": 14230
    },
    {
      "epoch": 2.5699332250496303,
      "grad_norm": 45.399696350097656,
      "learning_rate": 1.4587980508933406e-05,
      "loss": 0.5049,
      "step": 14240
    },
    {
      "epoch": 2.5717379534380074,
      "grad_norm": 39.44835662841797,
      "learning_rate": 1.457715213860314e-05,
      "loss": 0.3753,
      "step": 14250
    },
    {
      "epoch": 2.5735426818263853,
      "grad_norm": 18.92171859741211,
      "learning_rate": 1.4566323768272875e-05,
      "loss": 0.575,
      "step": 14260
    },
    {
      "epoch": 2.5753474102147624,
      "grad_norm": 17.969865798950195,
      "learning_rate": 1.4555495397942609e-05,
      "loss": 0.3319,
      "step": 14270
    },
    {
      "epoch": 2.5771521386031404,
      "grad_norm": 14.600591659545898,
      "learning_rate": 1.4544667027612345e-05,
      "loss": 0.4793,
      "step": 14280
    },
    {
      "epoch": 2.578956866991518,
      "grad_norm": 38.606109619140625,
      "learning_rate": 1.4533838657282079e-05,
      "loss": 0.4196,
      "step": 14290
    },
    {
      "epoch": 2.5807615953798955,
      "grad_norm": 30.13161277770996,
      "learning_rate": 1.4523010286951813e-05,
      "loss": 0.4994,
      "step": 14300
    },
    {
      "epoch": 2.582566323768273,
      "grad_norm": 25.164331436157227,
      "learning_rate": 1.4512181916621548e-05,
      "loss": 0.4884,
      "step": 14310
    },
    {
      "epoch": 2.5843710521566505,
      "grad_norm": 11.815780639648438,
      "learning_rate": 1.4501353546291283e-05,
      "loss": 0.3058,
      "step": 14320
    },
    {
      "epoch": 2.586175780545028,
      "grad_norm": 26.761878967285156,
      "learning_rate": 1.4490525175961018e-05,
      "loss": 0.3884,
      "step": 14330
    },
    {
      "epoch": 2.5879805089334056,
      "grad_norm": 11.393232345581055,
      "learning_rate": 1.4479696805630753e-05,
      "loss": 0.4915,
      "step": 14340
    },
    {
      "epoch": 2.589785237321783,
      "grad_norm": 20.02547264099121,
      "learning_rate": 1.4468868435300488e-05,
      "loss": 0.3389,
      "step": 14350
    },
    {
      "epoch": 2.5915899657101606,
      "grad_norm": 25.6577091217041,
      "learning_rate": 1.4458040064970221e-05,
      "loss": 0.4429,
      "step": 14360
    },
    {
      "epoch": 2.593394694098538,
      "grad_norm": 12.819981575012207,
      "learning_rate": 1.4447211694639958e-05,
      "loss": 0.5731,
      "step": 14370
    },
    {
      "epoch": 2.5951994224869157,
      "grad_norm": 9.10105037689209,
      "learning_rate": 1.4436383324309691e-05,
      "loss": 0.4172,
      "step": 14380
    },
    {
      "epoch": 2.5970041508752932,
      "grad_norm": 15.050483703613281,
      "learning_rate": 1.4425554953979426e-05,
      "loss": 0.4896,
      "step": 14390
    },
    {
      "epoch": 2.5988088792636708,
      "grad_norm": 14.847765922546387,
      "learning_rate": 1.441472658364916e-05,
      "loss": 0.3415,
      "step": 14400
    },
    {
      "epoch": 2.6006136076520483,
      "grad_norm": 34.50041961669922,
      "learning_rate": 1.4403898213318896e-05,
      "loss": 0.6139,
      "step": 14410
    },
    {
      "epoch": 2.602418336040426,
      "grad_norm": 13.996686935424805,
      "learning_rate": 1.439306984298863e-05,
      "loss": 0.3806,
      "step": 14420
    },
    {
      "epoch": 2.6042230644288034,
      "grad_norm": 14.164484977722168,
      "learning_rate": 1.4382241472658365e-05,
      "loss": 0.4613,
      "step": 14430
    },
    {
      "epoch": 2.606027792817181,
      "grad_norm": 17.8659725189209,
      "learning_rate": 1.43714131023281e-05,
      "loss": 0.5219,
      "step": 14440
    },
    {
      "epoch": 2.6078325212055584,
      "grad_norm": 6.493856430053711,
      "learning_rate": 1.4360584731997834e-05,
      "loss": 0.5263,
      "step": 14450
    },
    {
      "epoch": 2.609637249593936,
      "grad_norm": 6.4301438331604,
      "learning_rate": 1.434975636166757e-05,
      "loss": 0.3443,
      "step": 14460
    },
    {
      "epoch": 2.611441977982314,
      "grad_norm": 19.270652770996094,
      "learning_rate": 1.4338927991337303e-05,
      "loss": 0.6197,
      "step": 14470
    },
    {
      "epoch": 2.613246706370691,
      "grad_norm": 5.041683673858643,
      "learning_rate": 1.4328099621007038e-05,
      "loss": 0.5088,
      "step": 14480
    },
    {
      "epoch": 2.615051434759069,
      "grad_norm": 13.965982437133789,
      "learning_rate": 1.4317271250676773e-05,
      "loss": 0.3512,
      "step": 14490
    },
    {
      "epoch": 2.616856163147446,
      "grad_norm": 1.2618560791015625,
      "learning_rate": 1.4306442880346508e-05,
      "loss": 0.3745,
      "step": 14500
    },
    {
      "epoch": 2.618660891535824,
      "grad_norm": 19.454113006591797,
      "learning_rate": 1.4295614510016243e-05,
      "loss": 0.3831,
      "step": 14510
    },
    {
      "epoch": 2.6204656199242016,
      "grad_norm": 31.29338264465332,
      "learning_rate": 1.4284786139685978e-05,
      "loss": 0.4605,
      "step": 14520
    },
    {
      "epoch": 2.622270348312579,
      "grad_norm": 27.41926383972168,
      "learning_rate": 1.4273957769355713e-05,
      "loss": 0.4431,
      "step": 14530
    },
    {
      "epoch": 2.6240750767009566,
      "grad_norm": 4.865528583526611,
      "learning_rate": 1.4263129399025446e-05,
      "loss": 0.4628,
      "step": 14540
    },
    {
      "epoch": 2.625879805089334,
      "grad_norm": 10.005315780639648,
      "learning_rate": 1.4252301028695183e-05,
      "loss": 0.3376,
      "step": 14550
    },
    {
      "epoch": 2.6276845334777117,
      "grad_norm": 38.2673454284668,
      "learning_rate": 1.4241472658364916e-05,
      "loss": 0.4873,
      "step": 14560
    },
    {
      "epoch": 2.6294892618660892,
      "grad_norm": 11.848621368408203,
      "learning_rate": 1.423064428803465e-05,
      "loss": 0.4659,
      "step": 14570
    },
    {
      "epoch": 2.6312939902544668,
      "grad_norm": 8.954217910766602,
      "learning_rate": 1.4219815917704386e-05,
      "loss": 0.2637,
      "step": 14580
    },
    {
      "epoch": 2.6330987186428443,
      "grad_norm": 28.458356857299805,
      "learning_rate": 1.420898754737412e-05,
      "loss": 0.4352,
      "step": 14590
    },
    {
      "epoch": 2.634903447031222,
      "grad_norm": 3.6371471881866455,
      "learning_rate": 1.4198159177043854e-05,
      "loss": 0.2492,
      "step": 14600
    },
    {
      "epoch": 2.6367081754195993,
      "grad_norm": 26.611003875732422,
      "learning_rate": 1.418733080671359e-05,
      "loss": 0.471,
      "step": 14610
    },
    {
      "epoch": 2.638512903807977,
      "grad_norm": 42.028133392333984,
      "learning_rate": 1.4176502436383325e-05,
      "loss": 0.4188,
      "step": 14620
    },
    {
      "epoch": 2.6403176321963544,
      "grad_norm": 12.504463195800781,
      "learning_rate": 1.4165674066053058e-05,
      "loss": 0.392,
      "step": 14630
    },
    {
      "epoch": 2.642122360584732,
      "grad_norm": 13.717432022094727,
      "learning_rate": 1.4154845695722795e-05,
      "loss": 0.3352,
      "step": 14640
    },
    {
      "epoch": 2.6439270889731095,
      "grad_norm": 11.061332702636719,
      "learning_rate": 1.4144017325392528e-05,
      "loss": 0.229,
      "step": 14650
    },
    {
      "epoch": 2.645731817361487,
      "grad_norm": 28.57322883605957,
      "learning_rate": 1.4133188955062263e-05,
      "loss": 0.3099,
      "step": 14660
    },
    {
      "epoch": 2.6475365457498645,
      "grad_norm": 21.98423957824707,
      "learning_rate": 1.4122360584731998e-05,
      "loss": 0.308,
      "step": 14670
    },
    {
      "epoch": 2.649341274138242,
      "grad_norm": 17.584299087524414,
      "learning_rate": 1.4111532214401733e-05,
      "loss": 0.4789,
      "step": 14680
    },
    {
      "epoch": 2.6511460025266196,
      "grad_norm": 26.959415435791016,
      "learning_rate": 1.4100703844071466e-05,
      "loss": 0.42,
      "step": 14690
    },
    {
      "epoch": 2.6529507309149976,
      "grad_norm": 26.43844223022461,
      "learning_rate": 1.4089875473741203e-05,
      "loss": 0.4675,
      "step": 14700
    },
    {
      "epoch": 2.6547554593033746,
      "grad_norm": 18.124792098999023,
      "learning_rate": 1.4079047103410938e-05,
      "loss": 0.3926,
      "step": 14710
    },
    {
      "epoch": 2.6565601876917526,
      "grad_norm": 18.90839385986328,
      "learning_rate": 1.4068218733080671e-05,
      "loss": 0.3497,
      "step": 14720
    },
    {
      "epoch": 2.6583649160801297,
      "grad_norm": 10.752605438232422,
      "learning_rate": 1.4057390362750408e-05,
      "loss": 0.3536,
      "step": 14730
    },
    {
      "epoch": 2.6601696444685077,
      "grad_norm": 26.327848434448242,
      "learning_rate": 1.404656199242014e-05,
      "loss": 0.288,
      "step": 14740
    },
    {
      "epoch": 2.6619743728568848,
      "grad_norm": 21.8499698638916,
      "learning_rate": 1.4035733622089876e-05,
      "loss": 0.319,
      "step": 14750
    },
    {
      "epoch": 2.6637791012452627,
      "grad_norm": 37.01200866699219,
      "learning_rate": 1.402490525175961e-05,
      "loss": 0.4107,
      "step": 14760
    },
    {
      "epoch": 2.6655838296336403,
      "grad_norm": 44.252098083496094,
      "learning_rate": 1.4014076881429345e-05,
      "loss": 0.5189,
      "step": 14770
    },
    {
      "epoch": 2.667388558022018,
      "grad_norm": 36.45954132080078,
      "learning_rate": 1.4003248511099079e-05,
      "loss": 0.4531,
      "step": 14780
    },
    {
      "epoch": 2.6691932864103953,
      "grad_norm": 27.363162994384766,
      "learning_rate": 1.3992420140768815e-05,
      "loss": 0.2747,
      "step": 14790
    },
    {
      "epoch": 2.670998014798773,
      "grad_norm": 17.21931266784668,
      "learning_rate": 1.398159177043855e-05,
      "loss": 0.3937,
      "step": 14800
    },
    {
      "epoch": 2.6728027431871504,
      "grad_norm": 19.990938186645508,
      "learning_rate": 1.3970763400108283e-05,
      "loss": 0.3527,
      "step": 14810
    },
    {
      "epoch": 2.674607471575528,
      "grad_norm": 38.500240325927734,
      "learning_rate": 1.395993502977802e-05,
      "loss": 0.42,
      "step": 14820
    },
    {
      "epoch": 2.6764121999639054,
      "grad_norm": 29.74582290649414,
      "learning_rate": 1.3949106659447753e-05,
      "loss": 0.4084,
      "step": 14830
    },
    {
      "epoch": 2.678216928352283,
      "grad_norm": 19.76078224182129,
      "learning_rate": 1.3938278289117488e-05,
      "loss": 0.3942,
      "step": 14840
    },
    {
      "epoch": 2.6800216567406605,
      "grad_norm": 4.419131278991699,
      "learning_rate": 1.3927449918787223e-05,
      "loss": 0.4053,
      "step": 14850
    },
    {
      "epoch": 2.681826385129038,
      "grad_norm": 27.710556030273438,
      "learning_rate": 1.3916621548456958e-05,
      "loss": 0.3368,
      "step": 14860
    },
    {
      "epoch": 2.6836311135174156,
      "grad_norm": 17.368886947631836,
      "learning_rate": 1.3905793178126691e-05,
      "loss": 0.3736,
      "step": 14870
    },
    {
      "epoch": 2.685435841905793,
      "grad_norm": 20.278793334960938,
      "learning_rate": 1.3894964807796428e-05,
      "loss": 0.4067,
      "step": 14880
    },
    {
      "epoch": 2.6872405702941706,
      "grad_norm": 28.311328887939453,
      "learning_rate": 1.3885219274499188e-05,
      "loss": 0.3414,
      "step": 14890
    },
    {
      "epoch": 2.689045298682548,
      "grad_norm": 11.61982250213623,
      "learning_rate": 1.3874390904168924e-05,
      "loss": 0.4668,
      "step": 14900
    },
    {
      "epoch": 2.6908500270709257,
      "grad_norm": 7.5439934730529785,
      "learning_rate": 1.3863562533838658e-05,
      "loss": 0.3621,
      "step": 14910
    },
    {
      "epoch": 2.692654755459303,
      "grad_norm": 8.468953132629395,
      "learning_rate": 1.3852734163508392e-05,
      "loss": 0.3964,
      "step": 14920
    },
    {
      "epoch": 2.694459483847681,
      "grad_norm": 40.31131362915039,
      "learning_rate": 1.3841905793178126e-05,
      "loss": 0.5864,
      "step": 14930
    },
    {
      "epoch": 2.6962642122360583,
      "grad_norm": 11.41552448272705,
      "learning_rate": 1.3831077422847862e-05,
      "loss": 0.4558,
      "step": 14940
    },
    {
      "epoch": 2.6980689406244363,
      "grad_norm": 43.79143142700195,
      "learning_rate": 1.3820249052517595e-05,
      "loss": 0.451,
      "step": 14950
    },
    {
      "epoch": 2.6998736690128133,
      "grad_norm": 9.085685729980469,
      "learning_rate": 1.380942068218733e-05,
      "loss": 0.3812,
      "step": 14960
    },
    {
      "epoch": 2.7016783974011913,
      "grad_norm": 17.393869400024414,
      "learning_rate": 1.3798592311857067e-05,
      "loss": 0.4722,
      "step": 14970
    },
    {
      "epoch": 2.7034831257895684,
      "grad_norm": 23.400819778442383,
      "learning_rate": 1.37877639415268e-05,
      "loss": 0.3878,
      "step": 14980
    },
    {
      "epoch": 2.7052878541779464,
      "grad_norm": 17.67108154296875,
      "learning_rate": 1.3776935571196535e-05,
      "loss": 0.4016,
      "step": 14990
    },
    {
      "epoch": 2.707092582566324,
      "grad_norm": 12.3351469039917,
      "learning_rate": 1.376610720086627e-05,
      "loss": 0.4129,
      "step": 15000
    },
    {
      "epoch": 2.7088973109547014,
      "grad_norm": 8.67524528503418,
      "learning_rate": 1.3755278830536005e-05,
      "loss": 0.4722,
      "step": 15010
    },
    {
      "epoch": 2.710702039343079,
      "grad_norm": 12.939950942993164,
      "learning_rate": 1.3744450460205738e-05,
      "loss": 0.3672,
      "step": 15020
    },
    {
      "epoch": 2.7125067677314565,
      "grad_norm": 17.29833221435547,
      "learning_rate": 1.3733622089875475e-05,
      "loss": 0.4324,
      "step": 15030
    },
    {
      "epoch": 2.714311496119834,
      "grad_norm": 21.117013931274414,
      "learning_rate": 1.3722793719545208e-05,
      "loss": 0.5043,
      "step": 15040
    },
    {
      "epoch": 2.7161162245082116,
      "grad_norm": 23.401935577392578,
      "learning_rate": 1.3711965349214943e-05,
      "loss": 0.3821,
      "step": 15050
    },
    {
      "epoch": 2.717920952896589,
      "grad_norm": 25.929670333862305,
      "learning_rate": 1.3701136978884678e-05,
      "loss": 0.323,
      "step": 15060
    },
    {
      "epoch": 2.7197256812849666,
      "grad_norm": 39.37320327758789,
      "learning_rate": 1.3690308608554413e-05,
      "loss": 0.3819,
      "step": 15070
    },
    {
      "epoch": 2.721530409673344,
      "grad_norm": 20.614349365234375,
      "learning_rate": 1.3679480238224148e-05,
      "loss": 0.3959,
      "step": 15080
    },
    {
      "epoch": 2.7233351380617217,
      "grad_norm": 24.667551040649414,
      "learning_rate": 1.3668651867893882e-05,
      "loss": 0.4119,
      "step": 15090
    },
    {
      "epoch": 2.725139866450099,
      "grad_norm": 11.030952453613281,
      "learning_rate": 1.3657823497563617e-05,
      "loss": 0.3926,
      "step": 15100
    },
    {
      "epoch": 2.7269445948384767,
      "grad_norm": 4.310933589935303,
      "learning_rate": 1.364699512723335e-05,
      "loss": 0.3209,
      "step": 15110
    },
    {
      "epoch": 2.7287493232268543,
      "grad_norm": 23.640567779541016,
      "learning_rate": 1.3636166756903087e-05,
      "loss": 0.4591,
      "step": 15120
    },
    {
      "epoch": 2.730554051615232,
      "grad_norm": 29.499000549316406,
      "learning_rate": 1.362533838657282e-05,
      "loss": 0.489,
      "step": 15130
    },
    {
      "epoch": 2.7323587800036093,
      "grad_norm": 8.818421363830566,
      "learning_rate": 1.3614510016242555e-05,
      "loss": 0.3848,
      "step": 15140
    },
    {
      "epoch": 2.734163508391987,
      "grad_norm": 34.08946990966797,
      "learning_rate": 1.360368164591229e-05,
      "loss": 0.3234,
      "step": 15150
    },
    {
      "epoch": 2.735968236780365,
      "grad_norm": 12.14125919342041,
      "learning_rate": 1.3592853275582025e-05,
      "loss": 0.3885,
      "step": 15160
    },
    {
      "epoch": 2.737772965168742,
      "grad_norm": 14.478303909301758,
      "learning_rate": 1.358202490525176e-05,
      "loss": 0.2401,
      "step": 15170
    },
    {
      "epoch": 2.73957769355712,
      "grad_norm": 12.734464645385742,
      "learning_rate": 1.3571196534921495e-05,
      "loss": 0.6349,
      "step": 15180
    },
    {
      "epoch": 2.741382421945497,
      "grad_norm": 24.779464721679688,
      "learning_rate": 1.356036816459123e-05,
      "loss": 0.3295,
      "step": 15190
    },
    {
      "epoch": 2.743187150333875,
      "grad_norm": 7.214901924133301,
      "learning_rate": 1.3549539794260963e-05,
      "loss": 0.3826,
      "step": 15200
    },
    {
      "epoch": 2.744991878722252,
      "grad_norm": 26.360355377197266,
      "learning_rate": 1.35387114239307e-05,
      "loss": 0.3656,
      "step": 15210
    },
    {
      "epoch": 2.74679660711063,
      "grad_norm": 23.741594314575195,
      "learning_rate": 1.3527883053600433e-05,
      "loss": 0.5629,
      "step": 15220
    },
    {
      "epoch": 2.7486013354990075,
      "grad_norm": 25.571258544921875,
      "learning_rate": 1.3517054683270168e-05,
      "loss": 0.3566,
      "step": 15230
    },
    {
      "epoch": 2.750406063887385,
      "grad_norm": 14.687530517578125,
      "learning_rate": 1.3506226312939903e-05,
      "loss": 0.4947,
      "step": 15240
    },
    {
      "epoch": 2.7522107922757626,
      "grad_norm": 16.158191680908203,
      "learning_rate": 1.3495397942609638e-05,
      "loss": 0.3323,
      "step": 15250
    },
    {
      "epoch": 2.75401552066414,
      "grad_norm": 18.570646286010742,
      "learning_rate": 1.3484569572279372e-05,
      "loss": 0.5056,
      "step": 15260
    },
    {
      "epoch": 2.7558202490525177,
      "grad_norm": 23.655466079711914,
      "learning_rate": 1.3473741201949107e-05,
      "loss": 0.4407,
      "step": 15270
    },
    {
      "epoch": 2.757624977440895,
      "grad_norm": 1.6861408948898315,
      "learning_rate": 1.3462912831618842e-05,
      "loss": 0.5839,
      "step": 15280
    },
    {
      "epoch": 2.7594297058292727,
      "grad_norm": 19.276138305664062,
      "learning_rate": 1.3452084461288575e-05,
      "loss": 0.461,
      "step": 15290
    },
    {
      "epoch": 2.7612344342176502,
      "grad_norm": 12.423588752746582,
      "learning_rate": 1.3441256090958312e-05,
      "loss": 0.533,
      "step": 15300
    },
    {
      "epoch": 2.7630391626060278,
      "grad_norm": 10.595080375671387,
      "learning_rate": 1.3430427720628045e-05,
      "loss": 0.2539,
      "step": 15310
    },
    {
      "epoch": 2.7648438909944053,
      "grad_norm": 28.46795654296875,
      "learning_rate": 1.341959935029778e-05,
      "loss": 0.5201,
      "step": 15320
    },
    {
      "epoch": 2.766648619382783,
      "grad_norm": 21.59210205078125,
      "learning_rate": 1.3408770979967515e-05,
      "loss": 0.3727,
      "step": 15330
    },
    {
      "epoch": 2.7684533477711604,
      "grad_norm": 17.46586036682129,
      "learning_rate": 1.339794260963725e-05,
      "loss": 0.4784,
      "step": 15340
    },
    {
      "epoch": 2.770258076159538,
      "grad_norm": 23.12289810180664,
      "learning_rate": 1.3387114239306983e-05,
      "loss": 0.4726,
      "step": 15350
    },
    {
      "epoch": 2.7720628045479154,
      "grad_norm": 24.377246856689453,
      "learning_rate": 1.337628586897672e-05,
      "loss": 0.3425,
      "step": 15360
    },
    {
      "epoch": 2.773867532936293,
      "grad_norm": 24.793739318847656,
      "learning_rate": 1.3365457498646455e-05,
      "loss": 0.3811,
      "step": 15370
    },
    {
      "epoch": 2.7756722613246705,
      "grad_norm": 36.934879302978516,
      "learning_rate": 1.3354629128316188e-05,
      "loss": 0.3638,
      "step": 15380
    },
    {
      "epoch": 2.7774769897130485,
      "grad_norm": 25.540903091430664,
      "learning_rate": 1.3343800757985924e-05,
      "loss": 0.519,
      "step": 15390
    },
    {
      "epoch": 2.7792817181014255,
      "grad_norm": 12.746162414550781,
      "learning_rate": 1.3332972387655658e-05,
      "loss": 0.3484,
      "step": 15400
    },
    {
      "epoch": 2.7810864464898035,
      "grad_norm": 11.55698013305664,
      "learning_rate": 1.3322144017325393e-05,
      "loss": 0.3896,
      "step": 15410
    },
    {
      "epoch": 2.7828911748781806,
      "grad_norm": 17.7427921295166,
      "learning_rate": 1.3311315646995127e-05,
      "loss": 0.309,
      "step": 15420
    },
    {
      "epoch": 2.7846959032665586,
      "grad_norm": 17.560216903686523,
      "learning_rate": 1.3300487276664862e-05,
      "loss": 0.4125,
      "step": 15430
    },
    {
      "epoch": 2.7865006316549357,
      "grad_norm": 10.565731048583984,
      "learning_rate": 1.3289658906334596e-05,
      "loss": 0.51,
      "step": 15440
    },
    {
      "epoch": 2.7883053600433136,
      "grad_norm": 25.287260055541992,
      "learning_rate": 1.3278830536004332e-05,
      "loss": 0.5419,
      "step": 15450
    },
    {
      "epoch": 2.790110088431691,
      "grad_norm": 12.37594223022461,
      "learning_rate": 1.3268002165674067e-05,
      "loss": 0.4291,
      "step": 15460
    },
    {
      "epoch": 2.7919148168200687,
      "grad_norm": 11.599331855773926,
      "learning_rate": 1.32571737953438e-05,
      "loss": 0.3264,
      "step": 15470
    },
    {
      "epoch": 2.7937195452084462,
      "grad_norm": 8.062040328979492,
      "learning_rate": 1.3246345425013537e-05,
      "loss": 0.405,
      "step": 15480
    },
    {
      "epoch": 2.7955242735968238,
      "grad_norm": 4.559662342071533,
      "learning_rate": 1.323551705468327e-05,
      "loss": 0.2846,
      "step": 15490
    },
    {
      "epoch": 2.7973290019852013,
      "grad_norm": 9.875929832458496,
      "learning_rate": 1.3224688684353005e-05,
      "loss": 0.3663,
      "step": 15500
    },
    {
      "epoch": 2.799133730373579,
      "grad_norm": 29.60851287841797,
      "learning_rate": 1.321386031402274e-05,
      "loss": 0.4523,
      "step": 15510
    },
    {
      "epoch": 2.8009384587619564,
      "grad_norm": 6.363071918487549,
      "learning_rate": 1.3203031943692475e-05,
      "loss": 0.3933,
      "step": 15520
    },
    {
      "epoch": 2.802743187150334,
      "grad_norm": 5.660665988922119,
      "learning_rate": 1.3192203573362208e-05,
      "loss": 0.4024,
      "step": 15530
    },
    {
      "epoch": 2.8045479155387114,
      "grad_norm": 4.816920757293701,
      "learning_rate": 1.3181375203031945e-05,
      "loss": 0.3575,
      "step": 15540
    },
    {
      "epoch": 2.806352643927089,
      "grad_norm": 22.251588821411133,
      "learning_rate": 1.317054683270168e-05,
      "loss": 0.4479,
      "step": 15550
    },
    {
      "epoch": 2.8081573723154665,
      "grad_norm": 16.951915740966797,
      "learning_rate": 1.3159718462371413e-05,
      "loss": 0.4309,
      "step": 15560
    },
    {
      "epoch": 2.809962100703844,
      "grad_norm": 17.8449764251709,
      "learning_rate": 1.314889009204115e-05,
      "loss": 0.4585,
      "step": 15570
    },
    {
      "epoch": 2.8117668290922215,
      "grad_norm": 26.2398681640625,
      "learning_rate": 1.3138061721710883e-05,
      "loss": 0.561,
      "step": 15580
    },
    {
      "epoch": 2.813571557480599,
      "grad_norm": 19.999984741210938,
      "learning_rate": 1.3127233351380617e-05,
      "loss": 0.3669,
      "step": 15590
    },
    {
      "epoch": 2.8153762858689766,
      "grad_norm": 25.950565338134766,
      "learning_rate": 1.3116404981050352e-05,
      "loss": 0.4958,
      "step": 15600
    },
    {
      "epoch": 2.817181014257354,
      "grad_norm": 16.220365524291992,
      "learning_rate": 1.3105576610720087e-05,
      "loss": 0.4514,
      "step": 15610
    },
    {
      "epoch": 2.8189857426457317,
      "grad_norm": 11.107181549072266,
      "learning_rate": 1.309474824038982e-05,
      "loss": 0.3225,
      "step": 15620
    },
    {
      "epoch": 2.820790471034109,
      "grad_norm": 2.443264961242676,
      "learning_rate": 1.3083919870059557e-05,
      "loss": 0.373,
      "step": 15630
    },
    {
      "epoch": 2.822595199422487,
      "grad_norm": 15.713481903076172,
      "learning_rate": 1.307309149972929e-05,
      "loss": 0.4792,
      "step": 15640
    },
    {
      "epoch": 2.8243999278108642,
      "grad_norm": 4.1414265632629395,
      "learning_rate": 1.3062263129399025e-05,
      "loss": 0.4949,
      "step": 15650
    },
    {
      "epoch": 2.826204656199242,
      "grad_norm": 10.242949485778809,
      "learning_rate": 1.3051434759068762e-05,
      "loss": 0.4963,
      "step": 15660
    },
    {
      "epoch": 2.8280093845876193,
      "grad_norm": 15.805066108703613,
      "learning_rate": 1.3040606388738495e-05,
      "loss": 0.602,
      "step": 15670
    },
    {
      "epoch": 2.8298141129759973,
      "grad_norm": 11.550792694091797,
      "learning_rate": 1.302977801840823e-05,
      "loss": 0.4963,
      "step": 15680
    },
    {
      "epoch": 2.831618841364375,
      "grad_norm": 18.348705291748047,
      "learning_rate": 1.3018949648077965e-05,
      "loss": 0.4525,
      "step": 15690
    },
    {
      "epoch": 2.8334235697527523,
      "grad_norm": 13.355380058288574,
      "learning_rate": 1.30081212777477e-05,
      "loss": 0.3781,
      "step": 15700
    },
    {
      "epoch": 2.83522829814113,
      "grad_norm": 42.92998123168945,
      "learning_rate": 1.2997292907417433e-05,
      "loss": 0.3519,
      "step": 15710
    },
    {
      "epoch": 2.8370330265295074,
      "grad_norm": 19.38536834716797,
      "learning_rate": 1.298646453708717e-05,
      "loss": 0.4258,
      "step": 15720
    },
    {
      "epoch": 2.838837754917885,
      "grad_norm": 28.155094146728516,
      "learning_rate": 1.2975636166756903e-05,
      "loss": 0.2394,
      "step": 15730
    },
    {
      "epoch": 2.8406424833062625,
      "grad_norm": 17.12363052368164,
      "learning_rate": 1.2964807796426638e-05,
      "loss": 0.42,
      "step": 15740
    },
    {
      "epoch": 2.84244721169464,
      "grad_norm": 25.92533302307129,
      "learning_rate": 1.2953979426096374e-05,
      "loss": 0.6239,
      "step": 15750
    },
    {
      "epoch": 2.8442519400830175,
      "grad_norm": 8.370918273925781,
      "learning_rate": 1.2943151055766107e-05,
      "loss": 0.5203,
      "step": 15760
    },
    {
      "epoch": 2.846056668471395,
      "grad_norm": 9.083944320678711,
      "learning_rate": 1.2932322685435842e-05,
      "loss": 0.3834,
      "step": 15770
    },
    {
      "epoch": 2.8478613968597726,
      "grad_norm": 22.77663803100586,
      "learning_rate": 1.2921494315105577e-05,
      "loss": 0.4986,
      "step": 15780
    },
    {
      "epoch": 2.84966612524815,
      "grad_norm": 7.621475696563721,
      "learning_rate": 1.2910665944775312e-05,
      "loss": 0.4087,
      "step": 15790
    },
    {
      "epoch": 2.8514708536365276,
      "grad_norm": 12.868104934692383,
      "learning_rate": 1.2899837574445045e-05,
      "loss": 0.5285,
      "step": 15800
    },
    {
      "epoch": 2.853275582024905,
      "grad_norm": 15.534712791442871,
      "learning_rate": 1.2889009204114782e-05,
      "loss": 0.5405,
      "step": 15810
    },
    {
      "epoch": 2.8550803104132827,
      "grad_norm": 8.595139503479004,
      "learning_rate": 1.2878180833784515e-05,
      "loss": 0.4585,
      "step": 15820
    },
    {
      "epoch": 2.8568850388016602,
      "grad_norm": 5.587933540344238,
      "learning_rate": 1.286735246345425e-05,
      "loss": 0.3309,
      "step": 15830
    },
    {
      "epoch": 2.8586897671900378,
      "grad_norm": 23.696788787841797,
      "learning_rate": 1.2856524093123985e-05,
      "loss": 0.4843,
      "step": 15840
    },
    {
      "epoch": 2.8604944955784153,
      "grad_norm": 8.578890800476074,
      "learning_rate": 1.284569572279372e-05,
      "loss": 0.4896,
      "step": 15850
    },
    {
      "epoch": 2.862299223966793,
      "grad_norm": 12.549226760864258,
      "learning_rate": 1.2834867352463455e-05,
      "loss": 0.3465,
      "step": 15860
    },
    {
      "epoch": 2.864103952355171,
      "grad_norm": 11.847793579101562,
      "learning_rate": 1.282403898213319e-05,
      "loss": 0.3086,
      "step": 15870
    },
    {
      "epoch": 2.865908680743548,
      "grad_norm": 24.98706817626953,
      "learning_rate": 1.2813210611802925e-05,
      "loss": 0.3286,
      "step": 15880
    },
    {
      "epoch": 2.867713409131926,
      "grad_norm": 26.558359146118164,
      "learning_rate": 1.2802382241472658e-05,
      "loss": 0.3937,
      "step": 15890
    },
    {
      "epoch": 2.869518137520303,
      "grad_norm": 20.908390045166016,
      "learning_rate": 1.2791553871142394e-05,
      "loss": 0.4081,
      "step": 15900
    },
    {
      "epoch": 2.871322865908681,
      "grad_norm": 29.621898651123047,
      "learning_rate": 1.2780725500812128e-05,
      "loss": 0.4871,
      "step": 15910
    },
    {
      "epoch": 2.8731275942970584,
      "grad_norm": 12.381193161010742,
      "learning_rate": 1.2769897130481863e-05,
      "loss": 0.3268,
      "step": 15920
    },
    {
      "epoch": 2.874932322685436,
      "grad_norm": 17.992082595825195,
      "learning_rate": 1.2759068760151597e-05,
      "loss": 0.4287,
      "step": 15930
    },
    {
      "epoch": 2.8767370510738135,
      "grad_norm": 16.01919937133789,
      "learning_rate": 1.2748240389821332e-05,
      "loss": 0.4475,
      "step": 15940
    },
    {
      "epoch": 2.878541779462191,
      "grad_norm": 19.524852752685547,
      "learning_rate": 1.2737412019491067e-05,
      "loss": 0.3911,
      "step": 15950
    },
    {
      "epoch": 2.8803465078505686,
      "grad_norm": 18.752363204956055,
      "learning_rate": 1.2726583649160802e-05,
      "loss": 0.365,
      "step": 15960
    },
    {
      "epoch": 2.882151236238946,
      "grad_norm": 21.34170913696289,
      "learning_rate": 1.2715755278830537e-05,
      "loss": 0.4224,
      "step": 15970
    },
    {
      "epoch": 2.8839559646273236,
      "grad_norm": 2.9745874404907227,
      "learning_rate": 1.270492690850027e-05,
      "loss": 0.3826,
      "step": 15980
    },
    {
      "epoch": 2.885760693015701,
      "grad_norm": 9.814630508422852,
      "learning_rate": 1.2694098538170007e-05,
      "loss": 0.5135,
      "step": 15990
    },
    {
      "epoch": 2.8875654214040787,
      "grad_norm": 22.03261947631836,
      "learning_rate": 1.268327016783974e-05,
      "loss": 0.4952,
      "step": 16000
    },
    {
      "epoch": 2.889370149792456,
      "grad_norm": 15.534716606140137,
      "learning_rate": 1.2672441797509475e-05,
      "loss": 0.4184,
      "step": 16010
    },
    {
      "epoch": 2.8911748781808337,
      "grad_norm": 30.448768615722656,
      "learning_rate": 1.266161342717921e-05,
      "loss": 0.372,
      "step": 16020
    },
    {
      "epoch": 2.8929796065692113,
      "grad_norm": 13.06567096710205,
      "learning_rate": 1.2650785056848945e-05,
      "loss": 0.3538,
      "step": 16030
    },
    {
      "epoch": 2.894784334957589,
      "grad_norm": 47.124481201171875,
      "learning_rate": 1.263995668651868e-05,
      "loss": 0.3245,
      "step": 16040
    },
    {
      "epoch": 2.8965890633459663,
      "grad_norm": 31.506786346435547,
      "learning_rate": 1.2629128316188415e-05,
      "loss": 0.501,
      "step": 16050
    },
    {
      "epoch": 2.898393791734344,
      "grad_norm": 11.097862243652344,
      "learning_rate": 1.261829994585815e-05,
      "loss": 0.4216,
      "step": 16060
    },
    {
      "epoch": 2.9001985201227214,
      "grad_norm": 12.632270812988281,
      "learning_rate": 1.2607471575527883e-05,
      "loss": 0.4534,
      "step": 16070
    },
    {
      "epoch": 2.902003248511099,
      "grad_norm": 11.04045581817627,
      "learning_rate": 1.259664320519762e-05,
      "loss": 0.3523,
      "step": 16080
    },
    {
      "epoch": 2.9038079768994765,
      "grad_norm": 21.363208770751953,
      "learning_rate": 1.2585814834867353e-05,
      "loss": 0.4374,
      "step": 16090
    },
    {
      "epoch": 2.9056127052878544,
      "grad_norm": 28.798519134521484,
      "learning_rate": 1.2574986464537087e-05,
      "loss": 0.4142,
      "step": 16100
    },
    {
      "epoch": 2.9074174336762315,
      "grad_norm": 25.90221405029297,
      "learning_rate": 1.2564158094206822e-05,
      "loss": 0.3104,
      "step": 16110
    },
    {
      "epoch": 2.9092221620646095,
      "grad_norm": 14.821971893310547,
      "learning_rate": 1.2553329723876557e-05,
      "loss": 0.3952,
      "step": 16120
    },
    {
      "epoch": 2.9110268904529866,
      "grad_norm": 13.12425708770752,
      "learning_rate": 1.254250135354629e-05,
      "loss": 0.368,
      "step": 16130
    },
    {
      "epoch": 2.9128316188413645,
      "grad_norm": 22.04303550720215,
      "learning_rate": 1.2531672983216027e-05,
      "loss": 0.472,
      "step": 16140
    },
    {
      "epoch": 2.914636347229742,
      "grad_norm": 18.215232849121094,
      "learning_rate": 1.2520844612885762e-05,
      "loss": 0.3372,
      "step": 16150
    },
    {
      "epoch": 2.9164410756181196,
      "grad_norm": 5.446784019470215,
      "learning_rate": 1.2510016242555495e-05,
      "loss": 0.3828,
      "step": 16160
    },
    {
      "epoch": 2.918245804006497,
      "grad_norm": 12.813163757324219,
      "learning_rate": 1.2499187872225232e-05,
      "loss": 0.3761,
      "step": 16170
    },
    {
      "epoch": 2.9200505323948747,
      "grad_norm": 44.050506591796875,
      "learning_rate": 1.2488359501894965e-05,
      "loss": 0.5587,
      "step": 16180
    },
    {
      "epoch": 2.921855260783252,
      "grad_norm": 12.916165351867676,
      "learning_rate": 1.24775311315647e-05,
      "loss": 0.5866,
      "step": 16190
    },
    {
      "epoch": 2.9236599891716297,
      "grad_norm": 15.082343101501465,
      "learning_rate": 1.2466702761234435e-05,
      "loss": 0.3675,
      "step": 16200
    },
    {
      "epoch": 2.9254647175600073,
      "grad_norm": 29.438495635986328,
      "learning_rate": 1.245587439090417e-05,
      "loss": 0.4001,
      "step": 16210
    },
    {
      "epoch": 2.927269445948385,
      "grad_norm": 6.3017168045043945,
      "learning_rate": 1.2445046020573903e-05,
      "loss": 0.3339,
      "step": 16220
    },
    {
      "epoch": 2.9290741743367623,
      "grad_norm": 19.915918350219727,
      "learning_rate": 1.243421765024364e-05,
      "loss": 0.4102,
      "step": 16230
    },
    {
      "epoch": 2.93087890272514,
      "grad_norm": 8.85784912109375,
      "learning_rate": 1.2423389279913374e-05,
      "loss": 0.5379,
      "step": 16240
    },
    {
      "epoch": 2.9326836311135174,
      "grad_norm": 13.594500541687012,
      "learning_rate": 1.2412560909583108e-05,
      "loss": 0.487,
      "step": 16250
    },
    {
      "epoch": 2.934488359501895,
      "grad_norm": 13.441912651062012,
      "learning_rate": 1.2401732539252844e-05,
      "loss": 0.379,
      "step": 16260
    },
    {
      "epoch": 2.9362930878902724,
      "grad_norm": 29.656476974487305,
      "learning_rate": 1.2390904168922577e-05,
      "loss": 0.4828,
      "step": 16270
    },
    {
      "epoch": 2.93809781627865,
      "grad_norm": 24.34173583984375,
      "learning_rate": 1.2380075798592312e-05,
      "loss": 0.3802,
      "step": 16280
    },
    {
      "epoch": 2.9399025446670275,
      "grad_norm": 16.709369659423828,
      "learning_rate": 1.2369247428262047e-05,
      "loss": 0.5429,
      "step": 16290
    },
    {
      "epoch": 2.941707273055405,
      "grad_norm": 8.700211524963379,
      "learning_rate": 1.2358419057931782e-05,
      "loss": 0.2718,
      "step": 16300
    },
    {
      "epoch": 2.9435120014437826,
      "grad_norm": 30.16515350341797,
      "learning_rate": 1.2347590687601515e-05,
      "loss": 0.4665,
      "step": 16310
    },
    {
      "epoch": 2.94531672983216,
      "grad_norm": 7.251625061035156,
      "learning_rate": 1.2336762317271252e-05,
      "loss": 0.5609,
      "step": 16320
    },
    {
      "epoch": 2.947121458220538,
      "grad_norm": 11.841161727905273,
      "learning_rate": 1.2325933946940987e-05,
      "loss": 0.3448,
      "step": 16330
    },
    {
      "epoch": 2.948926186608915,
      "grad_norm": 13.790623664855957,
      "learning_rate": 1.231510557661072e-05,
      "loss": 0.4981,
      "step": 16340
    },
    {
      "epoch": 2.950730914997293,
      "grad_norm": 4.833889484405518,
      "learning_rate": 1.2304277206280455e-05,
      "loss": 0.3579,
      "step": 16350
    },
    {
      "epoch": 2.95253564338567,
      "grad_norm": 24.314510345458984,
      "learning_rate": 1.229344883595019e-05,
      "loss": 0.415,
      "step": 16360
    },
    {
      "epoch": 2.954340371774048,
      "grad_norm": 30.867650985717773,
      "learning_rate": 1.2282620465619925e-05,
      "loss": 0.5404,
      "step": 16370
    },
    {
      "epoch": 2.9561451001624257,
      "grad_norm": 6.806820869445801,
      "learning_rate": 1.2271792095289658e-05,
      "loss": 0.4882,
      "step": 16380
    },
    {
      "epoch": 2.9579498285508032,
      "grad_norm": 17.04306983947754,
      "learning_rate": 1.2260963724959395e-05,
      "loss": 0.6244,
      "step": 16390
    },
    {
      "epoch": 2.9597545569391808,
      "grad_norm": 16.117345809936523,
      "learning_rate": 1.2250135354629128e-05,
      "loss": 0.4282,
      "step": 16400
    },
    {
      "epoch": 2.9615592853275583,
      "grad_norm": 9.941349029541016,
      "learning_rate": 1.2239306984298863e-05,
      "loss": 0.3099,
      "step": 16410
    },
    {
      "epoch": 2.963364013715936,
      "grad_norm": 18.57317543029785,
      "learning_rate": 1.2228478613968598e-05,
      "loss": 0.4416,
      "step": 16420
    },
    {
      "epoch": 2.9651687421043134,
      "grad_norm": 21.278175354003906,
      "learning_rate": 1.2217650243638332e-05,
      "loss": 0.3272,
      "step": 16430
    },
    {
      "epoch": 2.966973470492691,
      "grad_norm": 16.682796478271484,
      "learning_rate": 1.2206821873308067e-05,
      "loss": 0.3863,
      "step": 16440
    },
    {
      "epoch": 2.9687781988810684,
      "grad_norm": 10.446466445922852,
      "learning_rate": 1.2195993502977802e-05,
      "loss": 0.3131,
      "step": 16450
    },
    {
      "epoch": 2.970582927269446,
      "grad_norm": 13.412257194519043,
      "learning_rate": 1.2185165132647537e-05,
      "loss": 0.374,
      "step": 16460
    },
    {
      "epoch": 2.9723876556578235,
      "grad_norm": 21.358383178710938,
      "learning_rate": 1.217433676231727e-05,
      "loss": 0.3712,
      "step": 16470
    },
    {
      "epoch": 2.974192384046201,
      "grad_norm": 9.527297019958496,
      "learning_rate": 1.2163508391987007e-05,
      "loss": 0.4791,
      "step": 16480
    },
    {
      "epoch": 2.9759971124345785,
      "grad_norm": 10.772530555725098,
      "learning_rate": 1.215268002165674e-05,
      "loss": 0.3885,
      "step": 16490
    },
    {
      "epoch": 2.977801840822956,
      "grad_norm": 13.74752140045166,
      "learning_rate": 1.2141851651326475e-05,
      "loss": 0.4476,
      "step": 16500
    },
    {
      "epoch": 2.9796065692113336,
      "grad_norm": 14.334566116333008,
      "learning_rate": 1.213102328099621e-05,
      "loss": 0.3854,
      "step": 16510
    },
    {
      "epoch": 2.981411297599711,
      "grad_norm": 27.439905166625977,
      "learning_rate": 1.2120194910665945e-05,
      "loss": 0.3074,
      "step": 16520
    },
    {
      "epoch": 2.9832160259880887,
      "grad_norm": 34.34479522705078,
      "learning_rate": 1.210936654033568e-05,
      "loss": 0.5127,
      "step": 16530
    },
    {
      "epoch": 2.985020754376466,
      "grad_norm": 13.278923988342285,
      "learning_rate": 1.2098538170005415e-05,
      "loss": 0.4854,
      "step": 16540
    },
    {
      "epoch": 2.9868254827648437,
      "grad_norm": 20.130970001220703,
      "learning_rate": 1.208770979967515e-05,
      "loss": 0.3216,
      "step": 16550
    },
    {
      "epoch": 2.9886302111532217,
      "grad_norm": 12.319601058959961,
      "learning_rate": 1.2076881429344883e-05,
      "loss": 0.4685,
      "step": 16560
    },
    {
      "epoch": 2.990434939541599,
      "grad_norm": 11.64255142211914,
      "learning_rate": 1.206605305901462e-05,
      "loss": 0.4634,
      "step": 16570
    },
    {
      "epoch": 2.9922396679299768,
      "grad_norm": 15.009732246398926,
      "learning_rate": 1.2055224688684353e-05,
      "loss": 0.4668,
      "step": 16580
    },
    {
      "epoch": 2.994044396318354,
      "grad_norm": 21.73628044128418,
      "learning_rate": 1.2044396318354088e-05,
      "loss": 0.4267,
      "step": 16590
    },
    {
      "epoch": 2.995849124706732,
      "grad_norm": 19.86180877685547,
      "learning_rate": 1.2033567948023822e-05,
      "loss": 0.2946,
      "step": 16600
    },
    {
      "epoch": 2.9976538530951093,
      "grad_norm": 15.813467979431152,
      "learning_rate": 1.2022739577693557e-05,
      "loss": 0.2996,
      "step": 16610
    },
    {
      "epoch": 2.999458581483487,
      "grad_norm": 4.553961277008057,
      "learning_rate": 1.2011911207363292e-05,
      "loss": 0.3209,
      "step": 16620
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8171276451743897,
      "eval_f1": 0.7536991658695217,
      "eval_loss": 0.6838319301605225,
      "eval_precision": 0.7465746513140893,
      "eval_recall": 0.7645891562493988,
      "eval_runtime": 160.3641,
      "eval_samples_per_second": 138.204,
      "eval_steps_per_second": 8.643,
      "step": 16623
    },
    {
      "epoch": 3.0012633098718644,
      "grad_norm": 4.686326026916504,
      "learning_rate": 1.2001082837033027e-05,
      "loss": 0.38,
      "step": 16630
    },
    {
      "epoch": 3.003068038260242,
      "grad_norm": 10.332103729248047,
      "learning_rate": 1.1990254466702762e-05,
      "loss": 0.3808,
      "step": 16640
    },
    {
      "epoch": 3.0048727666486195,
      "grad_norm": 25.62761688232422,
      "learning_rate": 1.1979426096372495e-05,
      "loss": 0.1817,
      "step": 16650
    },
    {
      "epoch": 3.006677495036997,
      "grad_norm": 27.03540802001953,
      "learning_rate": 1.1968597726042232e-05,
      "loss": 0.295,
      "step": 16660
    },
    {
      "epoch": 3.0084822234253745,
      "grad_norm": 9.862895011901855,
      "learning_rate": 1.1957769355711965e-05,
      "loss": 0.2745,
      "step": 16670
    },
    {
      "epoch": 3.010286951813752,
      "grad_norm": 0.7323330044746399,
      "learning_rate": 1.19469409853817e-05,
      "loss": 0.3452,
      "step": 16680
    },
    {
      "epoch": 3.0120916802021296,
      "grad_norm": 14.714471817016602,
      "learning_rate": 1.1936112615051435e-05,
      "loss": 0.1673,
      "step": 16690
    },
    {
      "epoch": 3.013896408590507,
      "grad_norm": 3.0123555660247803,
      "learning_rate": 1.192528424472117e-05,
      "loss": 0.1795,
      "step": 16700
    },
    {
      "epoch": 3.0157011369788846,
      "grad_norm": 34.73080825805664,
      "learning_rate": 1.1914455874390903e-05,
      "loss": 0.2998,
      "step": 16710
    },
    {
      "epoch": 3.017505865367262,
      "grad_norm": 32.0517578125,
      "learning_rate": 1.190362750406064e-05,
      "loss": 0.3093,
      "step": 16720
    },
    {
      "epoch": 3.0193105937556397,
      "grad_norm": 20.437294006347656,
      "learning_rate": 1.1892799133730374e-05,
      "loss": 0.2978,
      "step": 16730
    },
    {
      "epoch": 3.0211153221440172,
      "grad_norm": 35.9359016418457,
      "learning_rate": 1.1881970763400108e-05,
      "loss": 0.2535,
      "step": 16740
    },
    {
      "epoch": 3.0229200505323948,
      "grad_norm": 19.87786102294922,
      "learning_rate": 1.1871142393069844e-05,
      "loss": 0.5727,
      "step": 16750
    },
    {
      "epoch": 3.0247247789207723,
      "grad_norm": 22.228498458862305,
      "learning_rate": 1.1860314022739578e-05,
      "loss": 0.2779,
      "step": 16760
    },
    {
      "epoch": 3.02652950730915,
      "grad_norm": 8.368921279907227,
      "learning_rate": 1.1849485652409312e-05,
      "loss": 0.2198,
      "step": 16770
    },
    {
      "epoch": 3.0283342356975274,
      "grad_norm": 12.632722854614258,
      "learning_rate": 1.1838657282079047e-05,
      "loss": 0.2763,
      "step": 16780
    },
    {
      "epoch": 3.030138964085905,
      "grad_norm": 16.396881103515625,
      "learning_rate": 1.1827828911748782e-05,
      "loss": 0.2671,
      "step": 16790
    },
    {
      "epoch": 3.0319436924742824,
      "grad_norm": 14.941734313964844,
      "learning_rate": 1.1817000541418515e-05,
      "loss": 0.3212,
      "step": 16800
    },
    {
      "epoch": 3.0337484208626604,
      "grad_norm": 19.236282348632812,
      "learning_rate": 1.1806172171088252e-05,
      "loss": 0.3408,
      "step": 16810
    },
    {
      "epoch": 3.035553149251038,
      "grad_norm": 31.52625846862793,
      "learning_rate": 1.1795343800757987e-05,
      "loss": 0.2628,
      "step": 16820
    },
    {
      "epoch": 3.0373578776394154,
      "grad_norm": 19.191246032714844,
      "learning_rate": 1.178451543042772e-05,
      "loss": 0.2586,
      "step": 16830
    },
    {
      "epoch": 3.039162606027793,
      "grad_norm": 3.078315258026123,
      "learning_rate": 1.1773687060097457e-05,
      "loss": 0.4166,
      "step": 16840
    },
    {
      "epoch": 3.0409673344161705,
      "grad_norm": 25.219621658325195,
      "learning_rate": 1.176285868976719e-05,
      "loss": 0.2625,
      "step": 16850
    },
    {
      "epoch": 3.042772062804548,
      "grad_norm": 26.247337341308594,
      "learning_rate": 1.1752030319436925e-05,
      "loss": 0.3769,
      "step": 16860
    },
    {
      "epoch": 3.0445767911929256,
      "grad_norm": 13.841092109680176,
      "learning_rate": 1.174120194910666e-05,
      "loss": 0.32,
      "step": 16870
    },
    {
      "epoch": 3.046381519581303,
      "grad_norm": 23.601823806762695,
      "learning_rate": 1.1730373578776395e-05,
      "loss": 0.3424,
      "step": 16880
    },
    {
      "epoch": 3.0481862479696806,
      "grad_norm": 24.413074493408203,
      "learning_rate": 1.1719545208446128e-05,
      "loss": 0.5235,
      "step": 16890
    },
    {
      "epoch": 3.049990976358058,
      "grad_norm": 6.924914360046387,
      "learning_rate": 1.1708716838115864e-05,
      "loss": 0.2338,
      "step": 16900
    },
    {
      "epoch": 3.0517957047464357,
      "grad_norm": 2.5555171966552734,
      "learning_rate": 1.1697888467785598e-05,
      "loss": 0.1757,
      "step": 16910
    },
    {
      "epoch": 3.053600433134813,
      "grad_norm": 21.31715202331543,
      "learning_rate": 1.1687060097455333e-05,
      "loss": 0.3172,
      "step": 16920
    },
    {
      "epoch": 3.0554051615231907,
      "grad_norm": 7.567512035369873,
      "learning_rate": 1.167623172712507e-05,
      "loss": 0.2452,
      "step": 16930
    },
    {
      "epoch": 3.0572098899115683,
      "grad_norm": 2.3817906379699707,
      "learning_rate": 1.1665403356794802e-05,
      "loss": 0.2608,
      "step": 16940
    },
    {
      "epoch": 3.059014618299946,
      "grad_norm": 5.906301975250244,
      "learning_rate": 1.1654574986464537e-05,
      "loss": 0.2663,
      "step": 16950
    },
    {
      "epoch": 3.0608193466883233,
      "grad_norm": 32.97651290893555,
      "learning_rate": 1.1643746616134272e-05,
      "loss": 0.4032,
      "step": 16960
    },
    {
      "epoch": 3.062624075076701,
      "grad_norm": 8.328664779663086,
      "learning_rate": 1.1632918245804007e-05,
      "loss": 0.2234,
      "step": 16970
    },
    {
      "epoch": 3.0644288034650784,
      "grad_norm": 35.36061096191406,
      "learning_rate": 1.162208987547374e-05,
      "loss": 0.3118,
      "step": 16980
    },
    {
      "epoch": 3.066233531853456,
      "grad_norm": 32.677398681640625,
      "learning_rate": 1.1611261505143477e-05,
      "loss": 0.2806,
      "step": 16990
    },
    {
      "epoch": 3.0680382602418335,
      "grad_norm": 27.65266227722168,
      "learning_rate": 1.160043313481321e-05,
      "loss": 0.3536,
      "step": 17000
    },
    {
      "epoch": 3.069842988630211,
      "grad_norm": 13.459416389465332,
      "learning_rate": 1.1589604764482945e-05,
      "loss": 0.2466,
      "step": 17010
    },
    {
      "epoch": 3.0716477170185885,
      "grad_norm": 33.59754180908203,
      "learning_rate": 1.1578776394152682e-05,
      "loss": 0.2199,
      "step": 17020
    },
    {
      "epoch": 3.073452445406966,
      "grad_norm": 28.466581344604492,
      "learning_rate": 1.1567948023822415e-05,
      "loss": 0.2841,
      "step": 17030
    },
    {
      "epoch": 3.075257173795344,
      "grad_norm": 15.492993354797363,
      "learning_rate": 1.155711965349215e-05,
      "loss": 0.4295,
      "step": 17040
    },
    {
      "epoch": 3.0770619021837216,
      "grad_norm": 60.57014083862305,
      "learning_rate": 1.1546291283161885e-05,
      "loss": 0.3679,
      "step": 17050
    },
    {
      "epoch": 3.078866630572099,
      "grad_norm": 43.427734375,
      "learning_rate": 1.153546291283162e-05,
      "loss": 0.2902,
      "step": 17060
    },
    {
      "epoch": 3.0806713589604766,
      "grad_norm": 7.26952600479126,
      "learning_rate": 1.1524634542501353e-05,
      "loss": 0.359,
      "step": 17070
    },
    {
      "epoch": 3.082476087348854,
      "grad_norm": 31.311922073364258,
      "learning_rate": 1.151380617217109e-05,
      "loss": 0.2461,
      "step": 17080
    },
    {
      "epoch": 3.0842808157372317,
      "grad_norm": 9.951268196105957,
      "learning_rate": 1.1502977801840823e-05,
      "loss": 0.2145,
      "step": 17090
    },
    {
      "epoch": 3.086085544125609,
      "grad_norm": 9.121729850769043,
      "learning_rate": 1.1492149431510557e-05,
      "loss": 0.2663,
      "step": 17100
    },
    {
      "epoch": 3.0878902725139867,
      "grad_norm": 8.171472549438477,
      "learning_rate": 1.1481321061180294e-05,
      "loss": 0.1032,
      "step": 17110
    },
    {
      "epoch": 3.0896950009023643,
      "grad_norm": 63.31949996948242,
      "learning_rate": 1.1470492690850027e-05,
      "loss": 0.4114,
      "step": 17120
    },
    {
      "epoch": 3.091499729290742,
      "grad_norm": 15.34856128692627,
      "learning_rate": 1.1459664320519762e-05,
      "loss": 0.4579,
      "step": 17130
    },
    {
      "epoch": 3.0933044576791193,
      "grad_norm": 28.295146942138672,
      "learning_rate": 1.1448835950189497e-05,
      "loss": 0.2772,
      "step": 17140
    },
    {
      "epoch": 3.095109186067497,
      "grad_norm": 27.056852340698242,
      "learning_rate": 1.1438007579859232e-05,
      "loss": 0.2247,
      "step": 17150
    },
    {
      "epoch": 3.0969139144558744,
      "grad_norm": 13.704736709594727,
      "learning_rate": 1.1427179209528965e-05,
      "loss": 0.292,
      "step": 17160
    },
    {
      "epoch": 3.098718642844252,
      "grad_norm": 20.101364135742188,
      "learning_rate": 1.1416350839198702e-05,
      "loss": 0.3397,
      "step": 17170
    },
    {
      "epoch": 3.1005233712326294,
      "grad_norm": 16.255970001220703,
      "learning_rate": 1.1405522468868435e-05,
      "loss": 0.2112,
      "step": 17180
    },
    {
      "epoch": 3.102328099621007,
      "grad_norm": 16.500476837158203,
      "learning_rate": 1.139469409853817e-05,
      "loss": 0.219,
      "step": 17190
    },
    {
      "epoch": 3.1041328280093845,
      "grad_norm": 13.457676887512207,
      "learning_rate": 1.1383865728207905e-05,
      "loss": 0.1949,
      "step": 17200
    },
    {
      "epoch": 3.105937556397762,
      "grad_norm": 32.41647720336914,
      "learning_rate": 1.137303735787764e-05,
      "loss": 0.2482,
      "step": 17210
    },
    {
      "epoch": 3.1077422847861396,
      "grad_norm": 17.908538818359375,
      "learning_rate": 1.1362208987547375e-05,
      "loss": 0.188,
      "step": 17220
    },
    {
      "epoch": 3.109547013174517,
      "grad_norm": 11.273274421691895,
      "learning_rate": 1.135138061721711e-05,
      "loss": 0.2649,
      "step": 17230
    },
    {
      "epoch": 3.1113517415628946,
      "grad_norm": 19.254974365234375,
      "learning_rate": 1.1340552246886844e-05,
      "loss": 0.243,
      "step": 17240
    },
    {
      "epoch": 3.113156469951272,
      "grad_norm": 23.35660743713379,
      "learning_rate": 1.1329723876556578e-05,
      "loss": 0.2248,
      "step": 17250
    },
    {
      "epoch": 3.1149611983396497,
      "grad_norm": 25.41313934326172,
      "learning_rate": 1.1318895506226314e-05,
      "loss": 0.2474,
      "step": 17260
    },
    {
      "epoch": 3.1167659267280277,
      "grad_norm": 45.2186393737793,
      "learning_rate": 1.1308067135896047e-05,
      "loss": 0.3619,
      "step": 17270
    },
    {
      "epoch": 3.118570655116405,
      "grad_norm": 27.874025344848633,
      "learning_rate": 1.1297238765565782e-05,
      "loss": 0.2145,
      "step": 17280
    },
    {
      "epoch": 3.1203753835047827,
      "grad_norm": 10.82240104675293,
      "learning_rate": 1.1286410395235517e-05,
      "loss": 0.2619,
      "step": 17290
    },
    {
      "epoch": 3.1221801118931602,
      "grad_norm": 25.00333595275879,
      "learning_rate": 1.1275582024905252e-05,
      "loss": 0.1663,
      "step": 17300
    },
    {
      "epoch": 3.123984840281538,
      "grad_norm": 34.41892623901367,
      "learning_rate": 1.1264753654574987e-05,
      "loss": 0.3278,
      "step": 17310
    },
    {
      "epoch": 3.1257895686699153,
      "grad_norm": 3.286431312561035,
      "learning_rate": 1.1253925284244722e-05,
      "loss": 0.253,
      "step": 17320
    },
    {
      "epoch": 3.127594297058293,
      "grad_norm": 24.10135841369629,
      "learning_rate": 1.1243096913914457e-05,
      "loss": 0.3246,
      "step": 17330
    },
    {
      "epoch": 3.1293990254466704,
      "grad_norm": 13.313995361328125,
      "learning_rate": 1.123226854358419e-05,
      "loss": 0.3227,
      "step": 17340
    },
    {
      "epoch": 3.131203753835048,
      "grad_norm": 46.377132415771484,
      "learning_rate": 1.1221440173253927e-05,
      "loss": 0.2911,
      "step": 17350
    },
    {
      "epoch": 3.1330084822234254,
      "grad_norm": 5.272046089172363,
      "learning_rate": 1.121061180292366e-05,
      "loss": 0.285,
      "step": 17360
    },
    {
      "epoch": 3.134813210611803,
      "grad_norm": 11.422159194946289,
      "learning_rate": 1.1199783432593395e-05,
      "loss": 0.2174,
      "step": 17370
    },
    {
      "epoch": 3.1366179390001805,
      "grad_norm": 26.15447425842285,
      "learning_rate": 1.118895506226313e-05,
      "loss": 0.2519,
      "step": 17380
    },
    {
      "epoch": 3.138422667388558,
      "grad_norm": 1.978100061416626,
      "learning_rate": 1.1178126691932865e-05,
      "loss": 0.2337,
      "step": 17390
    },
    {
      "epoch": 3.1402273957769355,
      "grad_norm": 52.541202545166016,
      "learning_rate": 1.11672983216026e-05,
      "loss": 0.3648,
      "step": 17400
    },
    {
      "epoch": 3.142032124165313,
      "grad_norm": 32.91189956665039,
      "learning_rate": 1.1156469951272334e-05,
      "loss": 0.2859,
      "step": 17410
    },
    {
      "epoch": 3.1438368525536906,
      "grad_norm": 40.41348648071289,
      "learning_rate": 1.114564158094207e-05,
      "loss": 0.2027,
      "step": 17420
    },
    {
      "epoch": 3.145641580942068,
      "grad_norm": 11.476075172424316,
      "learning_rate": 1.1134813210611803e-05,
      "loss": 0.2844,
      "step": 17430
    },
    {
      "epoch": 3.1474463093304457,
      "grad_norm": 3.184875726699829,
      "learning_rate": 1.1123984840281539e-05,
      "loss": 0.1364,
      "step": 17440
    },
    {
      "epoch": 3.149251037718823,
      "grad_norm": 22.38290023803711,
      "learning_rate": 1.1113156469951272e-05,
      "loss": 0.2777,
      "step": 17450
    },
    {
      "epoch": 3.1510557661072007,
      "grad_norm": 19.669841766357422,
      "learning_rate": 1.1102328099621007e-05,
      "loss": 0.2808,
      "step": 17460
    },
    {
      "epoch": 3.1528604944955783,
      "grad_norm": 9.768157958984375,
      "learning_rate": 1.1091499729290742e-05,
      "loss": 0.2291,
      "step": 17470
    },
    {
      "epoch": 3.154665222883956,
      "grad_norm": 24.10875701904297,
      "learning_rate": 1.1080671358960477e-05,
      "loss": 0.5371,
      "step": 17480
    },
    {
      "epoch": 3.1564699512723333,
      "grad_norm": 44.87602996826172,
      "learning_rate": 1.106984298863021e-05,
      "loss": 0.2082,
      "step": 17490
    },
    {
      "epoch": 3.1582746796607113,
      "grad_norm": 22.015947341918945,
      "learning_rate": 1.1059014618299947e-05,
      "loss": 0.3138,
      "step": 17500
    },
    {
      "epoch": 3.1600794080490884,
      "grad_norm": 9.281291007995605,
      "learning_rate": 1.1048186247969682e-05,
      "loss": 0.282,
      "step": 17510
    },
    {
      "epoch": 3.1618841364374664,
      "grad_norm": 32.33757400512695,
      "learning_rate": 1.1037357877639415e-05,
      "loss": 0.2593,
      "step": 17520
    },
    {
      "epoch": 3.163688864825844,
      "grad_norm": 43.470054626464844,
      "learning_rate": 1.1026529507309152e-05,
      "loss": 0.651,
      "step": 17530
    },
    {
      "epoch": 3.1654935932142214,
      "grad_norm": 9.500412940979004,
      "learning_rate": 1.1015701136978885e-05,
      "loss": 0.3052,
      "step": 17540
    },
    {
      "epoch": 3.167298321602599,
      "grad_norm": 66.68048095703125,
      "learning_rate": 1.100487276664862e-05,
      "loss": 0.3066,
      "step": 17550
    },
    {
      "epoch": 3.1691030499909765,
      "grad_norm": 33.935386657714844,
      "learning_rate": 1.0994044396318355e-05,
      "loss": 0.1821,
      "step": 17560
    },
    {
      "epoch": 3.170907778379354,
      "grad_norm": 40.98355484008789,
      "learning_rate": 1.098321602598809e-05,
      "loss": 0.3039,
      "step": 17570
    },
    {
      "epoch": 3.1727125067677315,
      "grad_norm": 17.160400390625,
      "learning_rate": 1.0972387655657823e-05,
      "loss": 0.3584,
      "step": 17580
    },
    {
      "epoch": 3.174517235156109,
      "grad_norm": 32.22148513793945,
      "learning_rate": 1.096155928532756e-05,
      "loss": 0.2984,
      "step": 17590
    },
    {
      "epoch": 3.1763219635444866,
      "grad_norm": 20.199827194213867,
      "learning_rate": 1.0950730914997294e-05,
      "loss": 0.3379,
      "step": 17600
    },
    {
      "epoch": 3.178126691932864,
      "grad_norm": 22.955183029174805,
      "learning_rate": 1.0939902544667027e-05,
      "loss": 0.2523,
      "step": 17610
    },
    {
      "epoch": 3.1799314203212417,
      "grad_norm": 73.25780487060547,
      "learning_rate": 1.0929074174336764e-05,
      "loss": 0.1712,
      "step": 17620
    },
    {
      "epoch": 3.181736148709619,
      "grad_norm": 1.4550758600234985,
      "learning_rate": 1.0918245804006497e-05,
      "loss": 0.2954,
      "step": 17630
    },
    {
      "epoch": 3.1835408770979967,
      "grad_norm": 19.95570945739746,
      "learning_rate": 1.0907417433676232e-05,
      "loss": 0.2422,
      "step": 17640
    },
    {
      "epoch": 3.1853456054863742,
      "grad_norm": 14.822650909423828,
      "learning_rate": 1.0896589063345967e-05,
      "loss": 0.3185,
      "step": 17650
    },
    {
      "epoch": 3.1871503338747518,
      "grad_norm": 23.632238388061523,
      "learning_rate": 1.0885760693015702e-05,
      "loss": 0.2453,
      "step": 17660
    },
    {
      "epoch": 3.1889550622631293,
      "grad_norm": 1.4464761018753052,
      "learning_rate": 1.0874932322685435e-05,
      "loss": 0.1962,
      "step": 17670
    },
    {
      "epoch": 3.190759790651507,
      "grad_norm": 25.611360549926758,
      "learning_rate": 1.0864103952355172e-05,
      "loss": 0.2096,
      "step": 17680
    },
    {
      "epoch": 3.1925645190398844,
      "grad_norm": 46.47650146484375,
      "learning_rate": 1.0853275582024905e-05,
      "loss": 0.4267,
      "step": 17690
    },
    {
      "epoch": 3.194369247428262,
      "grad_norm": 60.86802291870117,
      "learning_rate": 1.084244721169464e-05,
      "loss": 0.1965,
      "step": 17700
    },
    {
      "epoch": 3.1961739758166394,
      "grad_norm": 85.6983413696289,
      "learning_rate": 1.0831618841364376e-05,
      "loss": 0.3376,
      "step": 17710
    },
    {
      "epoch": 3.197978704205017,
      "grad_norm": 9.96571159362793,
      "learning_rate": 1.082079047103411e-05,
      "loss": 0.3019,
      "step": 17720
    },
    {
      "epoch": 3.199783432593395,
      "grad_norm": 69.35523986816406,
      "learning_rate": 1.0809962100703845e-05,
      "loss": 0.2579,
      "step": 17730
    },
    {
      "epoch": 3.201588160981772,
      "grad_norm": 19.783674240112305,
      "learning_rate": 1.079913373037358e-05,
      "loss": 0.2769,
      "step": 17740
    },
    {
      "epoch": 3.20339288937015,
      "grad_norm": 6.121344089508057,
      "learning_rate": 1.0788305360043314e-05,
      "loss": 0.3009,
      "step": 17750
    },
    {
      "epoch": 3.2051976177585275,
      "grad_norm": 38.420257568359375,
      "learning_rate": 1.0777476989713048e-05,
      "loss": 0.3544,
      "step": 17760
    },
    {
      "epoch": 3.207002346146905,
      "grad_norm": 86.2308120727539,
      "learning_rate": 1.0766648619382782e-05,
      "loss": 0.2093,
      "step": 17770
    },
    {
      "epoch": 3.2088070745352826,
      "grad_norm": 16.231191635131836,
      "learning_rate": 1.0755820249052517e-05,
      "loss": 0.2258,
      "step": 17780
    },
    {
      "epoch": 3.21061180292366,
      "grad_norm": 17.746068954467773,
      "learning_rate": 1.0744991878722252e-05,
      "loss": 0.308,
      "step": 17790
    },
    {
      "epoch": 3.2124165313120376,
      "grad_norm": 15.590856552124023,
      "learning_rate": 1.0734163508391987e-05,
      "loss": 0.4225,
      "step": 17800
    },
    {
      "epoch": 3.214221259700415,
      "grad_norm": 15.47471809387207,
      "learning_rate": 1.0723335138061722e-05,
      "loss": 0.3561,
      "step": 17810
    },
    {
      "epoch": 3.2160259880887927,
      "grad_norm": 6.169948577880859,
      "learning_rate": 1.0712506767731457e-05,
      "loss": 0.3443,
      "step": 17820
    },
    {
      "epoch": 3.2178307164771702,
      "grad_norm": 11.228036880493164,
      "learning_rate": 1.070167839740119e-05,
      "loss": 0.3819,
      "step": 17830
    },
    {
      "epoch": 3.2196354448655478,
      "grad_norm": 14.459627151489258,
      "learning_rate": 1.0690850027070927e-05,
      "loss": 0.0886,
      "step": 17840
    },
    {
      "epoch": 3.2214401732539253,
      "grad_norm": 3.641806125640869,
      "learning_rate": 1.068002165674066e-05,
      "loss": 0.3401,
      "step": 17850
    },
    {
      "epoch": 3.223244901642303,
      "grad_norm": 2.0868723392486572,
      "learning_rate": 1.0669193286410395e-05,
      "loss": 0.3457,
      "step": 17860
    },
    {
      "epoch": 3.2250496300306803,
      "grad_norm": 42.1612434387207,
      "learning_rate": 1.065836491608013e-05,
      "loss": 0.3702,
      "step": 17870
    },
    {
      "epoch": 3.226854358419058,
      "grad_norm": 83.58853912353516,
      "learning_rate": 1.0647536545749865e-05,
      "loss": 0.2448,
      "step": 17880
    },
    {
      "epoch": 3.2286590868074354,
      "grad_norm": 44.30959701538086,
      "learning_rate": 1.06367081754196e-05,
      "loss": 0.2737,
      "step": 17890
    },
    {
      "epoch": 3.230463815195813,
      "grad_norm": 1.9331586360931396,
      "learning_rate": 1.0625879805089335e-05,
      "loss": 0.1758,
      "step": 17900
    },
    {
      "epoch": 3.2322685435841905,
      "grad_norm": 5.968591213226318,
      "learning_rate": 1.061505143475907e-05,
      "loss": 0.2617,
      "step": 17910
    },
    {
      "epoch": 3.234073271972568,
      "grad_norm": 6.618546962738037,
      "learning_rate": 1.0604223064428803e-05,
      "loss": 0.4441,
      "step": 17920
    },
    {
      "epoch": 3.2358780003609455,
      "grad_norm": 24.815401077270508,
      "learning_rate": 1.059339469409854e-05,
      "loss": 0.3085,
      "step": 17930
    },
    {
      "epoch": 3.237682728749323,
      "grad_norm": 54.3385124206543,
      "learning_rate": 1.0582566323768272e-05,
      "loss": 0.2936,
      "step": 17940
    },
    {
      "epoch": 3.2394874571377006,
      "grad_norm": 16.28166389465332,
      "learning_rate": 1.0571737953438007e-05,
      "loss": 0.2319,
      "step": 17950
    },
    {
      "epoch": 3.241292185526078,
      "grad_norm": 7.340639591217041,
      "learning_rate": 1.0560909583107742e-05,
      "loss": 0.1805,
      "step": 17960
    },
    {
      "epoch": 3.2430969139144556,
      "grad_norm": 31.14578628540039,
      "learning_rate": 1.0550081212777477e-05,
      "loss": 0.323,
      "step": 17970
    },
    {
      "epoch": 3.2449016423028336,
      "grad_norm": 1.2032490968704224,
      "learning_rate": 1.053925284244721e-05,
      "loss": 0.3208,
      "step": 17980
    },
    {
      "epoch": 3.246706370691211,
      "grad_norm": 53.11332321166992,
      "learning_rate": 1.0528424472116947e-05,
      "loss": 0.2439,
      "step": 17990
    },
    {
      "epoch": 3.2485110990795887,
      "grad_norm": 17.17512321472168,
      "learning_rate": 1.0517596101786682e-05,
      "loss": 0.2708,
      "step": 18000
    },
    {
      "epoch": 3.250315827467966,
      "grad_norm": 3.126328706741333,
      "learning_rate": 1.0506767731456415e-05,
      "loss": 0.4476,
      "step": 18010
    },
    {
      "epoch": 3.2521205558563437,
      "grad_norm": 5.338082790374756,
      "learning_rate": 1.0495939361126152e-05,
      "loss": 0.2837,
      "step": 18020
    },
    {
      "epoch": 3.2539252842447213,
      "grad_norm": 20.83074951171875,
      "learning_rate": 1.0485110990795885e-05,
      "loss": 0.3926,
      "step": 18030
    },
    {
      "epoch": 3.255730012633099,
      "grad_norm": 4.668416976928711,
      "learning_rate": 1.047428262046562e-05,
      "loss": 0.4022,
      "step": 18040
    },
    {
      "epoch": 3.2575347410214763,
      "grad_norm": 42.465938568115234,
      "learning_rate": 1.0463454250135355e-05,
      "loss": 0.4526,
      "step": 18050
    },
    {
      "epoch": 3.259339469409854,
      "grad_norm": 4.079095363616943,
      "learning_rate": 1.045262587980509e-05,
      "loss": 0.379,
      "step": 18060
    },
    {
      "epoch": 3.2611441977982314,
      "grad_norm": 2.7919671535491943,
      "learning_rate": 1.0441797509474823e-05,
      "loss": 0.2077,
      "step": 18070
    },
    {
      "epoch": 3.262948926186609,
      "grad_norm": 25.191049575805664,
      "learning_rate": 1.043096913914456e-05,
      "loss": 0.4374,
      "step": 18080
    },
    {
      "epoch": 3.2647536545749865,
      "grad_norm": 52.981895446777344,
      "learning_rate": 1.0420140768814294e-05,
      "loss": 0.2317,
      "step": 18090
    },
    {
      "epoch": 3.266558382963364,
      "grad_norm": 9.171489715576172,
      "learning_rate": 1.0409312398484028e-05,
      "loss": 0.3943,
      "step": 18100
    },
    {
      "epoch": 3.2683631113517415,
      "grad_norm": 17.703506469726562,
      "learning_rate": 1.0398484028153764e-05,
      "loss": 0.3594,
      "step": 18110
    },
    {
      "epoch": 3.270167839740119,
      "grad_norm": 70.18862915039062,
      "learning_rate": 1.0387655657823497e-05,
      "loss": 0.3375,
      "step": 18120
    },
    {
      "epoch": 3.2719725681284966,
      "grad_norm": 56.836669921875,
      "learning_rate": 1.0376827287493232e-05,
      "loss": 0.2178,
      "step": 18130
    },
    {
      "epoch": 3.273777296516874,
      "grad_norm": 9.68991470336914,
      "learning_rate": 1.0365998917162967e-05,
      "loss": 0.3105,
      "step": 18140
    },
    {
      "epoch": 3.2755820249052516,
      "grad_norm": 41.256080627441406,
      "learning_rate": 1.0355170546832702e-05,
      "loss": 0.4102,
      "step": 18150
    },
    {
      "epoch": 3.277386753293629,
      "grad_norm": 26.35664939880371,
      "learning_rate": 1.0344342176502435e-05,
      "loss": 0.307,
      "step": 18160
    },
    {
      "epoch": 3.2791914816820067,
      "grad_norm": 52.53809356689453,
      "learning_rate": 1.0333513806172172e-05,
      "loss": 0.2248,
      "step": 18170
    },
    {
      "epoch": 3.2809962100703842,
      "grad_norm": 25.194168090820312,
      "learning_rate": 1.0322685435841907e-05,
      "loss": 0.3049,
      "step": 18180
    },
    {
      "epoch": 3.282800938458762,
      "grad_norm": 39.374916076660156,
      "learning_rate": 1.031185706551164e-05,
      "loss": 0.2681,
      "step": 18190
    },
    {
      "epoch": 3.2846056668471393,
      "grad_norm": 10.218722343444824,
      "learning_rate": 1.0301028695181377e-05,
      "loss": 0.3033,
      "step": 18200
    },
    {
      "epoch": 3.2864103952355173,
      "grad_norm": 11.342252731323242,
      "learning_rate": 1.029020032485111e-05,
      "loss": 0.278,
      "step": 18210
    },
    {
      "epoch": 3.288215123623895,
      "grad_norm": 10.37932300567627,
      "learning_rate": 1.0279371954520845e-05,
      "loss": 0.3498,
      "step": 18220
    },
    {
      "epoch": 3.2900198520122723,
      "grad_norm": 1.975196361541748,
      "learning_rate": 1.026854358419058e-05,
      "loss": 0.2898,
      "step": 18230
    },
    {
      "epoch": 3.29182458040065,
      "grad_norm": 10.419394493103027,
      "learning_rate": 1.0257715213860315e-05,
      "loss": 0.3646,
      "step": 18240
    },
    {
      "epoch": 3.2936293087890274,
      "grad_norm": 43.46549606323242,
      "learning_rate": 1.0246886843530048e-05,
      "loss": 0.2527,
      "step": 18250
    },
    {
      "epoch": 3.295434037177405,
      "grad_norm": 30.416040420532227,
      "learning_rate": 1.0236058473199784e-05,
      "loss": 0.3016,
      "step": 18260
    },
    {
      "epoch": 3.2972387655657824,
      "grad_norm": 29.191877365112305,
      "learning_rate": 1.0225230102869518e-05,
      "loss": 0.2714,
      "step": 18270
    },
    {
      "epoch": 3.29904349395416,
      "grad_norm": 3.0742344856262207,
      "learning_rate": 1.0214401732539252e-05,
      "loss": 0.2816,
      "step": 18280
    },
    {
      "epoch": 3.3008482223425375,
      "grad_norm": 28.604398727416992,
      "learning_rate": 1.0203573362208989e-05,
      "loss": 0.1867,
      "step": 18290
    },
    {
      "epoch": 3.302652950730915,
      "grad_norm": 31.487455368041992,
      "learning_rate": 1.0192744991878722e-05,
      "loss": 0.2713,
      "step": 18300
    },
    {
      "epoch": 3.3044576791192926,
      "grad_norm": 71.341796875,
      "learning_rate": 1.0181916621548457e-05,
      "loss": 0.372,
      "step": 18310
    },
    {
      "epoch": 3.30626240750767,
      "grad_norm": 52.0954475402832,
      "learning_rate": 1.0171088251218192e-05,
      "loss": 0.2064,
      "step": 18320
    },
    {
      "epoch": 3.3080671358960476,
      "grad_norm": 24.9444580078125,
      "learning_rate": 1.0160259880887927e-05,
      "loss": 0.452,
      "step": 18330
    },
    {
      "epoch": 3.309871864284425,
      "grad_norm": 49.82071304321289,
      "learning_rate": 1.014943151055766e-05,
      "loss": 0.2262,
      "step": 18340
    },
    {
      "epoch": 3.3116765926728027,
      "grad_norm": 0.5258662700653076,
      "learning_rate": 1.0138603140227397e-05,
      "loss": 0.3725,
      "step": 18350
    },
    {
      "epoch": 3.31348132106118,
      "grad_norm": 13.512232780456543,
      "learning_rate": 1.012777476989713e-05,
      "loss": 0.5159,
      "step": 18360
    },
    {
      "epoch": 3.3152860494495577,
      "grad_norm": 24.954158782958984,
      "learning_rate": 1.0116946399566865e-05,
      "loss": 0.2707,
      "step": 18370
    },
    {
      "epoch": 3.3170907778379353,
      "grad_norm": 4.238191604614258,
      "learning_rate": 1.0106118029236601e-05,
      "loss": 0.116,
      "step": 18380
    },
    {
      "epoch": 3.318895506226313,
      "grad_norm": 44.62630081176758,
      "learning_rate": 1.0095289658906335e-05,
      "loss": 0.2861,
      "step": 18390
    },
    {
      "epoch": 3.3207002346146903,
      "grad_norm": 53.54899597167969,
      "learning_rate": 1.008446128857607e-05,
      "loss": 0.2756,
      "step": 18400
    },
    {
      "epoch": 3.322504963003068,
      "grad_norm": 8.742081642150879,
      "learning_rate": 1.0073632918245804e-05,
      "loss": 0.2287,
      "step": 18410
    },
    {
      "epoch": 3.324309691391446,
      "grad_norm": 40.12411880493164,
      "learning_rate": 1.006280454791554e-05,
      "loss": 0.3765,
      "step": 18420
    },
    {
      "epoch": 3.326114419779823,
      "grad_norm": 26.192296981811523,
      "learning_rate": 1.0051976177585273e-05,
      "loss": 0.3491,
      "step": 18430
    },
    {
      "epoch": 3.327919148168201,
      "grad_norm": 29.999984741210938,
      "learning_rate": 1.004114780725501e-05,
      "loss": 0.3543,
      "step": 18440
    },
    {
      "epoch": 3.3297238765565784,
      "grad_norm": 21.333486557006836,
      "learning_rate": 1.0030319436924742e-05,
      "loss": 0.2436,
      "step": 18450
    },
    {
      "epoch": 3.331528604944956,
      "grad_norm": 11.907448768615723,
      "learning_rate": 1.0019491066594477e-05,
      "loss": 0.1932,
      "step": 18460
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 18.230924606323242,
      "learning_rate": 1.0008662696264212e-05,
      "loss": 0.3612,
      "step": 18470
    },
    {
      "epoch": 3.335138061721711,
      "grad_norm": 7.408628463745117,
      "learning_rate": 9.997834325933947e-06,
      "loss": 0.3949,
      "step": 18480
    },
    {
      "epoch": 3.3369427901100885,
      "grad_norm": 56.68439483642578,
      "learning_rate": 9.987005955603682e-06,
      "loss": 0.3436,
      "step": 18490
    },
    {
      "epoch": 3.338747518498466,
      "grad_norm": 51.61479187011719,
      "learning_rate": 9.976177585273417e-06,
      "loss": 0.3059,
      "step": 18500
    },
    {
      "epoch": 3.3405522468868436,
      "grad_norm": 38.94914245605469,
      "learning_rate": 9.965349214943152e-06,
      "loss": 0.1959,
      "step": 18510
    },
    {
      "epoch": 3.342356975275221,
      "grad_norm": 6.211492538452148,
      "learning_rate": 9.954520844612885e-06,
      "loss": 0.4651,
      "step": 18520
    },
    {
      "epoch": 3.3441617036635987,
      "grad_norm": 7.988531589508057,
      "learning_rate": 9.943692474282622e-06,
      "loss": 0.4293,
      "step": 18530
    },
    {
      "epoch": 3.345966432051976,
      "grad_norm": 48.75084686279297,
      "learning_rate": 9.932864103952355e-06,
      "loss": 0.2949,
      "step": 18540
    },
    {
      "epoch": 3.3477711604403537,
      "grad_norm": 0.7093913555145264,
      "learning_rate": 9.92203573362209e-06,
      "loss": 0.2031,
      "step": 18550
    },
    {
      "epoch": 3.3495758888287313,
      "grad_norm": 10.842201232910156,
      "learning_rate": 9.911207363291825e-06,
      "loss": 0.3558,
      "step": 18560
    },
    {
      "epoch": 3.351380617217109,
      "grad_norm": 31.19820213317871,
      "learning_rate": 9.90037899296156e-06,
      "loss": 0.3468,
      "step": 18570
    },
    {
      "epoch": 3.3531853456054863,
      "grad_norm": 12.973312377929688,
      "learning_rate": 9.889550622631294e-06,
      "loss": 0.3039,
      "step": 18580
    },
    {
      "epoch": 3.354990073993864,
      "grad_norm": 14.31255054473877,
      "learning_rate": 9.87872225230103e-06,
      "loss": 0.395,
      "step": 18590
    },
    {
      "epoch": 3.3567948023822414,
      "grad_norm": 10.84827709197998,
      "learning_rate": 9.867893881970764e-06,
      "loss": 0.31,
      "step": 18600
    },
    {
      "epoch": 3.358599530770619,
      "grad_norm": 16.367063522338867,
      "learning_rate": 9.857065511640497e-06,
      "loss": 0.3559,
      "step": 18610
    },
    {
      "epoch": 3.3604042591589964,
      "grad_norm": 40.1414794921875,
      "learning_rate": 9.846237141310234e-06,
      "loss": 0.3536,
      "step": 18620
    },
    {
      "epoch": 3.362208987547374,
      "grad_norm": 2.959808111190796,
      "learning_rate": 9.835408770979967e-06,
      "loss": 0.2534,
      "step": 18630
    },
    {
      "epoch": 3.3640137159357515,
      "grad_norm": 4.86169958114624,
      "learning_rate": 9.824580400649702e-06,
      "loss": 0.2111,
      "step": 18640
    },
    {
      "epoch": 3.3658184443241295,
      "grad_norm": 17.400075912475586,
      "learning_rate": 9.813752030319437e-06,
      "loss": 0.2004,
      "step": 18650
    },
    {
      "epoch": 3.3676231727125066,
      "grad_norm": 6.805636882781982,
      "learning_rate": 9.802923659989172e-06,
      "loss": 0.2982,
      "step": 18660
    },
    {
      "epoch": 3.3694279011008845,
      "grad_norm": 7.572901248931885,
      "learning_rate": 9.792095289658907e-06,
      "loss": 0.3654,
      "step": 18670
    },
    {
      "epoch": 3.371232629489262,
      "grad_norm": 11.647687911987305,
      "learning_rate": 9.781266919328642e-06,
      "loss": 0.5454,
      "step": 18680
    },
    {
      "epoch": 3.3730373578776396,
      "grad_norm": 75.8009033203125,
      "learning_rate": 9.770438548998377e-06,
      "loss": 0.3051,
      "step": 18690
    },
    {
      "epoch": 3.374842086266017,
      "grad_norm": 64.45244598388672,
      "learning_rate": 9.75961017866811e-06,
      "loss": 0.3243,
      "step": 18700
    },
    {
      "epoch": 3.3766468146543946,
      "grad_norm": 75.38687133789062,
      "learning_rate": 9.748781808337847e-06,
      "loss": 0.1873,
      "step": 18710
    },
    {
      "epoch": 3.378451543042772,
      "grad_norm": 15.683817863464355,
      "learning_rate": 9.73795343800758e-06,
      "loss": 0.1962,
      "step": 18720
    },
    {
      "epoch": 3.3802562714311497,
      "grad_norm": 44.92477035522461,
      "learning_rate": 9.727125067677315e-06,
      "loss": 0.3047,
      "step": 18730
    },
    {
      "epoch": 3.3820609998195272,
      "grad_norm": 61.50442886352539,
      "learning_rate": 9.71629669734705e-06,
      "loss": 0.3096,
      "step": 18740
    },
    {
      "epoch": 3.3838657282079048,
      "grad_norm": 4.439727306365967,
      "learning_rate": 9.705468327016784e-06,
      "loss": 0.2844,
      "step": 18750
    },
    {
      "epoch": 3.3856704565962823,
      "grad_norm": 15.14769172668457,
      "learning_rate": 9.694639956686518e-06,
      "loss": 0.2405,
      "step": 18760
    },
    {
      "epoch": 3.38747518498466,
      "grad_norm": 22.428241729736328,
      "learning_rate": 9.683811586356254e-06,
      "loss": 0.2513,
      "step": 18770
    },
    {
      "epoch": 3.3892799133730374,
      "grad_norm": 34.44944381713867,
      "learning_rate": 9.67298321602599e-06,
      "loss": 0.4117,
      "step": 18780
    },
    {
      "epoch": 3.391084641761415,
      "grad_norm": 2.483886957168579,
      "learning_rate": 9.662154845695722e-06,
      "loss": 0.3823,
      "step": 18790
    },
    {
      "epoch": 3.3928893701497924,
      "grad_norm": 32.7730827331543,
      "learning_rate": 9.651326475365459e-06,
      "loss": 0.2415,
      "step": 18800
    },
    {
      "epoch": 3.39469409853817,
      "grad_norm": 21.87212562561035,
      "learning_rate": 9.640498105035192e-06,
      "loss": 0.4204,
      "step": 18810
    },
    {
      "epoch": 3.3964988269265475,
      "grad_norm": 28.309846878051758,
      "learning_rate": 9.629669734704927e-06,
      "loss": 0.2421,
      "step": 18820
    },
    {
      "epoch": 3.398303555314925,
      "grad_norm": 19.294740676879883,
      "learning_rate": 9.618841364374662e-06,
      "loss": 0.2447,
      "step": 18830
    },
    {
      "epoch": 3.4001082837033025,
      "grad_norm": 17.328277587890625,
      "learning_rate": 9.608012994044397e-06,
      "loss": 0.4528,
      "step": 18840
    },
    {
      "epoch": 3.40191301209168,
      "grad_norm": 27.845874786376953,
      "learning_rate": 9.59718462371413e-06,
      "loss": 0.3176,
      "step": 18850
    },
    {
      "epoch": 3.4037177404800576,
      "grad_norm": 9.063881874084473,
      "learning_rate": 9.586356253383867e-06,
      "loss": 0.3124,
      "step": 18860
    },
    {
      "epoch": 3.405522468868435,
      "grad_norm": 23.158119201660156,
      "learning_rate": 9.575527883053602e-06,
      "loss": 0.4291,
      "step": 18870
    },
    {
      "epoch": 3.4073271972568127,
      "grad_norm": 15.572949409484863,
      "learning_rate": 9.564699512723335e-06,
      "loss": 0.1902,
      "step": 18880
    },
    {
      "epoch": 3.40913192564519,
      "grad_norm": 49.6837158203125,
      "learning_rate": 9.553871142393071e-06,
      "loss": 0.1518,
      "step": 18890
    },
    {
      "epoch": 3.410936654033568,
      "grad_norm": 4.448667526245117,
      "learning_rate": 9.543042772062805e-06,
      "loss": 0.3154,
      "step": 18900
    },
    {
      "epoch": 3.4127413824219457,
      "grad_norm": 13.04007625579834,
      "learning_rate": 9.533297238765565e-06,
      "loss": 0.2511,
      "step": 18910
    },
    {
      "epoch": 3.414546110810323,
      "grad_norm": 32.42179870605469,
      "learning_rate": 9.522468868435301e-06,
      "loss": 0.3515,
      "step": 18920
    },
    {
      "epoch": 3.4163508391987008,
      "grad_norm": 13.52257251739502,
      "learning_rate": 9.511640498105036e-06,
      "loss": 0.3442,
      "step": 18930
    },
    {
      "epoch": 3.4181555675870783,
      "grad_norm": 26.78583335876465,
      "learning_rate": 9.50081212777477e-06,
      "loss": 0.1561,
      "step": 18940
    },
    {
      "epoch": 3.419960295975456,
      "grad_norm": 12.14860725402832,
      "learning_rate": 9.489983757444506e-06,
      "loss": 0.2619,
      "step": 18950
    },
    {
      "epoch": 3.4217650243638333,
      "grad_norm": 22.10515785217285,
      "learning_rate": 9.47915538711424e-06,
      "loss": 0.2618,
      "step": 18960
    },
    {
      "epoch": 3.423569752752211,
      "grad_norm": 77.57978820800781,
      "learning_rate": 9.468327016783974e-06,
      "loss": 0.3626,
      "step": 18970
    },
    {
      "epoch": 3.4253744811405884,
      "grad_norm": 35.88684844970703,
      "learning_rate": 9.457498646453709e-06,
      "loss": 0.3721,
      "step": 18980
    },
    {
      "epoch": 3.427179209528966,
      "grad_norm": 71.70176696777344,
      "learning_rate": 9.446670276123444e-06,
      "loss": 0.4759,
      "step": 18990
    },
    {
      "epoch": 3.4289839379173435,
      "grad_norm": 2.6180596351623535,
      "learning_rate": 9.435841905793177e-06,
      "loss": 0.3612,
      "step": 19000
    },
    {
      "epoch": 3.430788666305721,
      "grad_norm": 60.887664794921875,
      "learning_rate": 9.425013535462914e-06,
      "loss": 0.2265,
      "step": 19010
    },
    {
      "epoch": 3.4325933946940985,
      "grad_norm": 7.602043151855469,
      "learning_rate": 9.414185165132647e-06,
      "loss": 0.3548,
      "step": 19020
    },
    {
      "epoch": 3.434398123082476,
      "grad_norm": 19.910865783691406,
      "learning_rate": 9.403356794802382e-06,
      "loss": 0.184,
      "step": 19030
    },
    {
      "epoch": 3.4362028514708536,
      "grad_norm": 20.652711868286133,
      "learning_rate": 9.392528424472118e-06,
      "loss": 0.2595,
      "step": 19040
    },
    {
      "epoch": 3.438007579859231,
      "grad_norm": 4.411715984344482,
      "learning_rate": 9.381700054141852e-06,
      "loss": 0.2746,
      "step": 19050
    },
    {
      "epoch": 3.4398123082476086,
      "grad_norm": 13.552937507629395,
      "learning_rate": 9.370871683811587e-06,
      "loss": 0.3091,
      "step": 19060
    },
    {
      "epoch": 3.441617036635986,
      "grad_norm": 41.05535125732422,
      "learning_rate": 9.360043313481321e-06,
      "loss": 0.3975,
      "step": 19070
    },
    {
      "epoch": 3.4434217650243637,
      "grad_norm": 26.870010375976562,
      "learning_rate": 9.349214943151056e-06,
      "loss": 0.307,
      "step": 19080
    },
    {
      "epoch": 3.4452264934127412,
      "grad_norm": 4.418450355529785,
      "learning_rate": 9.33838657282079e-06,
      "loss": 0.163,
      "step": 19090
    },
    {
      "epoch": 3.4470312218011188,
      "grad_norm": 22.18150520324707,
      "learning_rate": 9.327558202490526e-06,
      "loss": 0.281,
      "step": 19100
    },
    {
      "epoch": 3.4488359501894963,
      "grad_norm": 24.385595321655273,
      "learning_rate": 9.31672983216026e-06,
      "loss": 0.164,
      "step": 19110
    },
    {
      "epoch": 3.450640678577874,
      "grad_norm": 11.920063972473145,
      "learning_rate": 9.305901461829994e-06,
      "loss": 0.277,
      "step": 19120
    },
    {
      "epoch": 3.452445406966252,
      "grad_norm": 19.510738372802734,
      "learning_rate": 9.295073091499731e-06,
      "loss": 0.3315,
      "step": 19130
    },
    {
      "epoch": 3.454250135354629,
      "grad_norm": 1.2524662017822266,
      "learning_rate": 9.284244721169464e-06,
      "loss": 0.2901,
      "step": 19140
    },
    {
      "epoch": 3.456054863743007,
      "grad_norm": 5.838420867919922,
      "learning_rate": 9.273416350839199e-06,
      "loss": 0.2824,
      "step": 19150
    },
    {
      "epoch": 3.4578595921313844,
      "grad_norm": 54.21100616455078,
      "learning_rate": 9.262587980508934e-06,
      "loss": 0.4101,
      "step": 19160
    },
    {
      "epoch": 3.459664320519762,
      "grad_norm": 59.51984405517578,
      "learning_rate": 9.251759610178669e-06,
      "loss": 0.331,
      "step": 19170
    },
    {
      "epoch": 3.4614690489081394,
      "grad_norm": 64.31460571289062,
      "learning_rate": 9.240931239848402e-06,
      "loss": 0.1652,
      "step": 19180
    },
    {
      "epoch": 3.463273777296517,
      "grad_norm": 18.92101287841797,
      "learning_rate": 9.230102869518139e-06,
      "loss": 0.2215,
      "step": 19190
    },
    {
      "epoch": 3.4650785056848945,
      "grad_norm": 50.391082763671875,
      "learning_rate": 9.219274499187872e-06,
      "loss": 0.2318,
      "step": 19200
    },
    {
      "epoch": 3.466883234073272,
      "grad_norm": 2.927807569503784,
      "learning_rate": 9.208446128857607e-06,
      "loss": 0.3019,
      "step": 19210
    },
    {
      "epoch": 3.4686879624616496,
      "grad_norm": 9.124032974243164,
      "learning_rate": 9.197617758527342e-06,
      "loss": 0.2582,
      "step": 19220
    },
    {
      "epoch": 3.470492690850027,
      "grad_norm": 2.0047733783721924,
      "learning_rate": 9.186789388197077e-06,
      "loss": 0.3124,
      "step": 19230
    },
    {
      "epoch": 3.4722974192384046,
      "grad_norm": 91.72483825683594,
      "learning_rate": 9.175961017866811e-06,
      "loss": 0.2936,
      "step": 19240
    },
    {
      "epoch": 3.474102147626782,
      "grad_norm": 77.04988098144531,
      "learning_rate": 9.165132647536546e-06,
      "loss": 0.3715,
      "step": 19250
    },
    {
      "epoch": 3.4759068760151597,
      "grad_norm": 32.646446228027344,
      "learning_rate": 9.154304277206281e-06,
      "loss": 0.2442,
      "step": 19260
    },
    {
      "epoch": 3.477711604403537,
      "grad_norm": 40.67108917236328,
      "learning_rate": 9.143475906876014e-06,
      "loss": 0.3003,
      "step": 19270
    },
    {
      "epoch": 3.4795163327919147,
      "grad_norm": 17.581798553466797,
      "learning_rate": 9.132647536545751e-06,
      "loss": 0.478,
      "step": 19280
    },
    {
      "epoch": 3.4813210611802923,
      "grad_norm": 12.349029541015625,
      "learning_rate": 9.121819166215484e-06,
      "loss": 0.1209,
      "step": 19290
    },
    {
      "epoch": 3.48312578956867,
      "grad_norm": 31.913169860839844,
      "learning_rate": 9.11099079588522e-06,
      "loss": 0.3002,
      "step": 19300
    },
    {
      "epoch": 3.4849305179570473,
      "grad_norm": 9.945823669433594,
      "learning_rate": 9.100162425554954e-06,
      "loss": 0.3584,
      "step": 19310
    },
    {
      "epoch": 3.486735246345425,
      "grad_norm": 20.02021598815918,
      "learning_rate": 9.089334055224689e-06,
      "loss": 0.3084,
      "step": 19320
    },
    {
      "epoch": 3.4885399747338024,
      "grad_norm": 123.77645874023438,
      "learning_rate": 9.078505684894424e-06,
      "loss": 0.3597,
      "step": 19330
    },
    {
      "epoch": 3.49034470312218,
      "grad_norm": 50.29381561279297,
      "learning_rate": 9.067677314564159e-06,
      "loss": 0.2244,
      "step": 19340
    },
    {
      "epoch": 3.4921494315105575,
      "grad_norm": 3.1282341480255127,
      "learning_rate": 9.056848944233894e-06,
      "loss": 0.4005,
      "step": 19350
    },
    {
      "epoch": 3.4939541598989354,
      "grad_norm": 20.58060646057129,
      "learning_rate": 9.046020573903627e-06,
      "loss": 0.2389,
      "step": 19360
    },
    {
      "epoch": 3.4957588882873125,
      "grad_norm": 48.0073356628418,
      "learning_rate": 9.035192203573363e-06,
      "loss": 0.2453,
      "step": 19370
    },
    {
      "epoch": 3.4975636166756905,
      "grad_norm": 8.409774780273438,
      "learning_rate": 9.024363833243097e-06,
      "loss": 0.4449,
      "step": 19380
    },
    {
      "epoch": 3.499368345064068,
      "grad_norm": 12.082606315612793,
      "learning_rate": 9.013535462912832e-06,
      "loss": 0.2504,
      "step": 19390
    },
    {
      "epoch": 3.5011730734524455,
      "grad_norm": 12.622364044189453,
      "learning_rate": 9.002707092582567e-06,
      "loss": 0.3174,
      "step": 19400
    },
    {
      "epoch": 3.502977801840823,
      "grad_norm": 3.5117316246032715,
      "learning_rate": 8.991878722252301e-06,
      "loss": 0.3872,
      "step": 19410
    },
    {
      "epoch": 3.5047825302292006,
      "grad_norm": 27.96063995361328,
      "learning_rate": 8.981050351922036e-06,
      "loss": 0.232,
      "step": 19420
    },
    {
      "epoch": 3.506587258617578,
      "grad_norm": 0.5693327784538269,
      "learning_rate": 8.970221981591771e-06,
      "loss": 0.2072,
      "step": 19430
    },
    {
      "epoch": 3.5083919870059557,
      "grad_norm": 8.722960472106934,
      "learning_rate": 8.959393611261506e-06,
      "loss": 0.1654,
      "step": 19440
    },
    {
      "epoch": 3.510196715394333,
      "grad_norm": 11.824747085571289,
      "learning_rate": 8.94856524093124e-06,
      "loss": 0.2459,
      "step": 19450
    },
    {
      "epoch": 3.5120014437827107,
      "grad_norm": 37.77851486206055,
      "learning_rate": 8.937736870600976e-06,
      "loss": 0.2396,
      "step": 19460
    },
    {
      "epoch": 3.5138061721710883,
      "grad_norm": 111.15522003173828,
      "learning_rate": 8.926908500270709e-06,
      "loss": 0.3646,
      "step": 19470
    },
    {
      "epoch": 3.515610900559466,
      "grad_norm": 12.587224960327148,
      "learning_rate": 8.916080129940444e-06,
      "loss": 0.3194,
      "step": 19480
    },
    {
      "epoch": 3.5174156289478433,
      "grad_norm": 63.3674430847168,
      "learning_rate": 8.905251759610179e-06,
      "loss": 0.2294,
      "step": 19490
    },
    {
      "epoch": 3.519220357336221,
      "grad_norm": 16.360889434814453,
      "learning_rate": 8.894423389279914e-06,
      "loss": 0.2703,
      "step": 19500
    },
    {
      "epoch": 3.5210250857245984,
      "grad_norm": 28.062742233276367,
      "learning_rate": 8.883595018949647e-06,
      "loss": 0.2221,
      "step": 19510
    },
    {
      "epoch": 3.522829814112976,
      "grad_norm": 5.160360813140869,
      "learning_rate": 8.872766648619384e-06,
      "loss": 0.3378,
      "step": 19520
    },
    {
      "epoch": 3.5246345425013534,
      "grad_norm": 33.59894561767578,
      "learning_rate": 8.861938278289119e-06,
      "loss": 0.5023,
      "step": 19530
    },
    {
      "epoch": 3.526439270889731,
      "grad_norm": 3.246046543121338,
      "learning_rate": 8.851109907958852e-06,
      "loss": 0.3302,
      "step": 19540
    },
    {
      "epoch": 3.5282439992781085,
      "grad_norm": 23.875703811645508,
      "learning_rate": 8.840281537628588e-06,
      "loss": 0.2272,
      "step": 19550
    },
    {
      "epoch": 3.530048727666486,
      "grad_norm": 46.0523567199707,
      "learning_rate": 8.829453167298322e-06,
      "loss": 0.4141,
      "step": 19560
    },
    {
      "epoch": 3.531853456054864,
      "grad_norm": 20.764097213745117,
      "learning_rate": 8.818624796968056e-06,
      "loss": 0.3813,
      "step": 19570
    },
    {
      "epoch": 3.533658184443241,
      "grad_norm": 2.8313841819763184,
      "learning_rate": 8.807796426637791e-06,
      "loss": 0.2029,
      "step": 19580
    },
    {
      "epoch": 3.535462912831619,
      "grad_norm": 36.07177734375,
      "learning_rate": 8.798050893340553e-06,
      "loss": 0.4026,
      "step": 19590
    },
    {
      "epoch": 3.537267641219996,
      "grad_norm": 6.977352142333984,
      "learning_rate": 8.787222523010286e-06,
      "loss": 0.3365,
      "step": 19600
    },
    {
      "epoch": 3.539072369608374,
      "grad_norm": 27.40195655822754,
      "learning_rate": 8.776394152680023e-06,
      "loss": 0.263,
      "step": 19610
    },
    {
      "epoch": 3.5408770979967517,
      "grad_norm": 46.09944534301758,
      "learning_rate": 8.765565782349756e-06,
      "loss": 0.3308,
      "step": 19620
    },
    {
      "epoch": 3.542681826385129,
      "grad_norm": 48.6751594543457,
      "learning_rate": 8.754737412019491e-06,
      "loss": 0.3178,
      "step": 19630
    },
    {
      "epoch": 3.5444865547735067,
      "grad_norm": 11.689988136291504,
      "learning_rate": 8.743909041689226e-06,
      "loss": 0.3524,
      "step": 19640
    },
    {
      "epoch": 3.5462912831618842,
      "grad_norm": 26.1003360748291,
      "learning_rate": 8.733080671358961e-06,
      "loss": 0.2152,
      "step": 19650
    },
    {
      "epoch": 3.5480960115502618,
      "grad_norm": 22.156641006469727,
      "learning_rate": 8.722252301028694e-06,
      "loss": 0.2464,
      "step": 19660
    },
    {
      "epoch": 3.5499007399386393,
      "grad_norm": 10.009259223937988,
      "learning_rate": 8.71142393069843e-06,
      "loss": 0.3013,
      "step": 19670
    },
    {
      "epoch": 3.551705468327017,
      "grad_norm": 1.8138846158981323,
      "learning_rate": 8.700595560368164e-06,
      "loss": 0.3079,
      "step": 19680
    },
    {
      "epoch": 3.5535101967153944,
      "grad_norm": 45.533199310302734,
      "learning_rate": 8.689767190037899e-06,
      "loss": 0.4123,
      "step": 19690
    },
    {
      "epoch": 3.555314925103772,
      "grad_norm": 8.109941482543945,
      "learning_rate": 8.678938819707635e-06,
      "loss": 0.325,
      "step": 19700
    },
    {
      "epoch": 3.5571196534921494,
      "grad_norm": 13.054364204406738,
      "learning_rate": 8.668110449377369e-06,
      "loss": 0.3827,
      "step": 19710
    },
    {
      "epoch": 3.558924381880527,
      "grad_norm": 25.422452926635742,
      "learning_rate": 8.657282079047104e-06,
      "loss": 0.3629,
      "step": 19720
    },
    {
      "epoch": 3.5607291102689045,
      "grad_norm": 52.08665466308594,
      "learning_rate": 8.646453708716838e-06,
      "loss": 0.3802,
      "step": 19730
    },
    {
      "epoch": 3.562533838657282,
      "grad_norm": 12.06253719329834,
      "learning_rate": 8.635625338386573e-06,
      "loss": 0.218,
      "step": 19740
    },
    {
      "epoch": 3.5643385670456595,
      "grad_norm": 62.61819839477539,
      "learning_rate": 8.624796968056307e-06,
      "loss": 0.1998,
      "step": 19750
    },
    {
      "epoch": 3.566143295434037,
      "grad_norm": 31.12265968322754,
      "learning_rate": 8.613968597726043e-06,
      "loss": 0.2664,
      "step": 19760
    },
    {
      "epoch": 3.5679480238224146,
      "grad_norm": 19.847614288330078,
      "learning_rate": 8.603140227395776e-06,
      "loss": 0.2563,
      "step": 19770
    },
    {
      "epoch": 3.569752752210792,
      "grad_norm": 15.37315845489502,
      "learning_rate": 8.592311857065511e-06,
      "loss": 0.3604,
      "step": 19780
    },
    {
      "epoch": 3.5715574805991697,
      "grad_norm": 14.484673500061035,
      "learning_rate": 8.581483486735248e-06,
      "loss": 0.3865,
      "step": 19790
    },
    {
      "epoch": 3.5733622089875476,
      "grad_norm": 32.446842193603516,
      "learning_rate": 8.570655116404981e-06,
      "loss": 0.278,
      "step": 19800
    },
    {
      "epoch": 3.5751669373759247,
      "grad_norm": 20.035892486572266,
      "learning_rate": 8.559826746074716e-06,
      "loss": 0.1487,
      "step": 19810
    },
    {
      "epoch": 3.5769716657643027,
      "grad_norm": 60.8520393371582,
      "learning_rate": 8.548998375744451e-06,
      "loss": 0.3707,
      "step": 19820
    },
    {
      "epoch": 3.57877639415268,
      "grad_norm": 4.323741912841797,
      "learning_rate": 8.538170005414186e-06,
      "loss": 0.4429,
      "step": 19830
    },
    {
      "epoch": 3.5805811225410578,
      "grad_norm": 14.396164894104004,
      "learning_rate": 8.527341635083919e-06,
      "loss": 0.3244,
      "step": 19840
    },
    {
      "epoch": 3.582385850929435,
      "grad_norm": 9.301177978515625,
      "learning_rate": 8.516513264753656e-06,
      "loss": 0.2189,
      "step": 19850
    },
    {
      "epoch": 3.584190579317813,
      "grad_norm": 32.6847038269043,
      "learning_rate": 8.505684894423389e-06,
      "loss": 0.1809,
      "step": 19860
    },
    {
      "epoch": 3.5859953077061903,
      "grad_norm": 53.345760345458984,
      "learning_rate": 8.494856524093124e-06,
      "loss": 0.3776,
      "step": 19870
    },
    {
      "epoch": 3.587800036094568,
      "grad_norm": 29.934537887573242,
      "learning_rate": 8.48402815376286e-06,
      "loss": 0.2756,
      "step": 19880
    },
    {
      "epoch": 3.5896047644829454,
      "grad_norm": 25.863811492919922,
      "learning_rate": 8.473199783432593e-06,
      "loss": 0.1722,
      "step": 19890
    },
    {
      "epoch": 3.591409492871323,
      "grad_norm": 43.48604965209961,
      "learning_rate": 8.462371413102328e-06,
      "loss": 0.293,
      "step": 19900
    },
    {
      "epoch": 3.5932142212597005,
      "grad_norm": 13.452326774597168,
      "learning_rate": 8.451543042772063e-06,
      "loss": 0.2719,
      "step": 19910
    },
    {
      "epoch": 3.595018949648078,
      "grad_norm": 56.54164505004883,
      "learning_rate": 8.440714672441798e-06,
      "loss": 0.2882,
      "step": 19920
    },
    {
      "epoch": 3.5968236780364555,
      "grad_norm": 34.73105239868164,
      "learning_rate": 8.429886302111531e-06,
      "loss": 0.4307,
      "step": 19930
    },
    {
      "epoch": 3.598628406424833,
      "grad_norm": 2.137875556945801,
      "learning_rate": 8.419057931781268e-06,
      "loss": 0.4255,
      "step": 19940
    },
    {
      "epoch": 3.6004331348132106,
      "grad_norm": 7.22125768661499,
      "learning_rate": 8.408229561451001e-06,
      "loss": 0.1837,
      "step": 19950
    },
    {
      "epoch": 3.602237863201588,
      "grad_norm": 60.197818756103516,
      "learning_rate": 8.397401191120736e-06,
      "loss": 0.4412,
      "step": 19960
    },
    {
      "epoch": 3.6040425915899656,
      "grad_norm": 26.025781631469727,
      "learning_rate": 8.386572820790471e-06,
      "loss": 0.4052,
      "step": 19970
    },
    {
      "epoch": 3.605847319978343,
      "grad_norm": 70.93995666503906,
      "learning_rate": 8.375744450460206e-06,
      "loss": 0.3386,
      "step": 19980
    },
    {
      "epoch": 3.6076520483667207,
      "grad_norm": 3.5034053325653076,
      "learning_rate": 8.36491608012994e-06,
      "loss": 0.2037,
      "step": 19990
    },
    {
      "epoch": 3.6094567767550982,
      "grad_norm": 11.715985298156738,
      "learning_rate": 8.354087709799676e-06,
      "loss": 0.4073,
      "step": 20000
    },
    {
      "epoch": 3.6112615051434758,
      "grad_norm": 58.94094467163086,
      "learning_rate": 8.34325933946941e-06,
      "loss": 0.2643,
      "step": 20010
    },
    {
      "epoch": 3.6130662335318533,
      "grad_norm": 7.236018180847168,
      "learning_rate": 8.332430969139144e-06,
      "loss": 0.3515,
      "step": 20020
    },
    {
      "epoch": 3.6148709619202313,
      "grad_norm": 25.98568344116211,
      "learning_rate": 8.32160259880888e-06,
      "loss": 0.4924,
      "step": 20030
    },
    {
      "epoch": 3.6166756903086084,
      "grad_norm": 20.443666458129883,
      "learning_rate": 8.310774228478614e-06,
      "loss": 0.1967,
      "step": 20040
    },
    {
      "epoch": 3.6184804186969863,
      "grad_norm": 51.00764465332031,
      "learning_rate": 8.299945858148349e-06,
      "loss": 0.2716,
      "step": 20050
    },
    {
      "epoch": 3.6202851470853634,
      "grad_norm": 39.69538497924805,
      "learning_rate": 8.289117487818083e-06,
      "loss": 0.3735,
      "step": 20060
    },
    {
      "epoch": 3.6220898754737414,
      "grad_norm": 4.363041877746582,
      "learning_rate": 8.278289117487818e-06,
      "loss": 0.362,
      "step": 20070
    },
    {
      "epoch": 3.6238946038621185,
      "grad_norm": 14.248127937316895,
      "learning_rate": 8.267460747157553e-06,
      "loss": 0.3844,
      "step": 20080
    },
    {
      "epoch": 3.6256993322504965,
      "grad_norm": 26.83836555480957,
      "learning_rate": 8.256632376827288e-06,
      "loss": 0.3352,
      "step": 20090
    },
    {
      "epoch": 3.627504060638874,
      "grad_norm": 3.2757556438446045,
      "learning_rate": 8.245804006497023e-06,
      "loss": 0.19,
      "step": 20100
    },
    {
      "epoch": 3.6293087890272515,
      "grad_norm": 15.5042085647583,
      "learning_rate": 8.234975636166756e-06,
      "loss": 0.1705,
      "step": 20110
    },
    {
      "epoch": 3.631113517415629,
      "grad_norm": 52.44493103027344,
      "learning_rate": 8.224147265836493e-06,
      "loss": 0.307,
      "step": 20120
    },
    {
      "epoch": 3.6329182458040066,
      "grad_norm": 43.27370834350586,
      "learning_rate": 8.213318895506226e-06,
      "loss": 0.3232,
      "step": 20130
    },
    {
      "epoch": 3.634722974192384,
      "grad_norm": 18.921586990356445,
      "learning_rate": 8.202490525175961e-06,
      "loss": 0.3259,
      "step": 20140
    },
    {
      "epoch": 3.6365277025807616,
      "grad_norm": 12.598518371582031,
      "learning_rate": 8.191662154845696e-06,
      "loss": 0.2943,
      "step": 20150
    },
    {
      "epoch": 3.638332430969139,
      "grad_norm": 50.638648986816406,
      "learning_rate": 8.18083378451543e-06,
      "loss": 0.2785,
      "step": 20160
    },
    {
      "epoch": 3.6401371593575167,
      "grad_norm": 84.51235961914062,
      "learning_rate": 8.170005414185166e-06,
      "loss": 0.4754,
      "step": 20170
    },
    {
      "epoch": 3.6419418877458942,
      "grad_norm": 4.928553581237793,
      "learning_rate": 8.1591770438549e-06,
      "loss": 0.2414,
      "step": 20180
    },
    {
      "epoch": 3.6437466161342718,
      "grad_norm": 17.921161651611328,
      "learning_rate": 8.148348673524636e-06,
      "loss": 0.2529,
      "step": 20190
    },
    {
      "epoch": 3.6455513445226493,
      "grad_norm": 9.66262149810791,
      "learning_rate": 8.137520303194369e-06,
      "loss": 0.2363,
      "step": 20200
    },
    {
      "epoch": 3.647356072911027,
      "grad_norm": 45.5087890625,
      "learning_rate": 8.126691932864105e-06,
      "loss": 0.1789,
      "step": 20210
    },
    {
      "epoch": 3.6491608012994043,
      "grad_norm": 15.34039306640625,
      "learning_rate": 8.115863562533839e-06,
      "loss": 0.4156,
      "step": 20220
    },
    {
      "epoch": 3.650965529687782,
      "grad_norm": 9.378482818603516,
      "learning_rate": 8.105035192203573e-06,
      "loss": 0.301,
      "step": 20230
    },
    {
      "epoch": 3.6527702580761594,
      "grad_norm": 10.066020011901855,
      "learning_rate": 8.094206821873308e-06,
      "loss": 0.3787,
      "step": 20240
    },
    {
      "epoch": 3.654574986464537,
      "grad_norm": 9.04762077331543,
      "learning_rate": 8.083378451543043e-06,
      "loss": 0.3074,
      "step": 20250
    },
    {
      "epoch": 3.656379714852915,
      "grad_norm": 39.120704650878906,
      "learning_rate": 8.072550081212776e-06,
      "loss": 0.3389,
      "step": 20260
    },
    {
      "epoch": 3.658184443241292,
      "grad_norm": 14.239563941955566,
      "learning_rate": 8.061721710882513e-06,
      "loss": 0.3588,
      "step": 20270
    },
    {
      "epoch": 3.65998917162967,
      "grad_norm": 37.726051330566406,
      "learning_rate": 8.050893340552248e-06,
      "loss": 0.3061,
      "step": 20280
    },
    {
      "epoch": 3.661793900018047,
      "grad_norm": 38.61568069458008,
      "learning_rate": 8.040064970221981e-06,
      "loss": 0.333,
      "step": 20290
    },
    {
      "epoch": 3.663598628406425,
      "grad_norm": 6.01126766204834,
      "learning_rate": 8.029236599891718e-06,
      "loss": 0.3623,
      "step": 20300
    },
    {
      "epoch": 3.665403356794802,
      "grad_norm": 36.311065673828125,
      "learning_rate": 8.018408229561451e-06,
      "loss": 0.36,
      "step": 20310
    },
    {
      "epoch": 3.66720808518318,
      "grad_norm": 6.465641021728516,
      "learning_rate": 8.007579859231186e-06,
      "loss": 0.305,
      "step": 20320
    },
    {
      "epoch": 3.6690128135715576,
      "grad_norm": 23.301576614379883,
      "learning_rate": 7.99675148890092e-06,
      "loss": 0.52,
      "step": 20330
    },
    {
      "epoch": 3.670817541959935,
      "grad_norm": 56.51067352294922,
      "learning_rate": 7.985923118570656e-06,
      "loss": 0.1938,
      "step": 20340
    },
    {
      "epoch": 3.6726222703483127,
      "grad_norm": 12.768831253051758,
      "learning_rate": 7.975094748240389e-06,
      "loss": 0.2067,
      "step": 20350
    },
    {
      "epoch": 3.67442699873669,
      "grad_norm": 24.237207412719727,
      "learning_rate": 7.964266377910125e-06,
      "loss": 0.3321,
      "step": 20360
    },
    {
      "epoch": 3.6762317271250677,
      "grad_norm": 41.070648193359375,
      "learning_rate": 7.95343800757986e-06,
      "loss": 0.2084,
      "step": 20370
    },
    {
      "epoch": 3.6780364555134453,
      "grad_norm": 7.013434410095215,
      "learning_rate": 7.942609637249594e-06,
      "loss": 0.3412,
      "step": 20380
    },
    {
      "epoch": 3.679841183901823,
      "grad_norm": 23.501544952392578,
      "learning_rate": 7.93178126691933e-06,
      "loss": 0.4852,
      "step": 20390
    },
    {
      "epoch": 3.6816459122902003,
      "grad_norm": 3.1553733348846436,
      "learning_rate": 7.920952896589063e-06,
      "loss": 0.1407,
      "step": 20400
    },
    {
      "epoch": 3.683450640678578,
      "grad_norm": 5.01614236831665,
      "learning_rate": 7.910124526258798e-06,
      "loss": 0.2508,
      "step": 20410
    },
    {
      "epoch": 3.6852553690669554,
      "grad_norm": 42.84613037109375,
      "learning_rate": 7.899296155928533e-06,
      "loss": 0.1977,
      "step": 20420
    },
    {
      "epoch": 3.687060097455333,
      "grad_norm": 12.495580673217773,
      "learning_rate": 7.888467785598268e-06,
      "loss": 0.271,
      "step": 20430
    },
    {
      "epoch": 3.6888648258437104,
      "grad_norm": 34.5440559387207,
      "learning_rate": 7.877639415268001e-06,
      "loss": 0.1931,
      "step": 20440
    },
    {
      "epoch": 3.690669554232088,
      "grad_norm": 21.650657653808594,
      "learning_rate": 7.866811044937738e-06,
      "loss": 0.2325,
      "step": 20450
    },
    {
      "epoch": 3.6924742826204655,
      "grad_norm": 24.493555068969727,
      "learning_rate": 7.855982674607471e-06,
      "loss": 0.3444,
      "step": 20460
    },
    {
      "epoch": 3.694279011008843,
      "grad_norm": 9.004615783691406,
      "learning_rate": 7.845154304277206e-06,
      "loss": 0.3056,
      "step": 20470
    },
    {
      "epoch": 3.6960837393972206,
      "grad_norm": 35.20819854736328,
      "learning_rate": 7.834325933946943e-06,
      "loss": 0.2772,
      "step": 20480
    },
    {
      "epoch": 3.6978884677855985,
      "grad_norm": 49.01835250854492,
      "learning_rate": 7.823497563616676e-06,
      "loss": 0.2818,
      "step": 20490
    },
    {
      "epoch": 3.6996931961739756,
      "grad_norm": 10.327383041381836,
      "learning_rate": 7.81266919328641e-06,
      "loss": 0.3115,
      "step": 20500
    },
    {
      "epoch": 3.7014979245623536,
      "grad_norm": 57.25752258300781,
      "learning_rate": 7.801840822956146e-06,
      "loss": 0.2194,
      "step": 20510
    },
    {
      "epoch": 3.7033026529507307,
      "grad_norm": 16.073644638061523,
      "learning_rate": 7.79101245262588e-06,
      "loss": 0.2767,
      "step": 20520
    },
    {
      "epoch": 3.7051073813391087,
      "grad_norm": 23.015949249267578,
      "learning_rate": 7.780184082295614e-06,
      "loss": 0.2695,
      "step": 20530
    },
    {
      "epoch": 3.7069121097274857,
      "grad_norm": 26.27745246887207,
      "learning_rate": 7.76935571196535e-06,
      "loss": 0.2735,
      "step": 20540
    },
    {
      "epoch": 3.7087168381158637,
      "grad_norm": 2.789228677749634,
      "learning_rate": 7.758527341635084e-06,
      "loss": 0.3472,
      "step": 20550
    },
    {
      "epoch": 3.7105215665042413,
      "grad_norm": 13.108200073242188,
      "learning_rate": 7.747698971304818e-06,
      "loss": 0.3199,
      "step": 20560
    },
    {
      "epoch": 3.712326294892619,
      "grad_norm": 22.27383041381836,
      "learning_rate": 7.736870600974555e-06,
      "loss": 0.2265,
      "step": 20570
    },
    {
      "epoch": 3.7141310232809963,
      "grad_norm": 9.087882041931152,
      "learning_rate": 7.726042230644288e-06,
      "loss": 0.1098,
      "step": 20580
    },
    {
      "epoch": 3.715935751669374,
      "grad_norm": 31.373445510864258,
      "learning_rate": 7.715213860314023e-06,
      "loss": 0.3152,
      "step": 20590
    },
    {
      "epoch": 3.7177404800577514,
      "grad_norm": 36.66810989379883,
      "learning_rate": 7.704385489983758e-06,
      "loss": 0.3515,
      "step": 20600
    },
    {
      "epoch": 3.719545208446129,
      "grad_norm": 15.909492492675781,
      "learning_rate": 7.693557119653493e-06,
      "loss": 0.2646,
      "step": 20610
    },
    {
      "epoch": 3.7213499368345064,
      "grad_norm": 47.21657180786133,
      "learning_rate": 7.682728749323226e-06,
      "loss": 0.1286,
      "step": 20620
    },
    {
      "epoch": 3.723154665222884,
      "grad_norm": 5.0092267990112305,
      "learning_rate": 7.671900378992961e-06,
      "loss": 0.1852,
      "step": 20630
    },
    {
      "epoch": 3.7249593936112615,
      "grad_norm": 37.361915588378906,
      "learning_rate": 7.661072008662696e-06,
      "loss": 0.4145,
      "step": 20640
    },
    {
      "epoch": 3.726764121999639,
      "grad_norm": 26.251266479492188,
      "learning_rate": 7.650243638332431e-06,
      "loss": 0.2346,
      "step": 20650
    },
    {
      "epoch": 3.7285688503880166,
      "grad_norm": 0.8358321189880371,
      "learning_rate": 7.639415268002166e-06,
      "loss": 0.132,
      "step": 20660
    },
    {
      "epoch": 3.730373578776394,
      "grad_norm": 26.377145767211914,
      "learning_rate": 7.628586897671901e-06,
      "loss": 0.2125,
      "step": 20670
    },
    {
      "epoch": 3.7321783071647716,
      "grad_norm": 73.18397521972656,
      "learning_rate": 7.617758527341635e-06,
      "loss": 0.3413,
      "step": 20680
    },
    {
      "epoch": 3.733983035553149,
      "grad_norm": 87.79424285888672,
      "learning_rate": 7.606930157011369e-06,
      "loss": 0.2512,
      "step": 20690
    },
    {
      "epoch": 3.7357877639415267,
      "grad_norm": 3.7821688652038574,
      "learning_rate": 7.596101786681105e-06,
      "loss": 0.2242,
      "step": 20700
    },
    {
      "epoch": 3.737592492329904,
      "grad_norm": 14.122878074645996,
      "learning_rate": 7.5852734163508395e-06,
      "loss": 0.4258,
      "step": 20710
    },
    {
      "epoch": 3.7393972207182817,
      "grad_norm": 16.603294372558594,
      "learning_rate": 7.5744450460205736e-06,
      "loss": 0.1229,
      "step": 20720
    },
    {
      "epoch": 3.7412019491066593,
      "grad_norm": 26.71063995361328,
      "learning_rate": 7.563616675690309e-06,
      "loss": 0.3105,
      "step": 20730
    },
    {
      "epoch": 3.7430066774950372,
      "grad_norm": 41.02286148071289,
      "learning_rate": 7.552788305360043e-06,
      "loss": 0.2636,
      "step": 20740
    },
    {
      "epoch": 3.7448114058834143,
      "grad_norm": 7.089657306671143,
      "learning_rate": 7.5419599350297774e-06,
      "loss": 0.4549,
      "step": 20750
    },
    {
      "epoch": 3.7466161342717923,
      "grad_norm": 0.978696346282959,
      "learning_rate": 7.531131564699513e-06,
      "loss": 0.5481,
      "step": 20760
    },
    {
      "epoch": 3.7484208626601694,
      "grad_norm": 30.9450626373291,
      "learning_rate": 7.520303194369247e-06,
      "loss": 0.3649,
      "step": 20770
    },
    {
      "epoch": 3.7502255910485474,
      "grad_norm": 7.359540939331055,
      "learning_rate": 7.509474824038981e-06,
      "loss": 0.4709,
      "step": 20780
    },
    {
      "epoch": 3.752030319436925,
      "grad_norm": 12.367627143859863,
      "learning_rate": 7.498646453708717e-06,
      "loss": 0.3521,
      "step": 20790
    },
    {
      "epoch": 3.7538350478253024,
      "grad_norm": 32.458370208740234,
      "learning_rate": 7.487818083378451e-06,
      "loss": 0.3147,
      "step": 20800
    },
    {
      "epoch": 3.75563977621368,
      "grad_norm": 44.316795349121094,
      "learning_rate": 7.476989713048187e-06,
      "loss": 0.2989,
      "step": 20810
    },
    {
      "epoch": 3.7574445046020575,
      "grad_norm": 34.644100189208984,
      "learning_rate": 7.466161342717922e-06,
      "loss": 0.3906,
      "step": 20820
    },
    {
      "epoch": 3.759249232990435,
      "grad_norm": 69.24274444580078,
      "learning_rate": 7.455332972387656e-06,
      "loss": 0.2348,
      "step": 20830
    },
    {
      "epoch": 3.7610539613788125,
      "grad_norm": 25.450820922851562,
      "learning_rate": 7.444504602057391e-06,
      "loss": 0.2743,
      "step": 20840
    },
    {
      "epoch": 3.76285868976719,
      "grad_norm": 28.54010581970215,
      "learning_rate": 7.433676231727126e-06,
      "loss": 0.3021,
      "step": 20850
    },
    {
      "epoch": 3.7646634181555676,
      "grad_norm": 55.498130798339844,
      "learning_rate": 7.42284786139686e-06,
      "loss": 0.2911,
      "step": 20860
    },
    {
      "epoch": 3.766468146543945,
      "grad_norm": 4.923892021179199,
      "learning_rate": 7.412019491066595e-06,
      "loss": 0.1389,
      "step": 20870
    },
    {
      "epoch": 3.7682728749323227,
      "grad_norm": 10.80626392364502,
      "learning_rate": 7.4011911207363295e-06,
      "loss": 0.313,
      "step": 20880
    },
    {
      "epoch": 3.7700776033207,
      "grad_norm": 24.402267456054688,
      "learning_rate": 7.3903627504060636e-06,
      "loss": 0.2157,
      "step": 20890
    },
    {
      "epoch": 3.7718823317090777,
      "grad_norm": 16.76115608215332,
      "learning_rate": 7.379534380075799e-06,
      "loss": 0.3677,
      "step": 20900
    },
    {
      "epoch": 3.7736870600974552,
      "grad_norm": 2.0107598304748535,
      "learning_rate": 7.368706009745534e-06,
      "loss": 0.3442,
      "step": 20910
    },
    {
      "epoch": 3.775491788485833,
      "grad_norm": 31.346519470214844,
      "learning_rate": 7.357877639415268e-06,
      "loss": 0.197,
      "step": 20920
    },
    {
      "epoch": 3.7772965168742103,
      "grad_norm": 40.415931701660156,
      "learning_rate": 7.347049269085003e-06,
      "loss": 0.1763,
      "step": 20930
    },
    {
      "epoch": 3.779101245262588,
      "grad_norm": 13.527064323425293,
      "learning_rate": 7.336220898754738e-06,
      "loss": 0.5324,
      "step": 20940
    },
    {
      "epoch": 3.7809059736509654,
      "grad_norm": 4.297542095184326,
      "learning_rate": 7.325392528424472e-06,
      "loss": 0.2211,
      "step": 20950
    },
    {
      "epoch": 3.782710702039343,
      "grad_norm": 23.628135681152344,
      "learning_rate": 7.314564158094207e-06,
      "loss": 0.2761,
      "step": 20960
    },
    {
      "epoch": 3.784515430427721,
      "grad_norm": 47.73928451538086,
      "learning_rate": 7.303735787763941e-06,
      "loss": 0.3154,
      "step": 20970
    },
    {
      "epoch": 3.786320158816098,
      "grad_norm": 57.29744338989258,
      "learning_rate": 7.292907417433676e-06,
      "loss": 0.2583,
      "step": 20980
    },
    {
      "epoch": 3.788124887204476,
      "grad_norm": 23.58721160888672,
      "learning_rate": 7.282079047103411e-06,
      "loss": 0.203,
      "step": 20990
    },
    {
      "epoch": 3.789929615592853,
      "grad_norm": 10.532463073730469,
      "learning_rate": 7.271250676773146e-06,
      "loss": 0.2514,
      "step": 21000
    },
    {
      "epoch": 3.791734343981231,
      "grad_norm": 29.84111785888672,
      "learning_rate": 7.260422306442881e-06,
      "loss": 0.161,
      "step": 21010
    },
    {
      "epoch": 3.7935390723696085,
      "grad_norm": 2.623715877532959,
      "learning_rate": 7.249593936112616e-06,
      "loss": 0.2408,
      "step": 21020
    },
    {
      "epoch": 3.795343800757986,
      "grad_norm": 14.384989738464355,
      "learning_rate": 7.23876556578235e-06,
      "loss": 0.2182,
      "step": 21030
    },
    {
      "epoch": 3.7971485291463636,
      "grad_norm": 9.720314025878906,
      "learning_rate": 7.2279371954520846e-06,
      "loss": 0.3008,
      "step": 21040
    },
    {
      "epoch": 3.798953257534741,
      "grad_norm": 23.753738403320312,
      "learning_rate": 7.2171088251218195e-06,
      "loss": 0.3862,
      "step": 21050
    },
    {
      "epoch": 3.8007579859231186,
      "grad_norm": 20.29755973815918,
      "learning_rate": 7.2062804547915535e-06,
      "loss": 0.2266,
      "step": 21060
    },
    {
      "epoch": 3.802562714311496,
      "grad_norm": 93.66432189941406,
      "learning_rate": 7.1954520844612884e-06,
      "loss": 0.2841,
      "step": 21070
    },
    {
      "epoch": 3.8043674426998737,
      "grad_norm": 51.48801040649414,
      "learning_rate": 7.184623714131023e-06,
      "loss": 0.2665,
      "step": 21080
    },
    {
      "epoch": 3.8061721710882512,
      "grad_norm": 27.521665573120117,
      "learning_rate": 7.173795343800757e-06,
      "loss": 0.2471,
      "step": 21090
    },
    {
      "epoch": 3.8079768994766288,
      "grad_norm": 15.264355659484863,
      "learning_rate": 7.162966973470493e-06,
      "loss": 0.5877,
      "step": 21100
    },
    {
      "epoch": 3.8097816278650063,
      "grad_norm": 0.9832260012626648,
      "learning_rate": 7.152138603140228e-06,
      "loss": 0.2837,
      "step": 21110
    },
    {
      "epoch": 3.811586356253384,
      "grad_norm": 9.72519302368164,
      "learning_rate": 7.141310232809962e-06,
      "loss": 0.3819,
      "step": 21120
    },
    {
      "epoch": 3.8133910846417614,
      "grad_norm": 8.786755561828613,
      "learning_rate": 7.130481862479697e-06,
      "loss": 0.1614,
      "step": 21130
    },
    {
      "epoch": 3.815195813030139,
      "grad_norm": 18.38159942626953,
      "learning_rate": 7.119653492149432e-06,
      "loss": 0.4009,
      "step": 21140
    },
    {
      "epoch": 3.8170005414185164,
      "grad_norm": 1.8948252201080322,
      "learning_rate": 7.108825121819166e-06,
      "loss": 0.1586,
      "step": 21150
    },
    {
      "epoch": 3.818805269806894,
      "grad_norm": 0.1916278451681137,
      "learning_rate": 7.097996751488901e-06,
      "loss": 0.3413,
      "step": 21160
    },
    {
      "epoch": 3.8206099981952715,
      "grad_norm": 3.8765995502471924,
      "learning_rate": 7.087168381158636e-06,
      "loss": 0.3764,
      "step": 21170
    },
    {
      "epoch": 3.822414726583649,
      "grad_norm": 6.7994561195373535,
      "learning_rate": 7.07634001082837e-06,
      "loss": 0.193,
      "step": 21180
    },
    {
      "epoch": 3.8242194549720265,
      "grad_norm": 51.02156066894531,
      "learning_rate": 7.065511640498105e-06,
      "loss": 0.2783,
      "step": 21190
    },
    {
      "epoch": 3.8260241833604045,
      "grad_norm": 1.5391566753387451,
      "learning_rate": 7.0546832701678405e-06,
      "loss": 0.195,
      "step": 21200
    },
    {
      "epoch": 3.8278289117487816,
      "grad_norm": 45.167179107666016,
      "learning_rate": 7.0438548998375746e-06,
      "loss": 0.3119,
      "step": 21210
    },
    {
      "epoch": 3.8296336401371596,
      "grad_norm": 18.85416603088379,
      "learning_rate": 7.0330265295073095e-06,
      "loss": 0.3159,
      "step": 21220
    },
    {
      "epoch": 3.8314383685255367,
      "grad_norm": 8.088949203491211,
      "learning_rate": 7.022198159177044e-06,
      "loss": 0.212,
      "step": 21230
    },
    {
      "epoch": 3.8332430969139146,
      "grad_norm": 10.060822486877441,
      "learning_rate": 7.011369788846778e-06,
      "loss": 0.1657,
      "step": 21240
    },
    {
      "epoch": 3.835047825302292,
      "grad_norm": 25.774438858032227,
      "learning_rate": 7.000541418516513e-06,
      "loss": 0.3766,
      "step": 21250
    },
    {
      "epoch": 3.8368525536906697,
      "grad_norm": 4.733824253082275,
      "learning_rate": 6.989713048186248e-06,
      "loss": 0.1816,
      "step": 21260
    },
    {
      "epoch": 3.838657282079047,
      "grad_norm": 36.54297637939453,
      "learning_rate": 6.978884677855982e-06,
      "loss": 0.5452,
      "step": 21270
    },
    {
      "epoch": 3.8404620104674247,
      "grad_norm": 4.6303486824035645,
      "learning_rate": 6.968056307525717e-06,
      "loss": 0.1613,
      "step": 21280
    },
    {
      "epoch": 3.8422667388558023,
      "grad_norm": 21.003026962280273,
      "learning_rate": 6.957227937195453e-06,
      "loss": 0.2406,
      "step": 21290
    },
    {
      "epoch": 3.84407146724418,
      "grad_norm": 2.067143678665161,
      "learning_rate": 6.946399566865187e-06,
      "loss": 0.3087,
      "step": 21300
    },
    {
      "epoch": 3.8458761956325573,
      "grad_norm": 15.819096565246582,
      "learning_rate": 6.935571196534922e-06,
      "loss": 0.2074,
      "step": 21310
    },
    {
      "epoch": 3.847680924020935,
      "grad_norm": 21.625532150268555,
      "learning_rate": 6.924742826204657e-06,
      "loss": 0.3279,
      "step": 21320
    },
    {
      "epoch": 3.8494856524093124,
      "grad_norm": 17.725753784179688,
      "learning_rate": 6.913914455874391e-06,
      "loss": 0.1588,
      "step": 21330
    },
    {
      "epoch": 3.85129038079769,
      "grad_norm": 0.640703558921814,
      "learning_rate": 6.903086085544126e-06,
      "loss": 0.3081,
      "step": 21340
    },
    {
      "epoch": 3.8530951091860675,
      "grad_norm": 6.582629203796387,
      "learning_rate": 6.892257715213861e-06,
      "loss": 0.2892,
      "step": 21350
    },
    {
      "epoch": 3.854899837574445,
      "grad_norm": 8.166964530944824,
      "learning_rate": 6.881429344883595e-06,
      "loss": 0.3021,
      "step": 21360
    },
    {
      "epoch": 3.8567045659628225,
      "grad_norm": 15.192079544067383,
      "learning_rate": 6.87060097455333e-06,
      "loss": 0.237,
      "step": 21370
    },
    {
      "epoch": 3.8585092943512,
      "grad_norm": 9.431028366088867,
      "learning_rate": 6.8597726042230645e-06,
      "loss": 0.4127,
      "step": 21380
    },
    {
      "epoch": 3.8603140227395776,
      "grad_norm": 29.034669876098633,
      "learning_rate": 6.8489442338927994e-06,
      "loss": 0.3904,
      "step": 21390
    },
    {
      "epoch": 3.862118751127955,
      "grad_norm": 13.613419532775879,
      "learning_rate": 6.838115863562534e-06,
      "loss": 0.2576,
      "step": 21400
    },
    {
      "epoch": 3.8639234795163326,
      "grad_norm": 18.83815574645996,
      "learning_rate": 6.827287493232269e-06,
      "loss": 0.2681,
      "step": 21410
    },
    {
      "epoch": 3.86572820790471,
      "grad_norm": 95.71894073486328,
      "learning_rate": 6.816459122902003e-06,
      "loss": 0.4345,
      "step": 21420
    },
    {
      "epoch": 3.867532936293088,
      "grad_norm": 18.970727920532227,
      "learning_rate": 6.805630752571738e-06,
      "loss": 0.195,
      "step": 21430
    },
    {
      "epoch": 3.8693376646814652,
      "grad_norm": 53.01235580444336,
      "learning_rate": 6.794802382241473e-06,
      "loss": 0.2488,
      "step": 21440
    },
    {
      "epoch": 3.871142393069843,
      "grad_norm": 65.85359954833984,
      "learning_rate": 6.783974011911207e-06,
      "loss": 0.3016,
      "step": 21450
    },
    {
      "epoch": 3.8729471214582203,
      "grad_norm": 3.340888500213623,
      "learning_rate": 6.773145641580942e-06,
      "loss": 0.2014,
      "step": 21460
    },
    {
      "epoch": 3.8747518498465983,
      "grad_norm": 4.0852789878845215,
      "learning_rate": 6.762317271250677e-06,
      "loss": 0.2989,
      "step": 21470
    },
    {
      "epoch": 3.876556578234976,
      "grad_norm": 32.39738082885742,
      "learning_rate": 6.751488900920411e-06,
      "loss": 0.2887,
      "step": 21480
    },
    {
      "epoch": 3.8783613066233533,
      "grad_norm": 31.562480926513672,
      "learning_rate": 6.740660530590147e-06,
      "loss": 0.295,
      "step": 21490
    },
    {
      "epoch": 3.880166035011731,
      "grad_norm": 16.015060424804688,
      "learning_rate": 6.729832160259882e-06,
      "loss": 0.2263,
      "step": 21500
    },
    {
      "epoch": 3.8819707634001084,
      "grad_norm": 4.950950622558594,
      "learning_rate": 6.719003789929616e-06,
      "loss": 0.3008,
      "step": 21510
    },
    {
      "epoch": 3.883775491788486,
      "grad_norm": 47.974266052246094,
      "learning_rate": 6.708175419599351e-06,
      "loss": 0.298,
      "step": 21520
    },
    {
      "epoch": 3.8855802201768634,
      "grad_norm": 28.52312469482422,
      "learning_rate": 6.6973470492690855e-06,
      "loss": 0.31,
      "step": 21530
    },
    {
      "epoch": 3.887384948565241,
      "grad_norm": 32.470035552978516,
      "learning_rate": 6.68651867893882e-06,
      "loss": 0.0951,
      "step": 21540
    },
    {
      "epoch": 3.8891896769536185,
      "grad_norm": 31.444561004638672,
      "learning_rate": 6.6756903086085545e-06,
      "loss": 0.3535,
      "step": 21550
    },
    {
      "epoch": 3.890994405341996,
      "grad_norm": 38.463077545166016,
      "learning_rate": 6.664861938278289e-06,
      "loss": 0.2451,
      "step": 21560
    },
    {
      "epoch": 3.8927991337303736,
      "grad_norm": 1.3982844352722168,
      "learning_rate": 6.6540335679480235e-06,
      "loss": 0.2408,
      "step": 21570
    },
    {
      "epoch": 3.894603862118751,
      "grad_norm": 0.6907824873924255,
      "learning_rate": 6.643205197617758e-06,
      "loss": 0.2261,
      "step": 21580
    },
    {
      "epoch": 3.8964085905071286,
      "grad_norm": 36.01444625854492,
      "learning_rate": 6.632376827287494e-06,
      "loss": 0.392,
      "step": 21590
    },
    {
      "epoch": 3.898213318895506,
      "grad_norm": 18.411935806274414,
      "learning_rate": 6.621548456957228e-06,
      "loss": 0.3002,
      "step": 21600
    },
    {
      "epoch": 3.9000180472838837,
      "grad_norm": 6.870388031005859,
      "learning_rate": 6.610720086626963e-06,
      "loss": 0.26,
      "step": 21610
    },
    {
      "epoch": 3.901822775672261,
      "grad_norm": 11.150046348571777,
      "learning_rate": 6.599891716296698e-06,
      "loss": 0.2436,
      "step": 21620
    },
    {
      "epoch": 3.9036275040606387,
      "grad_norm": 53.477745056152344,
      "learning_rate": 6.589063345966432e-06,
      "loss": 0.3233,
      "step": 21630
    },
    {
      "epoch": 3.9054322324490163,
      "grad_norm": 36.849002838134766,
      "learning_rate": 6.578234975636167e-06,
      "loss": 0.2778,
      "step": 21640
    },
    {
      "epoch": 3.907236960837394,
      "grad_norm": 5.233004570007324,
      "learning_rate": 6.567406605305902e-06,
      "loss": 0.3886,
      "step": 21650
    },
    {
      "epoch": 3.9090416892257718,
      "grad_norm": 36.22602081298828,
      "learning_rate": 6.556578234975636e-06,
      "loss": 0.4149,
      "step": 21660
    },
    {
      "epoch": 3.910846417614149,
      "grad_norm": 3.8174057006835938,
      "learning_rate": 6.545749864645371e-06,
      "loss": 0.2395,
      "step": 21670
    },
    {
      "epoch": 3.912651146002527,
      "grad_norm": 3.5402822494506836,
      "learning_rate": 6.534921494315106e-06,
      "loss": 0.1021,
      "step": 21680
    },
    {
      "epoch": 3.914455874390904,
      "grad_norm": 40.67601776123047,
      "learning_rate": 6.524093123984841e-06,
      "loss": 0.2492,
      "step": 21690
    },
    {
      "epoch": 3.916260602779282,
      "grad_norm": 0.2870263457298279,
      "learning_rate": 6.5132647536545755e-06,
      "loss": 0.1291,
      "step": 21700
    },
    {
      "epoch": 3.9180653311676594,
      "grad_norm": 32.570648193359375,
      "learning_rate": 6.50243638332431e-06,
      "loss": 0.451,
      "step": 21710
    },
    {
      "epoch": 3.919870059556037,
      "grad_norm": 20.78813934326172,
      "learning_rate": 6.4916080129940445e-06,
      "loss": 0.372,
      "step": 21720
    },
    {
      "epoch": 3.9216747879444145,
      "grad_norm": 28.805042266845703,
      "learning_rate": 6.480779642663779e-06,
      "loss": 0.3087,
      "step": 21730
    },
    {
      "epoch": 3.923479516332792,
      "grad_norm": 4.141811370849609,
      "learning_rate": 6.4699512723335135e-06,
      "loss": 0.2572,
      "step": 21740
    },
    {
      "epoch": 3.9252842447211695,
      "grad_norm": 50.34230041503906,
      "learning_rate": 6.459122902003248e-06,
      "loss": 0.2502,
      "step": 21750
    },
    {
      "epoch": 3.927088973109547,
      "grad_norm": 56.60251235961914,
      "learning_rate": 6.448294531672983e-06,
      "loss": 0.1735,
      "step": 21760
    },
    {
      "epoch": 3.9288937014979246,
      "grad_norm": 24.23486328125,
      "learning_rate": 6.437466161342717e-06,
      "loss": 0.2293,
      "step": 21770
    },
    {
      "epoch": 3.930698429886302,
      "grad_norm": 38.92995834350586,
      "learning_rate": 6.426637791012453e-06,
      "loss": 0.3729,
      "step": 21780
    },
    {
      "epoch": 3.9325031582746797,
      "grad_norm": 12.55611801147461,
      "learning_rate": 6.415809420682188e-06,
      "loss": 0.4386,
      "step": 21790
    },
    {
      "epoch": 3.934307886663057,
      "grad_norm": 66.4541244506836,
      "learning_rate": 6.404981050351922e-06,
      "loss": 0.3801,
      "step": 21800
    },
    {
      "epoch": 3.9361126150514347,
      "grad_norm": 28.081361770629883,
      "learning_rate": 6.394152680021657e-06,
      "loss": 0.3589,
      "step": 21810
    },
    {
      "epoch": 3.9379173434398123,
      "grad_norm": 0.8139014840126038,
      "learning_rate": 6.383324309691392e-06,
      "loss": 0.2315,
      "step": 21820
    },
    {
      "epoch": 3.93972207182819,
      "grad_norm": 23.782140731811523,
      "learning_rate": 6.372495939361126e-06,
      "loss": 0.2175,
      "step": 21830
    },
    {
      "epoch": 3.9415268002165673,
      "grad_norm": 19.322847366333008,
      "learning_rate": 6.361667569030861e-06,
      "loss": 0.2045,
      "step": 21840
    },
    {
      "epoch": 3.943331528604945,
      "grad_norm": 8.606433868408203,
      "learning_rate": 6.350839198700596e-06,
      "loss": 0.3474,
      "step": 21850
    },
    {
      "epoch": 3.9451362569933224,
      "grad_norm": 3.728545665740967,
      "learning_rate": 6.34001082837033e-06,
      "loss": 0.2742,
      "step": 21860
    },
    {
      "epoch": 3.9469409853817,
      "grad_norm": 37.13081359863281,
      "learning_rate": 6.329182458040065e-06,
      "loss": 0.2951,
      "step": 21870
    },
    {
      "epoch": 3.9487457137700774,
      "grad_norm": 85.2458267211914,
      "learning_rate": 6.3183540877098e-06,
      "loss": 0.495,
      "step": 21880
    },
    {
      "epoch": 3.9505504421584554,
      "grad_norm": 27.695571899414062,
      "learning_rate": 6.3075257173795345e-06,
      "loss": 0.1887,
      "step": 21890
    },
    {
      "epoch": 3.9523551705468325,
      "grad_norm": 2.01399302482605,
      "learning_rate": 6.296697347049269e-06,
      "loss": 0.2974,
      "step": 21900
    },
    {
      "epoch": 3.9541598989352105,
      "grad_norm": 5.757099628448486,
      "learning_rate": 6.285868976719004e-06,
      "loss": 0.1819,
      "step": 21910
    },
    {
      "epoch": 3.9559646273235876,
      "grad_norm": 35.80559539794922,
      "learning_rate": 6.275040606388738e-06,
      "loss": 0.2835,
      "step": 21920
    },
    {
      "epoch": 3.9577693557119655,
      "grad_norm": 47.68564224243164,
      "learning_rate": 6.264212236058473e-06,
      "loss": 0.4092,
      "step": 21930
    },
    {
      "epoch": 3.959574084100343,
      "grad_norm": 51.97223663330078,
      "learning_rate": 6.253383865728208e-06,
      "loss": 0.2645,
      "step": 21940
    },
    {
      "epoch": 3.9613788124887206,
      "grad_norm": 64.20997619628906,
      "learning_rate": 6.242555495397942e-06,
      "loss": 0.3227,
      "step": 21950
    },
    {
      "epoch": 3.963183540877098,
      "grad_norm": 20.586179733276367,
      "learning_rate": 6.231727125067677e-06,
      "loss": 0.1958,
      "step": 21960
    },
    {
      "epoch": 3.9649882692654757,
      "grad_norm": 58.727474212646484,
      "learning_rate": 6.220898754737412e-06,
      "loss": 0.3337,
      "step": 21970
    },
    {
      "epoch": 3.966792997653853,
      "grad_norm": 7.4702324867248535,
      "learning_rate": 6.210070384407147e-06,
      "loss": 0.2109,
      "step": 21980
    },
    {
      "epoch": 3.9685977260422307,
      "grad_norm": 24.108797073364258,
      "learning_rate": 6.199242014076882e-06,
      "loss": 0.318,
      "step": 21990
    },
    {
      "epoch": 3.9704024544306082,
      "grad_norm": 8.511393547058105,
      "learning_rate": 6.188413643746617e-06,
      "loss": 0.3513,
      "step": 22000
    },
    {
      "epoch": 3.9722071828189858,
      "grad_norm": 9.046168327331543,
      "learning_rate": 6.177585273416351e-06,
      "loss": 0.2712,
      "step": 22010
    },
    {
      "epoch": 3.9740119112073633,
      "grad_norm": 14.185164451599121,
      "learning_rate": 6.166756903086086e-06,
      "loss": 0.2004,
      "step": 22020
    },
    {
      "epoch": 3.975816639595741,
      "grad_norm": 0.6772533059120178,
      "learning_rate": 6.155928532755821e-06,
      "loss": 0.2438,
      "step": 22030
    },
    {
      "epoch": 3.9776213679841184,
      "grad_norm": 7.106734752655029,
      "learning_rate": 6.145100162425555e-06,
      "loss": 0.2318,
      "step": 22040
    },
    {
      "epoch": 3.979426096372496,
      "grad_norm": 54.521080017089844,
      "learning_rate": 6.1342717920952895e-06,
      "loss": 0.3085,
      "step": 22050
    },
    {
      "epoch": 3.9812308247608734,
      "grad_norm": 24.133930206298828,
      "learning_rate": 6.1234434217650244e-06,
      "loss": 0.2753,
      "step": 22060
    },
    {
      "epoch": 3.983035553149251,
      "grad_norm": 0.26267915964126587,
      "learning_rate": 6.112615051434759e-06,
      "loss": 0.276,
      "step": 22070
    },
    {
      "epoch": 3.9848402815376285,
      "grad_norm": 3.5812125205993652,
      "learning_rate": 6.101786681104494e-06,
      "loss": 0.1819,
      "step": 22080
    },
    {
      "epoch": 3.986645009926006,
      "grad_norm": 3.454089879989624,
      "learning_rate": 6.090958310774229e-06,
      "loss": 0.2723,
      "step": 22090
    },
    {
      "epoch": 3.9884497383143835,
      "grad_norm": 41.40108871459961,
      "learning_rate": 6.080129940443963e-06,
      "loss": 0.4992,
      "step": 22100
    },
    {
      "epoch": 3.990254466702761,
      "grad_norm": 37.73141098022461,
      "learning_rate": 6.069301570113698e-06,
      "loss": 0.1962,
      "step": 22110
    },
    {
      "epoch": 3.992059195091139,
      "grad_norm": 17.97486114501953,
      "learning_rate": 6.058473199783433e-06,
      "loss": 0.2907,
      "step": 22120
    },
    {
      "epoch": 3.993863923479516,
      "grad_norm": 1.423384189605713,
      "learning_rate": 6.047644829453167e-06,
      "loss": 0.2523,
      "step": 22130
    },
    {
      "epoch": 3.995668651867894,
      "grad_norm": 22.94454002380371,
      "learning_rate": 6.036816459122902e-06,
      "loss": 0.1964,
      "step": 22140
    },
    {
      "epoch": 3.997473380256271,
      "grad_norm": 1.5091224908828735,
      "learning_rate": 6.025988088792637e-06,
      "loss": 0.2539,
      "step": 22150
    },
    {
      "epoch": 3.999278108644649,
      "grad_norm": 35.035118103027344,
      "learning_rate": 6.015159718462371e-06,
      "loss": 0.4268,
      "step": 22160
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8367549519469386,
      "eval_f1": 0.7633712522290622,
      "eval_loss": 0.9668555855751038,
      "eval_precision": 0.7692790739517075,
      "eval_recall": 0.760523526716848,
      "eval_runtime": 160.4898,
      "eval_samples_per_second": 138.096,
      "eval_steps_per_second": 8.636,
      "step": 22164
    },
    {
      "epoch": 4.001082837033026,
      "grad_norm": 23.1127872467041,
      "learning_rate": 6.004331348132107e-06,
      "loss": 0.3781,
      "step": 22170
    },
    {
      "epoch": 4.002887565421404,
      "grad_norm": 2.184041976928711,
      "learning_rate": 5.993502977801842e-06,
      "loss": 0.2953,
      "step": 22180
    },
    {
      "epoch": 4.004692293809781,
      "grad_norm": 36.05751419067383,
      "learning_rate": 5.982674607471576e-06,
      "loss": 0.2655,
      "step": 22190
    },
    {
      "epoch": 4.006497022198159,
      "grad_norm": 41.961116790771484,
      "learning_rate": 5.9718462371413106e-06,
      "loss": 0.3364,
      "step": 22200
    },
    {
      "epoch": 4.008301750586536,
      "grad_norm": 101.42117309570312,
      "learning_rate": 5.9610178668110455e-06,
      "loss": 0.1449,
      "step": 22210
    },
    {
      "epoch": 4.010106478974914,
      "grad_norm": 56.08428192138672,
      "learning_rate": 5.9501894964807795e-06,
      "loss": 0.1915,
      "step": 22220
    },
    {
      "epoch": 4.011911207363291,
      "grad_norm": 3.6336851119995117,
      "learning_rate": 5.9393611261505144e-06,
      "loss": 0.3092,
      "step": 22230
    },
    {
      "epoch": 4.013715935751669,
      "grad_norm": 9.360505104064941,
      "learning_rate": 5.928532755820249e-06,
      "loss": 0.2097,
      "step": 22240
    },
    {
      "epoch": 4.0155206641400465,
      "grad_norm": 10.205100059509277,
      "learning_rate": 5.917704385489983e-06,
      "loss": 0.1396,
      "step": 22250
    },
    {
      "epoch": 4.0173253925284245,
      "grad_norm": 1.2428439855575562,
      "learning_rate": 5.906876015159718e-06,
      "loss": 0.2506,
      "step": 22260
    },
    {
      "epoch": 4.019130120916802,
      "grad_norm": 0.46880802512168884,
      "learning_rate": 5.896047644829454e-06,
      "loss": 0.231,
      "step": 22270
    },
    {
      "epoch": 4.0209348493051795,
      "grad_norm": 1.3551814556121826,
      "learning_rate": 5.885219274499188e-06,
      "loss": 0.1319,
      "step": 22280
    },
    {
      "epoch": 4.0227395776935575,
      "grad_norm": 13.01402473449707,
      "learning_rate": 5.874390904168923e-06,
      "loss": 0.2734,
      "step": 22290
    },
    {
      "epoch": 4.024544306081935,
      "grad_norm": 13.623546600341797,
      "learning_rate": 5.863562533838658e-06,
      "loss": 0.1592,
      "step": 22300
    },
    {
      "epoch": 4.026349034470313,
      "grad_norm": 21.651437759399414,
      "learning_rate": 5.852734163508392e-06,
      "loss": 0.1595,
      "step": 22310
    },
    {
      "epoch": 4.02815376285869,
      "grad_norm": 6.640425682067871,
      "learning_rate": 5.841905793178127e-06,
      "loss": 0.2357,
      "step": 22320
    },
    {
      "epoch": 4.029958491247068,
      "grad_norm": 0.23324589431285858,
      "learning_rate": 5.831077422847862e-06,
      "loss": 0.2037,
      "step": 22330
    },
    {
      "epoch": 4.031763219635445,
      "grad_norm": 12.477275848388672,
      "learning_rate": 5.820249052517596e-06,
      "loss": 0.057,
      "step": 22340
    },
    {
      "epoch": 4.033567948023823,
      "grad_norm": 12.206063270568848,
      "learning_rate": 5.809420682187331e-06,
      "loss": 0.264,
      "step": 22350
    },
    {
      "epoch": 4.0353726764122,
      "grad_norm": 70.5079574584961,
      "learning_rate": 5.7985923118570665e-06,
      "loss": 0.2054,
      "step": 22360
    },
    {
      "epoch": 4.037177404800578,
      "grad_norm": 38.15581512451172,
      "learning_rate": 5.7877639415268005e-06,
      "loss": 0.1762,
      "step": 22370
    },
    {
      "epoch": 4.038982133188955,
      "grad_norm": 12.421483993530273,
      "learning_rate": 5.7769355711965354e-06,
      "loss": 0.356,
      "step": 22380
    },
    {
      "epoch": 4.040786861577333,
      "grad_norm": 37.72291564941406,
      "learning_rate": 5.7661072008662695e-06,
      "loss": 0.2199,
      "step": 22390
    },
    {
      "epoch": 4.04259158996571,
      "grad_norm": 12.603015899658203,
      "learning_rate": 5.755278830536004e-06,
      "loss": 0.1058,
      "step": 22400
    },
    {
      "epoch": 4.044396318354088,
      "grad_norm": 76.51185607910156,
      "learning_rate": 5.744450460205739e-06,
      "loss": 0.3553,
      "step": 22410
    },
    {
      "epoch": 4.046201046742465,
      "grad_norm": 36.788482666015625,
      "learning_rate": 5.733622089875473e-06,
      "loss": 0.1228,
      "step": 22420
    },
    {
      "epoch": 4.048005775130843,
      "grad_norm": 14.881492614746094,
      "learning_rate": 5.722793719545208e-06,
      "loss": 0.1446,
      "step": 22430
    },
    {
      "epoch": 4.04981050351922,
      "grad_norm": 0.7058395147323608,
      "learning_rate": 5.711965349214943e-06,
      "loss": 0.2936,
      "step": 22440
    },
    {
      "epoch": 4.051615231907598,
      "grad_norm": 3.026355743408203,
      "learning_rate": 5.701136978884677e-06,
      "loss": 0.223,
      "step": 22450
    },
    {
      "epoch": 4.053419960295975,
      "grad_norm": 9.789661407470703,
      "learning_rate": 5.690308608554413e-06,
      "loss": 0.2981,
      "step": 22460
    },
    {
      "epoch": 4.055224688684353,
      "grad_norm": 42.37092208862305,
      "learning_rate": 5.679480238224148e-06,
      "loss": 0.2651,
      "step": 22470
    },
    {
      "epoch": 4.05702941707273,
      "grad_norm": 45.310787200927734,
      "learning_rate": 5.668651867893882e-06,
      "loss": 0.2017,
      "step": 22480
    },
    {
      "epoch": 4.058834145461108,
      "grad_norm": 49.076690673828125,
      "learning_rate": 5.657823497563617e-06,
      "loss": 0.1435,
      "step": 22490
    },
    {
      "epoch": 4.060638873849486,
      "grad_norm": 2.887629508972168,
      "learning_rate": 5.646995127233352e-06,
      "loss": 0.0821,
      "step": 22500
    },
    {
      "epoch": 4.062443602237863,
      "grad_norm": 11.471270561218262,
      "learning_rate": 5.636166756903086e-06,
      "loss": 0.3577,
      "step": 22510
    },
    {
      "epoch": 4.064248330626241,
      "grad_norm": 11.168744087219238,
      "learning_rate": 5.625338386572821e-06,
      "loss": 0.1195,
      "step": 22520
    },
    {
      "epoch": 4.066053059014618,
      "grad_norm": 4.861672878265381,
      "learning_rate": 5.614510016242556e-06,
      "loss": 0.1354,
      "step": 22530
    },
    {
      "epoch": 4.067857787402996,
      "grad_norm": 8.343036651611328,
      "learning_rate": 5.60368164591229e-06,
      "loss": 0.1201,
      "step": 22540
    },
    {
      "epoch": 4.069662515791373,
      "grad_norm": 46.67564392089844,
      "learning_rate": 5.592853275582025e-06,
      "loss": 0.1703,
      "step": 22550
    },
    {
      "epoch": 4.071467244179751,
      "grad_norm": 80.84088897705078,
      "learning_rate": 5.58202490525176e-06,
      "loss": 0.0972,
      "step": 22560
    },
    {
      "epoch": 4.073271972568128,
      "grad_norm": 0.23553705215454102,
      "learning_rate": 5.571196534921494e-06,
      "loss": 0.1118,
      "step": 22570
    },
    {
      "epoch": 4.075076700956506,
      "grad_norm": 1.0015236139297485,
      "learning_rate": 5.560368164591229e-06,
      "loss": 0.1847,
      "step": 22580
    },
    {
      "epoch": 4.076881429344883,
      "grad_norm": 40.05290222167969,
      "learning_rate": 5.549539794260964e-06,
      "loss": 0.1305,
      "step": 22590
    },
    {
      "epoch": 4.078686157733261,
      "grad_norm": 11.622992515563965,
      "learning_rate": 5.538711423930698e-06,
      "loss": 0.2744,
      "step": 22600
    },
    {
      "epoch": 4.0804908861216385,
      "grad_norm": 63.08415985107422,
      "learning_rate": 5.527883053600433e-06,
      "loss": 0.241,
      "step": 22610
    },
    {
      "epoch": 4.082295614510016,
      "grad_norm": 26.723466873168945,
      "learning_rate": 5.517054683270168e-06,
      "loss": 0.2489,
      "step": 22620
    },
    {
      "epoch": 4.0841003428983935,
      "grad_norm": 13.648067474365234,
      "learning_rate": 5.506226312939902e-06,
      "loss": 0.051,
      "step": 22630
    },
    {
      "epoch": 4.0859050712867715,
      "grad_norm": 81.83600616455078,
      "learning_rate": 5.495397942609637e-06,
      "loss": 0.3173,
      "step": 22640
    },
    {
      "epoch": 4.087709799675149,
      "grad_norm": 85.56314086914062,
      "learning_rate": 5.484569572279372e-06,
      "loss": 0.2249,
      "step": 22650
    },
    {
      "epoch": 4.0895145280635266,
      "grad_norm": 71.11666870117188,
      "learning_rate": 5.473741201949107e-06,
      "loss": 0.3356,
      "step": 22660
    },
    {
      "epoch": 4.091319256451904,
      "grad_norm": 15.6878080368042,
      "learning_rate": 5.462912831618842e-06,
      "loss": 0.1885,
      "step": 22670
    },
    {
      "epoch": 4.093123984840282,
      "grad_norm": 0.1334271878004074,
      "learning_rate": 5.452084461288577e-06,
      "loss": 0.1022,
      "step": 22680
    },
    {
      "epoch": 4.094928713228659,
      "grad_norm": 0.4603853225708008,
      "learning_rate": 5.441256090958311e-06,
      "loss": 0.1731,
      "step": 22690
    },
    {
      "epoch": 4.096733441617037,
      "grad_norm": 1.5100816488265991,
      "learning_rate": 5.430427720628046e-06,
      "loss": 0.3695,
      "step": 22700
    },
    {
      "epoch": 4.098538170005414,
      "grad_norm": 1.2590278387069702,
      "learning_rate": 5.4195993502977805e-06,
      "loss": 0.1954,
      "step": 22710
    },
    {
      "epoch": 4.100342898393792,
      "grad_norm": 14.396673202514648,
      "learning_rate": 5.4087709799675146e-06,
      "loss": 0.3586,
      "step": 22720
    },
    {
      "epoch": 4.10214762678217,
      "grad_norm": 16.410184860229492,
      "learning_rate": 5.3979426096372495e-06,
      "loss": 0.2289,
      "step": 22730
    },
    {
      "epoch": 4.103952355170547,
      "grad_norm": 36.61826705932617,
      "learning_rate": 5.387114239306984e-06,
      "loss": 0.2667,
      "step": 22740
    },
    {
      "epoch": 4.105757083558925,
      "grad_norm": 4.991422176361084,
      "learning_rate": 5.376285868976719e-06,
      "loss": 0.1383,
      "step": 22750
    },
    {
      "epoch": 4.107561811947302,
      "grad_norm": 49.50053787231445,
      "learning_rate": 5.365457498646454e-06,
      "loss": 0.3331,
      "step": 22760
    },
    {
      "epoch": 4.10936654033568,
      "grad_norm": 17.619863510131836,
      "learning_rate": 5.354629128316189e-06,
      "loss": 0.2017,
      "step": 22770
    },
    {
      "epoch": 4.111171268724057,
      "grad_norm": 20.978939056396484,
      "learning_rate": 5.343800757985923e-06,
      "loss": 0.172,
      "step": 22780
    },
    {
      "epoch": 4.112975997112435,
      "grad_norm": 4.518848419189453,
      "learning_rate": 5.332972387655658e-06,
      "loss": 0.0672,
      "step": 22790
    },
    {
      "epoch": 4.114780725500812,
      "grad_norm": 5.5463714599609375,
      "learning_rate": 5.322144017325393e-06,
      "loss": 0.183,
      "step": 22800
    },
    {
      "epoch": 4.11658545388919,
      "grad_norm": 32.37409973144531,
      "learning_rate": 5.311315646995127e-06,
      "loss": 0.2444,
      "step": 22810
    },
    {
      "epoch": 4.118390182277567,
      "grad_norm": 12.97862434387207,
      "learning_rate": 5.300487276664862e-06,
      "loss": 0.2391,
      "step": 22820
    },
    {
      "epoch": 4.120194910665945,
      "grad_norm": 0.6206688284873962,
      "learning_rate": 5.289658906334597e-06,
      "loss": 0.4072,
      "step": 22830
    },
    {
      "epoch": 4.121999639054322,
      "grad_norm": 0.2971078157424927,
      "learning_rate": 5.278830536004331e-06,
      "loss": 0.4085,
      "step": 22840
    },
    {
      "epoch": 4.1238043674427,
      "grad_norm": 18.482635498046875,
      "learning_rate": 5.268002165674067e-06,
      "loss": 0.4079,
      "step": 22850
    },
    {
      "epoch": 4.125609095831077,
      "grad_norm": 74.3267593383789,
      "learning_rate": 5.2571737953438015e-06,
      "loss": 0.1945,
      "step": 22860
    },
    {
      "epoch": 4.127413824219455,
      "grad_norm": 8.746713638305664,
      "learning_rate": 5.246345425013536e-06,
      "loss": 0.2002,
      "step": 22870
    },
    {
      "epoch": 4.129218552607832,
      "grad_norm": 3.070162057876587,
      "learning_rate": 5.2355170546832705e-06,
      "loss": 0.161,
      "step": 22880
    },
    {
      "epoch": 4.13102328099621,
      "grad_norm": 7.983405113220215,
      "learning_rate": 5.224688684353005e-06,
      "loss": 0.1808,
      "step": 22890
    },
    {
      "epoch": 4.132828009384587,
      "grad_norm": 13.006985664367676,
      "learning_rate": 5.2138603140227394e-06,
      "loss": 0.1951,
      "step": 22900
    },
    {
      "epoch": 4.134632737772965,
      "grad_norm": 1.7652791738510132,
      "learning_rate": 5.203031943692474e-06,
      "loss": 0.1804,
      "step": 22910
    },
    {
      "epoch": 4.136437466161342,
      "grad_norm": 14.310812950134277,
      "learning_rate": 5.192203573362209e-06,
      "loss": 0.1516,
      "step": 22920
    },
    {
      "epoch": 4.13824219454972,
      "grad_norm": 5.075850963592529,
      "learning_rate": 5.181375203031943e-06,
      "loss": 0.1325,
      "step": 22930
    },
    {
      "epoch": 4.140046922938097,
      "grad_norm": 0.7655494809150696,
      "learning_rate": 5.170546832701678e-06,
      "loss": 0.267,
      "step": 22940
    },
    {
      "epoch": 4.141851651326475,
      "grad_norm": 3.6597743034362793,
      "learning_rate": 5.159718462371414e-06,
      "loss": 0.1476,
      "step": 22950
    },
    {
      "epoch": 4.143656379714853,
      "grad_norm": 49.82360076904297,
      "learning_rate": 5.148890092041148e-06,
      "loss": 0.242,
      "step": 22960
    },
    {
      "epoch": 4.14546110810323,
      "grad_norm": 4.477850914001465,
      "learning_rate": 5.138061721710883e-06,
      "loss": 0.1964,
      "step": 22970
    },
    {
      "epoch": 4.147265836491608,
      "grad_norm": 67.27470397949219,
      "learning_rate": 5.127233351380618e-06,
      "loss": 0.2428,
      "step": 22980
    },
    {
      "epoch": 4.1490705648799855,
      "grad_norm": 0.6170347332954407,
      "learning_rate": 5.116404981050352e-06,
      "loss": 0.1997,
      "step": 22990
    },
    {
      "epoch": 4.1508752932683635,
      "grad_norm": 21.750497817993164,
      "learning_rate": 5.105576610720087e-06,
      "loss": 0.34,
      "step": 23000
    },
    {
      "epoch": 4.1526800216567405,
      "grad_norm": 24.228940963745117,
      "learning_rate": 5.094748240389822e-06,
      "loss": 0.2421,
      "step": 23010
    },
    {
      "epoch": 4.1544847500451185,
      "grad_norm": 38.46881103515625,
      "learning_rate": 5.083919870059556e-06,
      "loss": 0.3144,
      "step": 23020
    },
    {
      "epoch": 4.156289478433496,
      "grad_norm": 4.001655101776123,
      "learning_rate": 5.073091499729291e-06,
      "loss": 0.1528,
      "step": 23030
    },
    {
      "epoch": 4.158094206821874,
      "grad_norm": 7.544920921325684,
      "learning_rate": 5.0622631293990256e-06,
      "loss": 0.1868,
      "step": 23040
    },
    {
      "epoch": 4.159898935210251,
      "grad_norm": 16.678386688232422,
      "learning_rate": 5.0514347590687605e-06,
      "loss": 0.0869,
      "step": 23050
    },
    {
      "epoch": 4.161703663598629,
      "grad_norm": 15.32739543914795,
      "learning_rate": 5.040606388738495e-06,
      "loss": 0.5139,
      "step": 23060
    },
    {
      "epoch": 4.163508391987006,
      "grad_norm": 51.91714859008789,
      "learning_rate": 5.02977801840823e-06,
      "loss": 0.1985,
      "step": 23070
    },
    {
      "epoch": 4.165313120375384,
      "grad_norm": 16.14574432373047,
      "learning_rate": 5.018949648077964e-06,
      "loss": 0.2658,
      "step": 23080
    },
    {
      "epoch": 4.167117848763761,
      "grad_norm": 0.4074099659919739,
      "learning_rate": 5.008121277747699e-06,
      "loss": 0.1855,
      "step": 23090
    },
    {
      "epoch": 4.168922577152139,
      "grad_norm": 7.328941345214844,
      "learning_rate": 4.997292907417433e-06,
      "loss": 0.1771,
      "step": 23100
    },
    {
      "epoch": 4.170727305540516,
      "grad_norm": 0.2386941909790039,
      "learning_rate": 4.986464537087168e-06,
      "loss": 0.1809,
      "step": 23110
    },
    {
      "epoch": 4.172532033928894,
      "grad_norm": 5.552281856536865,
      "learning_rate": 4.975636166756903e-06,
      "loss": 0.0748,
      "step": 23120
    },
    {
      "epoch": 4.174336762317271,
      "grad_norm": 15.56494426727295,
      "learning_rate": 4.964807796426637e-06,
      "loss": 0.2132,
      "step": 23130
    },
    {
      "epoch": 4.176141490705649,
      "grad_norm": 55.1138916015625,
      "learning_rate": 4.953979426096373e-06,
      "loss": 0.224,
      "step": 23140
    },
    {
      "epoch": 4.177946219094026,
      "grad_norm": 50.302520751953125,
      "learning_rate": 4.943151055766108e-06,
      "loss": 0.243,
      "step": 23150
    },
    {
      "epoch": 4.179750947482404,
      "grad_norm": 25.305709838867188,
      "learning_rate": 4.932322685435842e-06,
      "loss": 0.2842,
      "step": 23160
    },
    {
      "epoch": 4.181555675870781,
      "grad_norm": 82.49691009521484,
      "learning_rate": 4.921494315105577e-06,
      "loss": 0.1029,
      "step": 23170
    },
    {
      "epoch": 4.183360404259159,
      "grad_norm": 40.6524772644043,
      "learning_rate": 4.910665944775312e-06,
      "loss": 0.2606,
      "step": 23180
    },
    {
      "epoch": 4.185165132647537,
      "grad_norm": 6.146787643432617,
      "learning_rate": 4.899837574445046e-06,
      "loss": 0.1867,
      "step": 23190
    },
    {
      "epoch": 4.186969861035914,
      "grad_norm": 2.4634575843811035,
      "learning_rate": 4.889009204114781e-06,
      "loss": 0.1594,
      "step": 23200
    },
    {
      "epoch": 4.188774589424292,
      "grad_norm": 37.60564041137695,
      "learning_rate": 4.8781808337845155e-06,
      "loss": 0.1149,
      "step": 23210
    },
    {
      "epoch": 4.190579317812669,
      "grad_norm": 0.37318935990333557,
      "learning_rate": 4.86735246345425e-06,
      "loss": 0.1055,
      "step": 23220
    },
    {
      "epoch": 4.192384046201047,
      "grad_norm": 29.433635711669922,
      "learning_rate": 4.8565240931239845e-06,
      "loss": 0.714,
      "step": 23230
    },
    {
      "epoch": 4.194188774589424,
      "grad_norm": 45.85226821899414,
      "learning_rate": 4.84569572279372e-06,
      "loss": 0.3117,
      "step": 23240
    },
    {
      "epoch": 4.195993502977802,
      "grad_norm": 1.0646125078201294,
      "learning_rate": 4.834867352463454e-06,
      "loss": 0.2109,
      "step": 23250
    },
    {
      "epoch": 4.197798231366179,
      "grad_norm": 32.12308883666992,
      "learning_rate": 4.824038982133189e-06,
      "loss": 0.1688,
      "step": 23260
    },
    {
      "epoch": 4.199602959754557,
      "grad_norm": 0.22872449457645416,
      "learning_rate": 4.813210611802924e-06,
      "loss": 0.114,
      "step": 23270
    },
    {
      "epoch": 4.201407688142934,
      "grad_norm": 1.206719994544983,
      "learning_rate": 4.802382241472658e-06,
      "loss": 0.0183,
      "step": 23280
    },
    {
      "epoch": 4.203212416531312,
      "grad_norm": 0.3224843740463257,
      "learning_rate": 4.791553871142393e-06,
      "loss": 0.0809,
      "step": 23290
    },
    {
      "epoch": 4.205017144919689,
      "grad_norm": 1.4695652723312378,
      "learning_rate": 4.780725500812128e-06,
      "loss": 0.0556,
      "step": 23300
    },
    {
      "epoch": 4.206821873308067,
      "grad_norm": 1.4906120300292969,
      "learning_rate": 4.769897130481862e-06,
      "loss": 0.1859,
      "step": 23310
    },
    {
      "epoch": 4.208626601696444,
      "grad_norm": 11.68044662475586,
      "learning_rate": 4.759068760151597e-06,
      "loss": 0.2496,
      "step": 23320
    },
    {
      "epoch": 4.210431330084822,
      "grad_norm": 51.8348388671875,
      "learning_rate": 4.748240389821332e-06,
      "loss": 0.1425,
      "step": 23330
    },
    {
      "epoch": 4.2122360584731995,
      "grad_norm": 38.770469665527344,
      "learning_rate": 4.737412019491067e-06,
      "loss": 0.355,
      "step": 23340
    },
    {
      "epoch": 4.2140407868615775,
      "grad_norm": 119.32889556884766,
      "learning_rate": 4.726583649160802e-06,
      "loss": 0.1786,
      "step": 23350
    },
    {
      "epoch": 4.2158455152499545,
      "grad_norm": 0.258232057094574,
      "learning_rate": 4.7157552788305366e-06,
      "loss": 0.0985,
      "step": 23360
    },
    {
      "epoch": 4.2176502436383325,
      "grad_norm": 21.868446350097656,
      "learning_rate": 4.704926908500271e-06,
      "loss": 0.4077,
      "step": 23370
    },
    {
      "epoch": 4.21945497202671,
      "grad_norm": 8.810210227966309,
      "learning_rate": 4.6940985381700055e-06,
      "loss": 0.1772,
      "step": 23380
    },
    {
      "epoch": 4.221259700415088,
      "grad_norm": 0.10150264203548431,
      "learning_rate": 4.68327016783974e-06,
      "loss": 0.0959,
      "step": 23390
    },
    {
      "epoch": 4.223064428803465,
      "grad_norm": 12.457454681396484,
      "learning_rate": 4.6724417975094745e-06,
      "loss": 0.2138,
      "step": 23400
    },
    {
      "epoch": 4.224869157191843,
      "grad_norm": 20.81243324279785,
      "learning_rate": 4.661613427179209e-06,
      "loss": 0.2475,
      "step": 23410
    },
    {
      "epoch": 4.226673885580221,
      "grad_norm": 1.4200925827026367,
      "learning_rate": 4.650785056848944e-06,
      "loss": 0.2526,
      "step": 23420
    },
    {
      "epoch": 4.228478613968598,
      "grad_norm": 4.3423285484313965,
      "learning_rate": 4.639956686518679e-06,
      "loss": 0.1765,
      "step": 23430
    },
    {
      "epoch": 4.230283342356976,
      "grad_norm": 0.43781185150146484,
      "learning_rate": 4.629128316188414e-06,
      "loss": 0.4412,
      "step": 23440
    },
    {
      "epoch": 4.232088070745353,
      "grad_norm": 8.953688621520996,
      "learning_rate": 4.618299945858149e-06,
      "loss": 0.2436,
      "step": 23450
    },
    {
      "epoch": 4.233892799133731,
      "grad_norm": 21.41983985900879,
      "learning_rate": 4.607471575527883e-06,
      "loss": 0.2698,
      "step": 23460
    },
    {
      "epoch": 4.235697527522108,
      "grad_norm": 19.784923553466797,
      "learning_rate": 4.596643205197618e-06,
      "loss": 0.1461,
      "step": 23470
    },
    {
      "epoch": 4.237502255910486,
      "grad_norm": 62.242919921875,
      "learning_rate": 4.585814834867353e-06,
      "loss": 0.3253,
      "step": 23480
    },
    {
      "epoch": 4.239306984298863,
      "grad_norm": 11.885141372680664,
      "learning_rate": 4.574986464537087e-06,
      "loss": 0.1976,
      "step": 23490
    },
    {
      "epoch": 4.241111712687241,
      "grad_norm": 70.6827621459961,
      "learning_rate": 4.564158094206822e-06,
      "loss": 0.2839,
      "step": 23500
    },
    {
      "epoch": 4.242916441075618,
      "grad_norm": 3.791534185409546,
      "learning_rate": 4.553329723876557e-06,
      "loss": 0.1294,
      "step": 23510
    },
    {
      "epoch": 4.244721169463996,
      "grad_norm": 91.3245849609375,
      "learning_rate": 4.542501353546291e-06,
      "loss": 0.1204,
      "step": 23520
    },
    {
      "epoch": 4.246525897852373,
      "grad_norm": 3.213703155517578,
      "learning_rate": 4.5316729832160265e-06,
      "loss": 0.1263,
      "step": 23530
    },
    {
      "epoch": 4.248330626240751,
      "grad_norm": 37.131141662597656,
      "learning_rate": 4.5208446128857614e-06,
      "loss": 0.2049,
      "step": 23540
    },
    {
      "epoch": 4.250135354629128,
      "grad_norm": 18.708755493164062,
      "learning_rate": 4.5100162425554955e-06,
      "loss": 0.1942,
      "step": 23550
    },
    {
      "epoch": 4.251940083017506,
      "grad_norm": 10.643889427185059,
      "learning_rate": 4.49918787222523e-06,
      "loss": 0.1439,
      "step": 23560
    },
    {
      "epoch": 4.253744811405883,
      "grad_norm": 29.82956886291504,
      "learning_rate": 4.488359501894965e-06,
      "loss": 0.2592,
      "step": 23570
    },
    {
      "epoch": 4.255549539794261,
      "grad_norm": 0.16835828125476837,
      "learning_rate": 4.477531131564699e-06,
      "loss": 0.1974,
      "step": 23580
    },
    {
      "epoch": 4.257354268182638,
      "grad_norm": 1.6898980140686035,
      "learning_rate": 4.466702761234434e-06,
      "loss": 0.2335,
      "step": 23590
    },
    {
      "epoch": 4.259158996571016,
      "grad_norm": 0.1149059385061264,
      "learning_rate": 4.456957227937196e-06,
      "loss": 0.3951,
      "step": 23600
    },
    {
      "epoch": 4.260963724959393,
      "grad_norm": 50.53097152709961,
      "learning_rate": 4.446128857606931e-06,
      "loss": 0.2677,
      "step": 23610
    },
    {
      "epoch": 4.262768453347771,
      "grad_norm": 36.8356819152832,
      "learning_rate": 4.435300487276665e-06,
      "loss": 0.172,
      "step": 23620
    },
    {
      "epoch": 4.264573181736148,
      "grad_norm": 21.40631866455078,
      "learning_rate": 4.4244721169464e-06,
      "loss": 0.1802,
      "step": 23630
    },
    {
      "epoch": 4.266377910124526,
      "grad_norm": 5.526492595672607,
      "learning_rate": 4.413643746616135e-06,
      "loss": 0.2846,
      "step": 23640
    },
    {
      "epoch": 4.268182638512904,
      "grad_norm": 67.2464828491211,
      "learning_rate": 4.402815376285869e-06,
      "loss": 0.2408,
      "step": 23650
    },
    {
      "epoch": 4.269987366901281,
      "grad_norm": 1.882288932800293,
      "learning_rate": 4.391987005955604e-06,
      "loss": 0.348,
      "step": 23660
    },
    {
      "epoch": 4.271792095289659,
      "grad_norm": 1.3272786140441895,
      "learning_rate": 4.381158635625339e-06,
      "loss": 0.1994,
      "step": 23670
    },
    {
      "epoch": 4.273596823678036,
      "grad_norm": 6.79172420501709,
      "learning_rate": 4.370330265295073e-06,
      "loss": 0.2888,
      "step": 23680
    },
    {
      "epoch": 4.275401552066414,
      "grad_norm": 40.77579879760742,
      "learning_rate": 4.359501894964808e-06,
      "loss": 0.183,
      "step": 23690
    },
    {
      "epoch": 4.2772062804547915,
      "grad_norm": 31.17533302307129,
      "learning_rate": 4.348673524634543e-06,
      "loss": 0.0906,
      "step": 23700
    },
    {
      "epoch": 4.279011008843169,
      "grad_norm": 27.601919174194336,
      "learning_rate": 4.338927991337304e-06,
      "loss": 0.3176,
      "step": 23710
    },
    {
      "epoch": 4.2808157372315465,
      "grad_norm": 83.59614562988281,
      "learning_rate": 4.328099621007038e-06,
      "loss": 0.1946,
      "step": 23720
    },
    {
      "epoch": 4.2826204656199245,
      "grad_norm": 0.6318996548652649,
      "learning_rate": 4.317271250676773e-06,
      "loss": 0.1613,
      "step": 23730
    },
    {
      "epoch": 4.284425194008302,
      "grad_norm": 23.28143882751465,
      "learning_rate": 4.306442880346508e-06,
      "loss": 0.1948,
      "step": 23740
    },
    {
      "epoch": 4.2862299223966795,
      "grad_norm": 23.559720993041992,
      "learning_rate": 4.295614510016242e-06,
      "loss": 0.1924,
      "step": 23750
    },
    {
      "epoch": 4.288034650785057,
      "grad_norm": 11.889816284179688,
      "learning_rate": 4.284786139685977e-06,
      "loss": 0.2189,
      "step": 23760
    },
    {
      "epoch": 4.289839379173435,
      "grad_norm": 11.654132843017578,
      "learning_rate": 4.273957769355713e-06,
      "loss": 0.3148,
      "step": 23770
    },
    {
      "epoch": 4.291644107561812,
      "grad_norm": 49.55593490600586,
      "learning_rate": 4.263129399025447e-06,
      "loss": 0.3395,
      "step": 23780
    },
    {
      "epoch": 4.29344883595019,
      "grad_norm": 5.859428405761719,
      "learning_rate": 4.252301028695182e-06,
      "loss": 0.2642,
      "step": 23790
    },
    {
      "epoch": 4.295253564338567,
      "grad_norm": 11.878701210021973,
      "learning_rate": 4.241472658364917e-06,
      "loss": 0.1488,
      "step": 23800
    },
    {
      "epoch": 4.297058292726945,
      "grad_norm": 26.142118453979492,
      "learning_rate": 4.230644288034651e-06,
      "loss": 0.101,
      "step": 23810
    },
    {
      "epoch": 4.298863021115322,
      "grad_norm": 3.715123414993286,
      "learning_rate": 4.219815917704386e-06,
      "loss": 0.1929,
      "step": 23820
    },
    {
      "epoch": 4.3006677495037,
      "grad_norm": 69.15542602539062,
      "learning_rate": 4.20898754737412e-06,
      "loss": 0.1184,
      "step": 23830
    },
    {
      "epoch": 4.302472477892077,
      "grad_norm": 93.01521301269531,
      "learning_rate": 4.198159177043855e-06,
      "loss": 0.3384,
      "step": 23840
    },
    {
      "epoch": 4.304277206280455,
      "grad_norm": 14.335136413574219,
      "learning_rate": 4.1873308067135895e-06,
      "loss": 0.0989,
      "step": 23850
    },
    {
      "epoch": 4.306081934668832,
      "grad_norm": 5.368194580078125,
      "learning_rate": 4.1765024363833244e-06,
      "loss": 0.1723,
      "step": 23860
    },
    {
      "epoch": 4.30788666305721,
      "grad_norm": 21.860200881958008,
      "learning_rate": 4.165674066053059e-06,
      "loss": 0.0906,
      "step": 23870
    },
    {
      "epoch": 4.309691391445588,
      "grad_norm": 12.207304000854492,
      "learning_rate": 4.154845695722794e-06,
      "loss": 0.3403,
      "step": 23880
    },
    {
      "epoch": 4.311496119833965,
      "grad_norm": 31.51972198486328,
      "learning_rate": 4.144017325392528e-06,
      "loss": 0.1182,
      "step": 23890
    },
    {
      "epoch": 4.313300848222343,
      "grad_norm": 0.33679258823394775,
      "learning_rate": 4.133188955062263e-06,
      "loss": 0.1707,
      "step": 23900
    },
    {
      "epoch": 4.31510557661072,
      "grad_norm": 64.72559356689453,
      "learning_rate": 4.122360584731998e-06,
      "loss": 0.1198,
      "step": 23910
    },
    {
      "epoch": 4.316910304999098,
      "grad_norm": 14.847994804382324,
      "learning_rate": 4.111532214401732e-06,
      "loss": 0.2246,
      "step": 23920
    },
    {
      "epoch": 4.318715033387475,
      "grad_norm": 136.1717071533203,
      "learning_rate": 4.100703844071467e-06,
      "loss": 0.173,
      "step": 23930
    },
    {
      "epoch": 4.320519761775853,
      "grad_norm": 0.24587728083133698,
      "learning_rate": 4.089875473741202e-06,
      "loss": 0.0661,
      "step": 23940
    },
    {
      "epoch": 4.32232449016423,
      "grad_norm": 64.78455352783203,
      "learning_rate": 4.079047103410936e-06,
      "loss": 0.1084,
      "step": 23950
    },
    {
      "epoch": 4.324129218552608,
      "grad_norm": 4.081851482391357,
      "learning_rate": 4.068218733080672e-06,
      "loss": 0.3823,
      "step": 23960
    },
    {
      "epoch": 4.325933946940985,
      "grad_norm": 19.500097274780273,
      "learning_rate": 4.057390362750407e-06,
      "loss": 0.2661,
      "step": 23970
    },
    {
      "epoch": 4.327738675329363,
      "grad_norm": 8.291330337524414,
      "learning_rate": 4.046561992420141e-06,
      "loss": 0.2187,
      "step": 23980
    },
    {
      "epoch": 4.32954340371774,
      "grad_norm": 32.51881408691406,
      "learning_rate": 4.035733622089876e-06,
      "loss": 0.3113,
      "step": 23990
    },
    {
      "epoch": 4.331348132106118,
      "grad_norm": 23.6773738861084,
      "learning_rate": 4.0249052517596106e-06,
      "loss": 0.3426,
      "step": 24000
    },
    {
      "epoch": 4.333152860494495,
      "grad_norm": 49.04310607910156,
      "learning_rate": 4.014076881429345e-06,
      "loss": 0.2345,
      "step": 24010
    },
    {
      "epoch": 4.334957588882873,
      "grad_norm": 23.12110137939453,
      "learning_rate": 4.0032485110990795e-06,
      "loss": 0.112,
      "step": 24020
    },
    {
      "epoch": 4.33676231727125,
      "grad_norm": 66.28602600097656,
      "learning_rate": 3.992420140768814e-06,
      "loss": 0.1647,
      "step": 24030
    },
    {
      "epoch": 4.338567045659628,
      "grad_norm": 54.40726089477539,
      "learning_rate": 3.9815917704385485e-06,
      "loss": 0.2465,
      "step": 24040
    },
    {
      "epoch": 4.3403717740480054,
      "grad_norm": 78.24737548828125,
      "learning_rate": 3.970763400108283e-06,
      "loss": 0.2324,
      "step": 24050
    },
    {
      "epoch": 4.342176502436383,
      "grad_norm": 41.00082015991211,
      "learning_rate": 3.959935029778019e-06,
      "loss": 0.2632,
      "step": 24060
    },
    {
      "epoch": 4.3439812308247605,
      "grad_norm": 1.7536388635635376,
      "learning_rate": 3.949106659447753e-06,
      "loss": 0.243,
      "step": 24070
    },
    {
      "epoch": 4.3457859592131385,
      "grad_norm": 5.73295259475708,
      "learning_rate": 3.938278289117488e-06,
      "loss": 0.3099,
      "step": 24080
    },
    {
      "epoch": 4.347590687601516,
      "grad_norm": 131.39877319335938,
      "learning_rate": 3.927449918787223e-06,
      "loss": 0.3265,
      "step": 24090
    },
    {
      "epoch": 4.3493954159898935,
      "grad_norm": 2.553334951400757,
      "learning_rate": 3.916621548456957e-06,
      "loss": 0.2029,
      "step": 24100
    },
    {
      "epoch": 4.3512001443782715,
      "grad_norm": 87.15090942382812,
      "learning_rate": 3.905793178126692e-06,
      "loss": 0.162,
      "step": 24110
    },
    {
      "epoch": 4.353004872766649,
      "grad_norm": 26.896183013916016,
      "learning_rate": 3.894964807796427e-06,
      "loss": 0.1931,
      "step": 24120
    },
    {
      "epoch": 4.354809601155027,
      "grad_norm": 0.2260739654302597,
      "learning_rate": 3.884136437466161e-06,
      "loss": 0.2859,
      "step": 24130
    },
    {
      "epoch": 4.356614329543404,
      "grad_norm": 9.597719192504883,
      "learning_rate": 3.873308067135896e-06,
      "loss": 0.3201,
      "step": 24140
    },
    {
      "epoch": 4.358419057931782,
      "grad_norm": 55.85237121582031,
      "learning_rate": 3.862479696805631e-06,
      "loss": 0.1432,
      "step": 24150
    },
    {
      "epoch": 4.360223786320159,
      "grad_norm": 5.34642219543457,
      "learning_rate": 3.851651326475366e-06,
      "loss": 0.2988,
      "step": 24160
    },
    {
      "epoch": 4.362028514708537,
      "grad_norm": 85.81707000732422,
      "learning_rate": 3.8408229561451005e-06,
      "loss": 0.1451,
      "step": 24170
    },
    {
      "epoch": 4.363833243096914,
      "grad_norm": 24.953903198242188,
      "learning_rate": 3.8299945858148354e-06,
      "loss": 0.1949,
      "step": 24180
    },
    {
      "epoch": 4.365637971485292,
      "grad_norm": 0.31471171975135803,
      "learning_rate": 3.8191662154845695e-06,
      "loss": 0.0403,
      "step": 24190
    },
    {
      "epoch": 4.367442699873669,
      "grad_norm": 5.336174964904785,
      "learning_rate": 3.8083378451543044e-06,
      "loss": 0.2611,
      "step": 24200
    },
    {
      "epoch": 4.369247428262047,
      "grad_norm": 0.1421099454164505,
      "learning_rate": 3.7975094748240393e-06,
      "loss": 0.249,
      "step": 24210
    },
    {
      "epoch": 4.371052156650424,
      "grad_norm": 64.71163177490234,
      "learning_rate": 3.7866811044937734e-06,
      "loss": 0.1893,
      "step": 24220
    },
    {
      "epoch": 4.372856885038802,
      "grad_norm": 0.9476306438446045,
      "learning_rate": 3.7758527341635087e-06,
      "loss": 0.2881,
      "step": 24230
    },
    {
      "epoch": 4.374661613427179,
      "grad_norm": 36.96947479248047,
      "learning_rate": 3.7650243638332436e-06,
      "loss": 0.2687,
      "step": 24240
    },
    {
      "epoch": 4.376466341815557,
      "grad_norm": 101.78652954101562,
      "learning_rate": 3.7541959935029777e-06,
      "loss": 0.1573,
      "step": 24250
    },
    {
      "epoch": 4.378271070203934,
      "grad_norm": 33.65187454223633,
      "learning_rate": 3.7433676231727126e-06,
      "loss": 0.2931,
      "step": 24260
    },
    {
      "epoch": 4.380075798592312,
      "grad_norm": 6.665788173675537,
      "learning_rate": 3.732539252842447e-06,
      "loss": 0.2733,
      "step": 24270
    },
    {
      "epoch": 4.381880526980689,
      "grad_norm": 19.969953536987305,
      "learning_rate": 3.7217108825121824e-06,
      "loss": 0.1518,
      "step": 24280
    },
    {
      "epoch": 4.383685255369067,
      "grad_norm": 9.050814628601074,
      "learning_rate": 3.710882512181917e-06,
      "loss": 0.1513,
      "step": 24290
    },
    {
      "epoch": 4.385489983757444,
      "grad_norm": 12.528481483459473,
      "learning_rate": 3.7000541418516513e-06,
      "loss": 0.1365,
      "step": 24300
    },
    {
      "epoch": 4.387294712145822,
      "grad_norm": 37.0296630859375,
      "learning_rate": 3.6892257715213862e-06,
      "loss": 0.1332,
      "step": 24310
    },
    {
      "epoch": 4.389099440534199,
      "grad_norm": 26.98195457458496,
      "learning_rate": 3.6783974011911207e-06,
      "loss": 0.2362,
      "step": 24320
    },
    {
      "epoch": 4.390904168922577,
      "grad_norm": 51.99264144897461,
      "learning_rate": 3.6675690308608556e-06,
      "loss": 0.2688,
      "step": 24330
    },
    {
      "epoch": 4.392708897310955,
      "grad_norm": 2.5402114391326904,
      "learning_rate": 3.6567406605305905e-06,
      "loss": 0.1996,
      "step": 24340
    },
    {
      "epoch": 4.394513625699332,
      "grad_norm": 6.979500770568848,
      "learning_rate": 3.645912290200325e-06,
      "loss": 0.0702,
      "step": 24350
    },
    {
      "epoch": 4.39631835408771,
      "grad_norm": 8.725935935974121,
      "learning_rate": 3.6350839198700595e-06,
      "loss": 0.3258,
      "step": 24360
    },
    {
      "epoch": 4.398123082476087,
      "grad_norm": 5.701878547668457,
      "learning_rate": 3.624255549539794e-06,
      "loss": 0.1791,
      "step": 24370
    },
    {
      "epoch": 4.399927810864465,
      "grad_norm": 18.714523315429688,
      "learning_rate": 3.6134271792095293e-06,
      "loss": 0.2273,
      "step": 24380
    },
    {
      "epoch": 4.401732539252842,
      "grad_norm": 21.618051528930664,
      "learning_rate": 3.6025988088792638e-06,
      "loss": 0.1689,
      "step": 24390
    },
    {
      "epoch": 4.40353726764122,
      "grad_norm": 0.9141040444374084,
      "learning_rate": 3.5917704385489982e-06,
      "loss": 0.2014,
      "step": 24400
    },
    {
      "epoch": 4.405341996029597,
      "grad_norm": 4.191949367523193,
      "learning_rate": 3.580942068218733e-06,
      "loss": 0.2608,
      "step": 24410
    },
    {
      "epoch": 4.407146724417975,
      "grad_norm": 31.69906997680664,
      "learning_rate": 3.5701136978884676e-06,
      "loss": 0.405,
      "step": 24420
    },
    {
      "epoch": 4.4089514528063525,
      "grad_norm": 3.62660813331604,
      "learning_rate": 3.5592853275582025e-06,
      "loss": 0.1681,
      "step": 24430
    },
    {
      "epoch": 4.4107561811947305,
      "grad_norm": 21.646230697631836,
      "learning_rate": 3.5484569572279374e-06,
      "loss": 0.1164,
      "step": 24440
    },
    {
      "epoch": 4.4125609095831075,
      "grad_norm": 90.11127471923828,
      "learning_rate": 3.537628586897672e-06,
      "loss": 0.2789,
      "step": 24450
    },
    {
      "epoch": 4.4143656379714855,
      "grad_norm": 3.465796947479248,
      "learning_rate": 3.5268002165674064e-06,
      "loss": 0.1272,
      "step": 24460
    },
    {
      "epoch": 4.416170366359863,
      "grad_norm": 9.738372802734375,
      "learning_rate": 3.5159718462371417e-06,
      "loss": 0.0944,
      "step": 24470
    },
    {
      "epoch": 4.417975094748241,
      "grad_norm": 27.46832847595215,
      "learning_rate": 3.505143475906876e-06,
      "loss": 0.5045,
      "step": 24480
    },
    {
      "epoch": 4.419779823136618,
      "grad_norm": 58.48858642578125,
      "learning_rate": 3.4943151055766107e-06,
      "loss": 0.252,
      "step": 24490
    },
    {
      "epoch": 4.421584551524996,
      "grad_norm": 16.112415313720703,
      "learning_rate": 3.4834867352463456e-06,
      "loss": 0.1777,
      "step": 24500
    },
    {
      "epoch": 4.423389279913373,
      "grad_norm": 10.568315505981445,
      "learning_rate": 3.47265836491608e-06,
      "loss": 0.1999,
      "step": 24510
    },
    {
      "epoch": 4.425194008301751,
      "grad_norm": 0.5980237126350403,
      "learning_rate": 3.461829994585815e-06,
      "loss": 0.1536,
      "step": 24520
    },
    {
      "epoch": 4.426998736690128,
      "grad_norm": 0.3489263355731964,
      "learning_rate": 3.45100162425555e-06,
      "loss": 0.0538,
      "step": 24530
    },
    {
      "epoch": 4.428803465078506,
      "grad_norm": 11.056754112243652,
      "learning_rate": 3.4401732539252844e-06,
      "loss": 0.2708,
      "step": 24540
    },
    {
      "epoch": 4.430608193466883,
      "grad_norm": 43.22303009033203,
      "learning_rate": 3.429344883595019e-06,
      "loss": 0.1311,
      "step": 24550
    },
    {
      "epoch": 4.432412921855261,
      "grad_norm": 36.59505081176758,
      "learning_rate": 3.4185165132647537e-06,
      "loss": 0.2065,
      "step": 24560
    },
    {
      "epoch": 4.434217650243639,
      "grad_norm": 6.419366359710693,
      "learning_rate": 3.4076881429344886e-06,
      "loss": 0.1405,
      "step": 24570
    },
    {
      "epoch": 4.436022378632016,
      "grad_norm": 21.716272354125977,
      "learning_rate": 3.396859772604223e-06,
      "loss": 0.275,
      "step": 24580
    },
    {
      "epoch": 4.437827107020394,
      "grad_norm": 52.2505989074707,
      "learning_rate": 3.386031402273958e-06,
      "loss": 0.0935,
      "step": 24590
    },
    {
      "epoch": 4.439631835408771,
      "grad_norm": 17.779850006103516,
      "learning_rate": 3.3752030319436925e-06,
      "loss": 0.1416,
      "step": 24600
    },
    {
      "epoch": 4.441436563797149,
      "grad_norm": 69.04027557373047,
      "learning_rate": 3.364374661613427e-06,
      "loss": 0.3114,
      "step": 24610
    },
    {
      "epoch": 4.443241292185526,
      "grad_norm": 0.43129095435142517,
      "learning_rate": 3.3535462912831623e-06,
      "loss": 0.2545,
      "step": 24620
    },
    {
      "epoch": 4.445046020573904,
      "grad_norm": 0.269290566444397,
      "learning_rate": 3.342717920952897e-06,
      "loss": 0.1909,
      "step": 24630
    },
    {
      "epoch": 4.446850748962281,
      "grad_norm": 4.076455593109131,
      "learning_rate": 3.3318895506226313e-06,
      "loss": 0.211,
      "step": 24640
    },
    {
      "epoch": 4.448655477350659,
      "grad_norm": 26.580846786499023,
      "learning_rate": 3.321061180292366e-06,
      "loss": 0.2846,
      "step": 24650
    },
    {
      "epoch": 4.450460205739036,
      "grad_norm": 18.591947555541992,
      "learning_rate": 3.3102328099621007e-06,
      "loss": 0.2175,
      "step": 24660
    },
    {
      "epoch": 4.452264934127414,
      "grad_norm": 16.346866607666016,
      "learning_rate": 3.2994044396318356e-06,
      "loss": 0.3112,
      "step": 24670
    },
    {
      "epoch": 4.454069662515791,
      "grad_norm": 23.804019927978516,
      "learning_rate": 3.2885760693015705e-06,
      "loss": 0.1779,
      "step": 24680
    },
    {
      "epoch": 4.455874390904169,
      "grad_norm": 25.52959442138672,
      "learning_rate": 3.277747698971305e-06,
      "loss": 0.294,
      "step": 24690
    },
    {
      "epoch": 4.457679119292546,
      "grad_norm": 23.85869026184082,
      "learning_rate": 3.2669193286410394e-06,
      "loss": 0.3956,
      "step": 24700
    },
    {
      "epoch": 4.459483847680924,
      "grad_norm": 44.06748580932617,
      "learning_rate": 3.256090958310774e-06,
      "loss": 0.2992,
      "step": 24710
    },
    {
      "epoch": 4.461288576069301,
      "grad_norm": 39.42125701904297,
      "learning_rate": 3.2452625879805092e-06,
      "loss": 0.22,
      "step": 24720
    },
    {
      "epoch": 4.463093304457679,
      "grad_norm": 50.30181884765625,
      "learning_rate": 3.2344342176502437e-06,
      "loss": 0.1928,
      "step": 24730
    },
    {
      "epoch": 4.464898032846056,
      "grad_norm": 98.28559875488281,
      "learning_rate": 3.223605847319978e-06,
      "loss": 0.1117,
      "step": 24740
    },
    {
      "epoch": 4.466702761234434,
      "grad_norm": 78.33984375,
      "learning_rate": 3.212777476989713e-06,
      "loss": 0.2326,
      "step": 24750
    },
    {
      "epoch": 4.468507489622811,
      "grad_norm": 0.15700632333755493,
      "learning_rate": 3.2019491066594476e-06,
      "loss": 0.4224,
      "step": 24760
    },
    {
      "epoch": 4.470312218011189,
      "grad_norm": 16.517168045043945,
      "learning_rate": 3.1911207363291825e-06,
      "loss": 0.1362,
      "step": 24770
    },
    {
      "epoch": 4.4721169463995665,
      "grad_norm": 78.29984283447266,
      "learning_rate": 3.1802923659989174e-06,
      "loss": 0.1778,
      "step": 24780
    },
    {
      "epoch": 4.4739216747879444,
      "grad_norm": 0.5492737293243408,
      "learning_rate": 3.169463995668652e-06,
      "loss": 0.1646,
      "step": 24790
    },
    {
      "epoch": 4.475726403176322,
      "grad_norm": 47.19211959838867,
      "learning_rate": 3.1586356253383864e-06,
      "loss": 0.2261,
      "step": 24800
    },
    {
      "epoch": 4.4775311315646995,
      "grad_norm": 3.936302661895752,
      "learning_rate": 3.1478072550081213e-06,
      "loss": 0.2066,
      "step": 24810
    },
    {
      "epoch": 4.479335859953077,
      "grad_norm": 20.8284912109375,
      "learning_rate": 3.136978884677856e-06,
      "loss": 0.1875,
      "step": 24820
    },
    {
      "epoch": 4.481140588341455,
      "grad_norm": 4.937849998474121,
      "learning_rate": 3.1261505143475906e-06,
      "loss": 0.2002,
      "step": 24830
    },
    {
      "epoch": 4.4829453167298325,
      "grad_norm": 3.8004846572875977,
      "learning_rate": 3.1153221440173256e-06,
      "loss": 0.1631,
      "step": 24840
    },
    {
      "epoch": 4.48475004511821,
      "grad_norm": 60.72947692871094,
      "learning_rate": 3.10449377368706e-06,
      "loss": 0.2897,
      "step": 24850
    },
    {
      "epoch": 4.486554773506588,
      "grad_norm": 0.13287167251110077,
      "learning_rate": 3.093665403356795e-06,
      "loss": 0.221,
      "step": 24860
    },
    {
      "epoch": 4.488359501894965,
      "grad_norm": 11.001628875732422,
      "learning_rate": 3.08283703302653e-06,
      "loss": 0.1555,
      "step": 24870
    },
    {
      "epoch": 4.490164230283343,
      "grad_norm": 71.70939636230469,
      "learning_rate": 3.0720086626962643e-06,
      "loss": 0.3319,
      "step": 24880
    },
    {
      "epoch": 4.49196895867172,
      "grad_norm": 38.44944763183594,
      "learning_rate": 3.061180292365999e-06,
      "loss": 0.2426,
      "step": 24890
    },
    {
      "epoch": 4.493773687060098,
      "grad_norm": 7.125741958618164,
      "learning_rate": 3.0503519220357337e-06,
      "loss": 0.1353,
      "step": 24900
    },
    {
      "epoch": 4.495578415448475,
      "grad_norm": 79.53872680664062,
      "learning_rate": 3.0395235517054686e-06,
      "loss": 0.2179,
      "step": 24910
    },
    {
      "epoch": 4.497383143836853,
      "grad_norm": 6.91980504989624,
      "learning_rate": 3.028695181375203e-06,
      "loss": 0.2214,
      "step": 24920
    },
    {
      "epoch": 4.49918787222523,
      "grad_norm": 15.906259536743164,
      "learning_rate": 3.017866811044938e-06,
      "loss": 0.2992,
      "step": 24930
    },
    {
      "epoch": 4.500992600613608,
      "grad_norm": 47.134986877441406,
      "learning_rate": 3.0070384407146725e-06,
      "loss": 0.2535,
      "step": 24940
    },
    {
      "epoch": 4.502797329001985,
      "grad_norm": 0.2659333050251007,
      "learning_rate": 2.996210070384407e-06,
      "loss": 0.2534,
      "step": 24950
    },
    {
      "epoch": 4.504602057390363,
      "grad_norm": 8.622170448303223,
      "learning_rate": 2.9853817000541423e-06,
      "loss": 0.1799,
      "step": 24960
    },
    {
      "epoch": 4.50640678577874,
      "grad_norm": 40.812339782714844,
      "learning_rate": 2.9745533297238768e-06,
      "loss": 0.3515,
      "step": 24970
    },
    {
      "epoch": 4.508211514167118,
      "grad_norm": 49.80379867553711,
      "learning_rate": 2.9637249593936112e-06,
      "loss": 0.2762,
      "step": 24980
    },
    {
      "epoch": 4.510016242555495,
      "grad_norm": 27.252424240112305,
      "learning_rate": 2.952896589063346e-06,
      "loss": 0.2531,
      "step": 24990
    },
    {
      "epoch": 4.511820970943873,
      "grad_norm": 27.43296241760254,
      "learning_rate": 2.9420682187330806e-06,
      "loss": 0.2089,
      "step": 25000
    },
    {
      "epoch": 4.51362569933225,
      "grad_norm": 9.128632545471191,
      "learning_rate": 2.9312398484028155e-06,
      "loss": 0.2626,
      "step": 25010
    },
    {
      "epoch": 4.515430427720628,
      "grad_norm": 17.45279884338379,
      "learning_rate": 2.9204114780725504e-06,
      "loss": 0.1585,
      "step": 25020
    },
    {
      "epoch": 4.517235156109006,
      "grad_norm": 8.665582656860352,
      "learning_rate": 2.909583107742285e-06,
      "loss": 0.0951,
      "step": 25030
    },
    {
      "epoch": 4.519039884497383,
      "grad_norm": 15.807342529296875,
      "learning_rate": 2.8987547374120194e-06,
      "loss": 0.0636,
      "step": 25040
    },
    {
      "epoch": 4.52084461288576,
      "grad_norm": 5.48674201965332,
      "learning_rate": 2.8879263670817543e-06,
      "loss": 0.2218,
      "step": 25050
    },
    {
      "epoch": 4.522649341274138,
      "grad_norm": 50.058837890625,
      "learning_rate": 2.877097996751489e-06,
      "loss": 0.2251,
      "step": 25060
    },
    {
      "epoch": 4.524454069662516,
      "grad_norm": 0.09770600497722626,
      "learning_rate": 2.8662696264212237e-06,
      "loss": 0.2259,
      "step": 25070
    },
    {
      "epoch": 4.526258798050893,
      "grad_norm": 55.332801818847656,
      "learning_rate": 2.855441256090958e-06,
      "loss": 0.0873,
      "step": 25080
    },
    {
      "epoch": 4.528063526439271,
      "grad_norm": 32.217098236083984,
      "learning_rate": 2.844612885760693e-06,
      "loss": 0.1937,
      "step": 25090
    },
    {
      "epoch": 4.529868254827648,
      "grad_norm": 48.37852478027344,
      "learning_rate": 2.8337845154304275e-06,
      "loss": 0.2908,
      "step": 25100
    },
    {
      "epoch": 4.531672983216026,
      "grad_norm": 24.68082046508789,
      "learning_rate": 2.8229561451001625e-06,
      "loss": 0.3601,
      "step": 25110
    },
    {
      "epoch": 4.533477711604403,
      "grad_norm": 3.476224184036255,
      "learning_rate": 2.8121277747698974e-06,
      "loss": 0.2441,
      "step": 25120
    },
    {
      "epoch": 4.535282439992781,
      "grad_norm": 0.3442022502422333,
      "learning_rate": 2.801299404439632e-06,
      "loss": 0.2787,
      "step": 25130
    },
    {
      "epoch": 4.537087168381158,
      "grad_norm": 6.438998222351074,
      "learning_rate": 2.7904710341093663e-06,
      "loss": 0.2457,
      "step": 25140
    },
    {
      "epoch": 4.538891896769536,
      "grad_norm": 1.1964598894119263,
      "learning_rate": 2.7796426637791012e-06,
      "loss": 0.0646,
      "step": 25150
    },
    {
      "epoch": 4.5406966251579135,
      "grad_norm": 12.647242546081543,
      "learning_rate": 2.768814293448836e-06,
      "loss": 0.1153,
      "step": 25160
    },
    {
      "epoch": 4.5425013535462915,
      "grad_norm": 9.45180892944336,
      "learning_rate": 2.7579859231185706e-06,
      "loss": 0.1108,
      "step": 25170
    },
    {
      "epoch": 4.544306081934669,
      "grad_norm": 2.076389789581299,
      "learning_rate": 2.7471575527883055e-06,
      "loss": 0.0973,
      "step": 25180
    },
    {
      "epoch": 4.5461108103230465,
      "grad_norm": 14.903213500976562,
      "learning_rate": 2.73632918245804e-06,
      "loss": 0.3292,
      "step": 25190
    },
    {
      "epoch": 4.547915538711424,
      "grad_norm": 20.53279685974121,
      "learning_rate": 2.725500812127775e-06,
      "loss": 0.2371,
      "step": 25200
    },
    {
      "epoch": 4.549720267099802,
      "grad_norm": 21.508262634277344,
      "learning_rate": 2.71467244179751e-06,
      "loss": 0.3089,
      "step": 25210
    },
    {
      "epoch": 4.551524995488179,
      "grad_norm": 39.92063522338867,
      "learning_rate": 2.7038440714672443e-06,
      "loss": 0.2682,
      "step": 25220
    },
    {
      "epoch": 4.553329723876557,
      "grad_norm": 43.3856315612793,
      "learning_rate": 2.6930157011369788e-06,
      "loss": 0.1149,
      "step": 25230
    },
    {
      "epoch": 4.555134452264934,
      "grad_norm": 1.4415652751922607,
      "learning_rate": 2.6821873308067137e-06,
      "loss": 0.069,
      "step": 25240
    },
    {
      "epoch": 4.556939180653312,
      "grad_norm": 71.12503814697266,
      "learning_rate": 2.6713589604764486e-06,
      "loss": 0.1433,
      "step": 25250
    },
    {
      "epoch": 4.55874390904169,
      "grad_norm": 82.08731079101562,
      "learning_rate": 2.660530590146183e-06,
      "loss": 0.2834,
      "step": 25260
    },
    {
      "epoch": 4.560548637430067,
      "grad_norm": 0.79168301820755,
      "learning_rate": 2.649702219815918e-06,
      "loss": 0.0707,
      "step": 25270
    },
    {
      "epoch": 4.562353365818444,
      "grad_norm": 0.5189457535743713,
      "learning_rate": 2.6388738494856524e-06,
      "loss": 0.1669,
      "step": 25280
    },
    {
      "epoch": 4.564158094206822,
      "grad_norm": 50.18433380126953,
      "learning_rate": 2.628045479155387e-06,
      "loss": 0.3297,
      "step": 25290
    },
    {
      "epoch": 4.5659628225952,
      "grad_norm": 28.785419464111328,
      "learning_rate": 2.6172171088251222e-06,
      "loss": 0.2649,
      "step": 25300
    },
    {
      "epoch": 4.567767550983577,
      "grad_norm": 108.83392333984375,
      "learning_rate": 2.6063887384948567e-06,
      "loss": 0.2269,
      "step": 25310
    },
    {
      "epoch": 4.569572279371955,
      "grad_norm": 5.867471218109131,
      "learning_rate": 2.595560368164591e-06,
      "loss": 0.1848,
      "step": 25320
    },
    {
      "epoch": 4.571377007760332,
      "grad_norm": 4.481910228729248,
      "learning_rate": 2.584731997834326e-06,
      "loss": 0.1695,
      "step": 25330
    },
    {
      "epoch": 4.57318173614871,
      "grad_norm": 4.597375869750977,
      "learning_rate": 2.5739036275040606e-06,
      "loss": 0.2316,
      "step": 25340
    },
    {
      "epoch": 4.574986464537087,
      "grad_norm": 0.27497437596321106,
      "learning_rate": 2.5630752571737955e-06,
      "loss": 0.2448,
      "step": 25350
    },
    {
      "epoch": 4.576791192925465,
      "grad_norm": 5.786906719207764,
      "learning_rate": 2.5522468868435304e-06,
      "loss": 0.1799,
      "step": 25360
    },
    {
      "epoch": 4.578595921313842,
      "grad_norm": 2.4384853839874268,
      "learning_rate": 2.541418516513265e-06,
      "loss": 0.0964,
      "step": 25370
    },
    {
      "epoch": 4.58040064970222,
      "grad_norm": 37.367218017578125,
      "learning_rate": 2.5305901461829994e-06,
      "loss": 0.2531,
      "step": 25380
    },
    {
      "epoch": 4.582205378090597,
      "grad_norm": 1.467490553855896,
      "learning_rate": 2.5197617758527343e-06,
      "loss": 0.2517,
      "step": 25390
    },
    {
      "epoch": 4.584010106478975,
      "grad_norm": 80.4374008178711,
      "learning_rate": 2.508933405522469e-06,
      "loss": 0.222,
      "step": 25400
    },
    {
      "epoch": 4.585814834867352,
      "grad_norm": 12.917314529418945,
      "learning_rate": 2.4981050351922036e-06,
      "loss": 0.2544,
      "step": 25410
    },
    {
      "epoch": 4.58761956325573,
      "grad_norm": 1.1723308563232422,
      "learning_rate": 2.487276664861938e-06,
      "loss": 0.1954,
      "step": 25420
    },
    {
      "epoch": 4.589424291644107,
      "grad_norm": 55.61226272583008,
      "learning_rate": 2.476448294531673e-06,
      "loss": 0.1682,
      "step": 25430
    },
    {
      "epoch": 4.591229020032485,
      "grad_norm": 5.978397369384766,
      "learning_rate": 2.4656199242014075e-06,
      "loss": 0.2797,
      "step": 25440
    },
    {
      "epoch": 4.593033748420862,
      "grad_norm": 43.19834518432617,
      "learning_rate": 2.4547915538711424e-06,
      "loss": 0.1276,
      "step": 25450
    },
    {
      "epoch": 4.59483847680924,
      "grad_norm": 0.4847298860549927,
      "learning_rate": 2.4439631835408773e-06,
      "loss": 0.2176,
      "step": 25460
    },
    {
      "epoch": 4.596643205197617,
      "grad_norm": 47.8939094543457,
      "learning_rate": 2.433134813210612e-06,
      "loss": 0.1849,
      "step": 25470
    },
    {
      "epoch": 4.598447933585995,
      "grad_norm": 5.224641799926758,
      "learning_rate": 2.4223064428803463e-06,
      "loss": 0.293,
      "step": 25480
    },
    {
      "epoch": 4.600252661974373,
      "grad_norm": 105.87059020996094,
      "learning_rate": 2.411478072550081e-06,
      "loss": 0.1549,
      "step": 25490
    },
    {
      "epoch": 4.60205739036275,
      "grad_norm": 11.201373100280762,
      "learning_rate": 2.400649702219816e-06,
      "loss": 0.3673,
      "step": 25500
    },
    {
      "epoch": 4.6038621187511275,
      "grad_norm": 3.111862897872925,
      "learning_rate": 2.3898213318895506e-06,
      "loss": 0.1168,
      "step": 25510
    },
    {
      "epoch": 4.6056668471395055,
      "grad_norm": 15.635236740112305,
      "learning_rate": 2.3789929615592855e-06,
      "loss": 0.1425,
      "step": 25520
    },
    {
      "epoch": 4.6074715755278834,
      "grad_norm": 156.9169921875,
      "learning_rate": 2.36816459122902e-06,
      "loss": 0.3369,
      "step": 25530
    },
    {
      "epoch": 4.6092763039162605,
      "grad_norm": 2.8886168003082275,
      "learning_rate": 2.3573362208987544e-06,
      "loss": 0.2235,
      "step": 25540
    },
    {
      "epoch": 4.6110810323046385,
      "grad_norm": 83.27745819091797,
      "learning_rate": 2.3465078505684898e-06,
      "loss": 0.2763,
      "step": 25550
    },
    {
      "epoch": 4.612885760693016,
      "grad_norm": 45.26991271972656,
      "learning_rate": 2.3356794802382242e-06,
      "loss": 0.3021,
      "step": 25560
    },
    {
      "epoch": 4.614690489081394,
      "grad_norm": 0.6179746389389038,
      "learning_rate": 2.3248511099079587e-06,
      "loss": 0.1506,
      "step": 25570
    },
    {
      "epoch": 4.616495217469771,
      "grad_norm": 16.530305862426758,
      "learning_rate": 2.3140227395776936e-06,
      "loss": 0.2293,
      "step": 25580
    },
    {
      "epoch": 4.618299945858149,
      "grad_norm": 21.357168197631836,
      "learning_rate": 2.3031943692474285e-06,
      "loss": 0.1642,
      "step": 25590
    },
    {
      "epoch": 4.620104674246526,
      "grad_norm": 44.9887580871582,
      "learning_rate": 2.292365998917163e-06,
      "loss": 0.18,
      "step": 25600
    },
    {
      "epoch": 4.621909402634904,
      "grad_norm": 15.800149917602539,
      "learning_rate": 2.281537628586898e-06,
      "loss": 0.1426,
      "step": 25610
    },
    {
      "epoch": 4.623714131023281,
      "grad_norm": 8.993639945983887,
      "learning_rate": 2.2707092582566324e-06,
      "loss": 0.2656,
      "step": 25620
    },
    {
      "epoch": 4.625518859411659,
      "grad_norm": 61.846195220947266,
      "learning_rate": 2.259880887926367e-06,
      "loss": 0.2076,
      "step": 25630
    },
    {
      "epoch": 4.627323587800036,
      "grad_norm": 82.12850952148438,
      "learning_rate": 2.249052517596102e-06,
      "loss": 0.3284,
      "step": 25640
    },
    {
      "epoch": 4.629128316188414,
      "grad_norm": 28.79558563232422,
      "learning_rate": 2.2382241472658367e-06,
      "loss": 0.1746,
      "step": 25650
    },
    {
      "epoch": 4.630933044576791,
      "grad_norm": 56.0966796875,
      "learning_rate": 2.227395776935571e-06,
      "loss": 0.1095,
      "step": 25660
    },
    {
      "epoch": 4.632737772965169,
      "grad_norm": 26.8718204498291,
      "learning_rate": 2.216567406605306e-06,
      "loss": 0.201,
      "step": 25670
    },
    {
      "epoch": 4.634542501353546,
      "grad_norm": 0.9364940524101257,
      "learning_rate": 2.2057390362750405e-06,
      "loss": 0.2652,
      "step": 25680
    },
    {
      "epoch": 4.636347229741924,
      "grad_norm": 65.52814483642578,
      "learning_rate": 2.1949106659447754e-06,
      "loss": 0.4595,
      "step": 25690
    },
    {
      "epoch": 4.638151958130301,
      "grad_norm": 36.26509475708008,
      "learning_rate": 2.1840822956145104e-06,
      "loss": 0.2122,
      "step": 25700
    },
    {
      "epoch": 4.639956686518679,
      "grad_norm": 5.870367527008057,
      "learning_rate": 2.173253925284245e-06,
      "loss": 0.1763,
      "step": 25710
    },
    {
      "epoch": 4.641761414907057,
      "grad_norm": 0.21172769367694855,
      "learning_rate": 2.1624255549539793e-06,
      "loss": 0.2664,
      "step": 25720
    },
    {
      "epoch": 4.643566143295434,
      "grad_norm": 29.7778263092041,
      "learning_rate": 2.1515971846237142e-06,
      "loss": 0.3197,
      "step": 25730
    },
    {
      "epoch": 4.645370871683811,
      "grad_norm": 34.059776306152344,
      "learning_rate": 2.140768814293449e-06,
      "loss": 0.3471,
      "step": 25740
    },
    {
      "epoch": 4.647175600072189,
      "grad_norm": 3.0247180461883545,
      "learning_rate": 2.1299404439631836e-06,
      "loss": 0.1217,
      "step": 25750
    },
    {
      "epoch": 4.648980328460567,
      "grad_norm": 28.72260284423828,
      "learning_rate": 2.1191120736329185e-06,
      "loss": 0.192,
      "step": 25760
    },
    {
      "epoch": 4.650785056848944,
      "grad_norm": 2.221883773803711,
      "learning_rate": 2.108283703302653e-06,
      "loss": 0.0877,
      "step": 25770
    },
    {
      "epoch": 4.652589785237322,
      "grad_norm": 5.804189205169678,
      "learning_rate": 2.0974553329723875e-06,
      "loss": 0.0354,
      "step": 25780
    },
    {
      "epoch": 4.654394513625699,
      "grad_norm": 57.25080108642578,
      "learning_rate": 2.0866269626421224e-06,
      "loss": 0.217,
      "step": 25790
    },
    {
      "epoch": 4.656199242014077,
      "grad_norm": 99.5871810913086,
      "learning_rate": 2.0757985923118573e-06,
      "loss": 0.2104,
      "step": 25800
    },
    {
      "epoch": 4.658003970402454,
      "grad_norm": 26.75724220275879,
      "learning_rate": 2.0649702219815918e-06,
      "loss": 0.2449,
      "step": 25810
    },
    {
      "epoch": 4.659808698790832,
      "grad_norm": 31.88667869567871,
      "learning_rate": 2.0541418516513262e-06,
      "loss": 0.1108,
      "step": 25820
    },
    {
      "epoch": 4.661613427179209,
      "grad_norm": 6.7252373695373535,
      "learning_rate": 2.043313481321061e-06,
      "loss": 0.3746,
      "step": 25830
    },
    {
      "epoch": 4.663418155567587,
      "grad_norm": 12.055206298828125,
      "learning_rate": 2.032485110990796e-06,
      "loss": 0.199,
      "step": 25840
    },
    {
      "epoch": 4.665222883955964,
      "grad_norm": 2.1053638458251953,
      "learning_rate": 2.0216567406605305e-06,
      "loss": 0.2921,
      "step": 25850
    },
    {
      "epoch": 4.667027612344342,
      "grad_norm": 98.89359283447266,
      "learning_rate": 2.0108283703302654e-06,
      "loss": 0.1335,
      "step": 25860
    },
    {
      "epoch": 4.6688323407327195,
      "grad_norm": 85.12020874023438,
      "learning_rate": 2e-06,
      "loss": 0.3535,
      "step": 25870
    },
    {
      "epoch": 4.670637069121097,
      "grad_norm": 62.21826171875,
      "learning_rate": 1.9891716296697344e-06,
      "loss": 0.2741,
      "step": 25880
    },
    {
      "epoch": 4.6724417975094745,
      "grad_norm": 23.689924240112305,
      "learning_rate": 1.9783432593394697e-06,
      "loss": 0.1602,
      "step": 25890
    },
    {
      "epoch": 4.6742465258978525,
      "grad_norm": 25.15717887878418,
      "learning_rate": 1.967514889009204e-06,
      "loss": 0.2006,
      "step": 25900
    },
    {
      "epoch": 4.67605125428623,
      "grad_norm": 56.548702239990234,
      "learning_rate": 1.9566865186789387e-06,
      "loss": 0.4464,
      "step": 25910
    },
    {
      "epoch": 4.677855982674608,
      "grad_norm": 13.114908218383789,
      "learning_rate": 1.9458581483486736e-06,
      "loss": 0.2369,
      "step": 25920
    },
    {
      "epoch": 4.679660711062985,
      "grad_norm": 34.811676025390625,
      "learning_rate": 1.935029778018408e-06,
      "loss": 0.3545,
      "step": 25930
    },
    {
      "epoch": 4.681465439451363,
      "grad_norm": 0.6585181951522827,
      "learning_rate": 1.924201407688143e-06,
      "loss": 0.1624,
      "step": 25940
    },
    {
      "epoch": 4.683270167839741,
      "grad_norm": 0.4033952057361603,
      "learning_rate": 1.913373037357878e-06,
      "loss": 0.2382,
      "step": 25950
    },
    {
      "epoch": 4.685074896228118,
      "grad_norm": 1.2471213340759277,
      "learning_rate": 1.9025446670276124e-06,
      "loss": 0.2682,
      "step": 25960
    },
    {
      "epoch": 4.686879624616495,
      "grad_norm": 7.2515482902526855,
      "learning_rate": 1.891716296697347e-06,
      "loss": 0.2814,
      "step": 25970
    },
    {
      "epoch": 4.688684353004873,
      "grad_norm": 8.090352058410645,
      "learning_rate": 1.880887926367082e-06,
      "loss": 0.0725,
      "step": 25980
    },
    {
      "epoch": 4.690489081393251,
      "grad_norm": 19.352474212646484,
      "learning_rate": 1.8700595560368164e-06,
      "loss": 0.258,
      "step": 25990
    },
    {
      "epoch": 4.692293809781628,
      "grad_norm": 24.668094635009766,
      "learning_rate": 1.8592311857065513e-06,
      "loss": 0.2098,
      "step": 26000
    },
    {
      "epoch": 4.694098538170006,
      "grad_norm": 11.172764778137207,
      "learning_rate": 1.8484028153762858e-06,
      "loss": 0.1189,
      "step": 26010
    },
    {
      "epoch": 4.695903266558383,
      "grad_norm": 28.8054256439209,
      "learning_rate": 1.8375744450460207e-06,
      "loss": 0.3727,
      "step": 26020
    },
    {
      "epoch": 4.697707994946761,
      "grad_norm": 3.5910754203796387,
      "learning_rate": 1.8267460747157552e-06,
      "loss": 0.2346,
      "step": 26030
    },
    {
      "epoch": 4.699512723335138,
      "grad_norm": 7.6494140625,
      "learning_rate": 1.81591770438549e-06,
      "loss": 0.0906,
      "step": 26040
    },
    {
      "epoch": 4.701317451723516,
      "grad_norm": 1.0817701816558838,
      "learning_rate": 1.8050893340552248e-06,
      "loss": 0.0473,
      "step": 26050
    },
    {
      "epoch": 4.703122180111893,
      "grad_norm": 0.555985152721405,
      "learning_rate": 1.7942609637249593e-06,
      "loss": 0.1797,
      "step": 26060
    },
    {
      "epoch": 4.704926908500271,
      "grad_norm": 57.29962158203125,
      "learning_rate": 1.7834325933946942e-06,
      "loss": 0.3056,
      "step": 26070
    },
    {
      "epoch": 4.706731636888648,
      "grad_norm": 12.460505485534668,
      "learning_rate": 1.7726042230644289e-06,
      "loss": 0.2631,
      "step": 26080
    },
    {
      "epoch": 4.708536365277026,
      "grad_norm": 4.224626064300537,
      "learning_rate": 1.7617758527341636e-06,
      "loss": 0.2525,
      "step": 26090
    },
    {
      "epoch": 4.710341093665403,
      "grad_norm": 1.897551417350769,
      "learning_rate": 1.7509474824038983e-06,
      "loss": 0.4706,
      "step": 26100
    },
    {
      "epoch": 4.712145822053781,
      "grad_norm": 1.1900510787963867,
      "learning_rate": 1.740119112073633e-06,
      "loss": 0.2392,
      "step": 26110
    },
    {
      "epoch": 4.713950550442158,
      "grad_norm": 8.888129234313965,
      "learning_rate": 1.7292907417433676e-06,
      "loss": 0.2662,
      "step": 26120
    },
    {
      "epoch": 4.715755278830536,
      "grad_norm": 1.6849883794784546,
      "learning_rate": 1.7184623714131023e-06,
      "loss": 0.2038,
      "step": 26130
    },
    {
      "epoch": 4.717560007218913,
      "grad_norm": 74.42950439453125,
      "learning_rate": 1.7076340010828372e-06,
      "loss": 0.1941,
      "step": 26140
    },
    {
      "epoch": 4.719364735607291,
      "grad_norm": 3.7184762954711914,
      "learning_rate": 1.6978884677855984e-06,
      "loss": 0.1133,
      "step": 26150
    },
    {
      "epoch": 4.721169463995668,
      "grad_norm": 98.6966552734375,
      "learning_rate": 1.687060097455333e-06,
      "loss": 0.1461,
      "step": 26160
    },
    {
      "epoch": 4.722974192384046,
      "grad_norm": 0.25385770201683044,
      "learning_rate": 1.6762317271250677e-06,
      "loss": 0.2509,
      "step": 26170
    },
    {
      "epoch": 4.724778920772424,
      "grad_norm": 0.24257247149944305,
      "learning_rate": 1.6654033567948024e-06,
      "loss": 0.0646,
      "step": 26180
    },
    {
      "epoch": 4.726583649160801,
      "grad_norm": 4.659267425537109,
      "learning_rate": 1.6545749864645371e-06,
      "loss": 0.2339,
      "step": 26190
    },
    {
      "epoch": 4.728388377549178,
      "grad_norm": 21.3764591217041,
      "learning_rate": 1.6437466161342718e-06,
      "loss": 0.1655,
      "step": 26200
    },
    {
      "epoch": 4.730193105937556,
      "grad_norm": 0.16386622190475464,
      "learning_rate": 1.6329182458040065e-06,
      "loss": 0.232,
      "step": 26210
    },
    {
      "epoch": 4.731997834325934,
      "grad_norm": 11.043522834777832,
      "learning_rate": 1.6220898754737412e-06,
      "loss": 0.1444,
      "step": 26220
    },
    {
      "epoch": 4.733802562714311,
      "grad_norm": 10.313947677612305,
      "learning_rate": 1.6112615051434759e-06,
      "loss": 0.1732,
      "step": 26230
    },
    {
      "epoch": 4.735607291102689,
      "grad_norm": 0.3375966250896454,
      "learning_rate": 1.6004331348132106e-06,
      "loss": 0.1033,
      "step": 26240
    },
    {
      "epoch": 4.7374120194910665,
      "grad_norm": 9.6199951171875,
      "learning_rate": 1.5896047644829453e-06,
      "loss": 0.2373,
      "step": 26250
    },
    {
      "epoch": 4.7392167478794445,
      "grad_norm": 0.25093916058540344,
      "learning_rate": 1.5787763941526802e-06,
      "loss": 0.1506,
      "step": 26260
    },
    {
      "epoch": 4.7410214762678216,
      "grad_norm": 45.96794891357422,
      "learning_rate": 1.5679480238224147e-06,
      "loss": 0.2701,
      "step": 26270
    },
    {
      "epoch": 4.7428262046561995,
      "grad_norm": 27.700191497802734,
      "learning_rate": 1.5571196534921496e-06,
      "loss": 0.1057,
      "step": 26280
    },
    {
      "epoch": 4.744630933044577,
      "grad_norm": 20.61056900024414,
      "learning_rate": 1.5462912831618843e-06,
      "loss": 0.2608,
      "step": 26290
    },
    {
      "epoch": 4.746435661432955,
      "grad_norm": 0.09542044997215271,
      "learning_rate": 1.5354629128316187e-06,
      "loss": 0.2118,
      "step": 26300
    },
    {
      "epoch": 4.748240389821332,
      "grad_norm": 21.036283493041992,
      "learning_rate": 1.5246345425013536e-06,
      "loss": 0.2095,
      "step": 26310
    },
    {
      "epoch": 4.75004511820971,
      "grad_norm": 13.35890007019043,
      "learning_rate": 1.5138061721710883e-06,
      "loss": 0.0873,
      "step": 26320
    },
    {
      "epoch": 4.751849846598087,
      "grad_norm": 0.15643376111984253,
      "learning_rate": 1.502977801840823e-06,
      "loss": 0.2122,
      "step": 26330
    },
    {
      "epoch": 4.753654574986465,
      "grad_norm": 23.534414291381836,
      "learning_rate": 1.4921494315105577e-06,
      "loss": 0.2429,
      "step": 26340
    },
    {
      "epoch": 4.755459303374842,
      "grad_norm": 37.826820373535156,
      "learning_rate": 1.4813210611802924e-06,
      "loss": 0.3029,
      "step": 26350
    },
    {
      "epoch": 4.75726403176322,
      "grad_norm": 15.068083763122559,
      "learning_rate": 1.470492690850027e-06,
      "loss": 0.1002,
      "step": 26360
    },
    {
      "epoch": 4.759068760151597,
      "grad_norm": 53.604881286621094,
      "learning_rate": 1.4596643205197618e-06,
      "loss": 0.1964,
      "step": 26370
    },
    {
      "epoch": 4.760873488539975,
      "grad_norm": 50.49022674560547,
      "learning_rate": 1.4488359501894967e-06,
      "loss": 0.2513,
      "step": 26380
    },
    {
      "epoch": 4.762678216928352,
      "grad_norm": 40.47334671020508,
      "learning_rate": 1.4380075798592312e-06,
      "loss": 0.2617,
      "step": 26390
    },
    {
      "epoch": 4.76448294531673,
      "grad_norm": 0.14024628698825836,
      "learning_rate": 1.4271792095289659e-06,
      "loss": 0.2154,
      "step": 26400
    },
    {
      "epoch": 4.766287673705108,
      "grad_norm": 41.669376373291016,
      "learning_rate": 1.4163508391987006e-06,
      "loss": 0.2008,
      "step": 26410
    },
    {
      "epoch": 4.768092402093485,
      "grad_norm": 62.195343017578125,
      "learning_rate": 1.4055224688684353e-06,
      "loss": 0.2018,
      "step": 26420
    },
    {
      "epoch": 4.769897130481862,
      "grad_norm": 3.2160465717315674,
      "learning_rate": 1.3946940985381702e-06,
      "loss": 0.1205,
      "step": 26430
    },
    {
      "epoch": 4.77170185887024,
      "grad_norm": 27.387731552124023,
      "learning_rate": 1.3838657282079046e-06,
      "loss": 0.1053,
      "step": 26440
    },
    {
      "epoch": 4.773506587258618,
      "grad_norm": 45.93486022949219,
      "learning_rate": 1.3730373578776395e-06,
      "loss": 0.2056,
      "step": 26450
    },
    {
      "epoch": 4.775311315646995,
      "grad_norm": 56.38935852050781,
      "learning_rate": 1.3622089875473742e-06,
      "loss": 0.2057,
      "step": 26460
    },
    {
      "epoch": 4.777116044035373,
      "grad_norm": 7.063026428222656,
      "learning_rate": 1.3513806172171087e-06,
      "loss": 0.2201,
      "step": 26470
    },
    {
      "epoch": 4.77892077242375,
      "grad_norm": 66.04423522949219,
      "learning_rate": 1.3405522468868436e-06,
      "loss": 0.3221,
      "step": 26480
    },
    {
      "epoch": 4.780725500812128,
      "grad_norm": 37.35698318481445,
      "learning_rate": 1.3297238765565783e-06,
      "loss": 0.2818,
      "step": 26490
    },
    {
      "epoch": 4.782530229200505,
      "grad_norm": 103.63336181640625,
      "learning_rate": 1.318895506226313e-06,
      "loss": 0.2991,
      "step": 26500
    },
    {
      "epoch": 4.784334957588883,
      "grad_norm": 34.80553436279297,
      "learning_rate": 1.3080671358960477e-06,
      "loss": 0.1652,
      "step": 26510
    },
    {
      "epoch": 4.78613968597726,
      "grad_norm": 0.26482006907463074,
      "learning_rate": 1.2972387655657824e-06,
      "loss": 0.1562,
      "step": 26520
    },
    {
      "epoch": 4.787944414365638,
      "grad_norm": 115.09516906738281,
      "learning_rate": 1.286410395235517e-06,
      "loss": 0.1429,
      "step": 26530
    },
    {
      "epoch": 4.789749142754015,
      "grad_norm": 14.85277271270752,
      "learning_rate": 1.2755820249052518e-06,
      "loss": 0.1425,
      "step": 26540
    },
    {
      "epoch": 4.791553871142393,
      "grad_norm": 35.97285461425781,
      "learning_rate": 1.2647536545749867e-06,
      "loss": 0.2847,
      "step": 26550
    },
    {
      "epoch": 4.79335859953077,
      "grad_norm": 14.47656536102295,
      "learning_rate": 1.2539252842447212e-06,
      "loss": 0.2018,
      "step": 26560
    },
    {
      "epoch": 4.795163327919148,
      "grad_norm": 2.6920063495635986,
      "learning_rate": 1.2430969139144558e-06,
      "loss": 0.2042,
      "step": 26570
    },
    {
      "epoch": 4.796968056307525,
      "grad_norm": 22.058977127075195,
      "learning_rate": 1.2322685435841905e-06,
      "loss": 0.205,
      "step": 26580
    },
    {
      "epoch": 4.798772784695903,
      "grad_norm": 14.558258056640625,
      "learning_rate": 1.2214401732539252e-06,
      "loss": 0.3002,
      "step": 26590
    },
    {
      "epoch": 4.8005775130842805,
      "grad_norm": 48.73124313354492,
      "learning_rate": 1.2106118029236601e-06,
      "loss": 0.2375,
      "step": 26600
    },
    {
      "epoch": 4.8023822414726585,
      "grad_norm": 29.189847946166992,
      "learning_rate": 1.1997834325933946e-06,
      "loss": 0.3156,
      "step": 26610
    },
    {
      "epoch": 4.8041869698610355,
      "grad_norm": 0.6347815990447998,
      "learning_rate": 1.1889550622631295e-06,
      "loss": 0.0921,
      "step": 26620
    },
    {
      "epoch": 4.8059916982494135,
      "grad_norm": 11.930288314819336,
      "learning_rate": 1.1781266919328642e-06,
      "loss": 0.1123,
      "step": 26630
    },
    {
      "epoch": 4.8077964266377915,
      "grad_norm": 10.152060508728027,
      "learning_rate": 1.1672983216025987e-06,
      "loss": 0.1149,
      "step": 26640
    },
    {
      "epoch": 4.809601155026169,
      "grad_norm": 0.5593626499176025,
      "learning_rate": 1.1564699512723336e-06,
      "loss": 0.2186,
      "step": 26650
    },
    {
      "epoch": 4.811405883414546,
      "grad_norm": 44.00059127807617,
      "learning_rate": 1.1456415809420683e-06,
      "loss": 0.3502,
      "step": 26660
    },
    {
      "epoch": 4.813210611802924,
      "grad_norm": 16.062965393066406,
      "learning_rate": 1.134813210611803e-06,
      "loss": 0.0858,
      "step": 26670
    },
    {
      "epoch": 4.815015340191302,
      "grad_norm": 17.467145919799805,
      "learning_rate": 1.1239848402815377e-06,
      "loss": 0.3861,
      "step": 26680
    },
    {
      "epoch": 4.816820068579679,
      "grad_norm": 1.47977614402771,
      "learning_rate": 1.1131564699512724e-06,
      "loss": 0.181,
      "step": 26690
    },
    {
      "epoch": 4.818624796968057,
      "grad_norm": 0.8674963116645813,
      "learning_rate": 1.102328099621007e-06,
      "loss": 0.1848,
      "step": 26700
    },
    {
      "epoch": 4.820429525356434,
      "grad_norm": 21.48860740661621,
      "learning_rate": 1.0914997292907418e-06,
      "loss": 0.2816,
      "step": 26710
    },
    {
      "epoch": 4.822234253744812,
      "grad_norm": 37.285213470458984,
      "learning_rate": 1.0806713589604767e-06,
      "loss": 0.2548,
      "step": 26720
    },
    {
      "epoch": 4.824038982133189,
      "grad_norm": 0.6601774096488953,
      "learning_rate": 1.0698429886302111e-06,
      "loss": 0.3146,
      "step": 26730
    },
    {
      "epoch": 4.825843710521567,
      "grad_norm": 2.2271981239318848,
      "learning_rate": 1.0590146182999458e-06,
      "loss": 0.093,
      "step": 26740
    },
    {
      "epoch": 4.827648438909944,
      "grad_norm": 22.337642669677734,
      "learning_rate": 1.0481862479696805e-06,
      "loss": 0.2718,
      "step": 26750
    },
    {
      "epoch": 4.829453167298322,
      "grad_norm": 94.16957092285156,
      "learning_rate": 1.0373578776394152e-06,
      "loss": 0.2263,
      "step": 26760
    },
    {
      "epoch": 4.831257895686699,
      "grad_norm": 51.50407791137695,
      "learning_rate": 1.0265295073091501e-06,
      "loss": 0.214,
      "step": 26770
    },
    {
      "epoch": 4.833062624075077,
      "grad_norm": 1.7360899448394775,
      "learning_rate": 1.0157011369788846e-06,
      "loss": 0.0836,
      "step": 26780
    },
    {
      "epoch": 4.834867352463454,
      "grad_norm": 15.582099914550781,
      "learning_rate": 1.0048727666486193e-06,
      "loss": 0.3215,
      "step": 26790
    },
    {
      "epoch": 4.836672080851832,
      "grad_norm": 7.2748703956604,
      "learning_rate": 9.940443963183542e-07,
      "loss": 0.2208,
      "step": 26800
    },
    {
      "epoch": 4.838476809240209,
      "grad_norm": 23.813976287841797,
      "learning_rate": 9.832160259880887e-07,
      "loss": 0.2261,
      "step": 26810
    },
    {
      "epoch": 4.840281537628587,
      "grad_norm": 0.3373171091079712,
      "learning_rate": 9.723876556578236e-07,
      "loss": 0.1813,
      "step": 26820
    },
    {
      "epoch": 4.842086266016964,
      "grad_norm": 58.541053771972656,
      "learning_rate": 9.615592853275583e-07,
      "loss": 0.2606,
      "step": 26830
    },
    {
      "epoch": 4.843890994405342,
      "grad_norm": 93.69788360595703,
      "learning_rate": 9.507309149972929e-07,
      "loss": 0.0912,
      "step": 26840
    },
    {
      "epoch": 4.845695722793719,
      "grad_norm": 32.948699951171875,
      "learning_rate": 9.399025446670277e-07,
      "loss": 0.2745,
      "step": 26850
    },
    {
      "epoch": 4.847500451182097,
      "grad_norm": 2.856140613555908,
      "learning_rate": 9.290741743367623e-07,
      "loss": 0.103,
      "step": 26860
    },
    {
      "epoch": 4.849305179570475,
      "grad_norm": 16.536476135253906,
      "learning_rate": 9.182458040064971e-07,
      "loss": 0.1971,
      "step": 26870
    },
    {
      "epoch": 4.851109907958852,
      "grad_norm": 71.85607147216797,
      "learning_rate": 9.074174336762317e-07,
      "loss": 0.085,
      "step": 26880
    },
    {
      "epoch": 4.852914636347229,
      "grad_norm": 3.9239132404327393,
      "learning_rate": 8.965890633459664e-07,
      "loss": 0.242,
      "step": 26890
    },
    {
      "epoch": 4.854719364735607,
      "grad_norm": 19.244409561157227,
      "learning_rate": 8.857606930157011e-07,
      "loss": 0.1146,
      "step": 26900
    },
    {
      "epoch": 4.856524093123985,
      "grad_norm": 25.441316604614258,
      "learning_rate": 8.749323226854359e-07,
      "loss": 0.2107,
      "step": 26910
    },
    {
      "epoch": 4.858328821512362,
      "grad_norm": 1.2445231676101685,
      "learning_rate": 8.641039523551706e-07,
      "loss": 0.101,
      "step": 26920
    },
    {
      "epoch": 4.86013354990074,
      "grad_norm": 20.97418212890625,
      "learning_rate": 8.532755820249052e-07,
      "loss": 0.435,
      "step": 26930
    },
    {
      "epoch": 4.861938278289117,
      "grad_norm": 20.215490341186523,
      "learning_rate": 8.4244721169464e-07,
      "loss": 0.166,
      "step": 26940
    },
    {
      "epoch": 4.863743006677495,
      "grad_norm": 0.25399377942085266,
      "learning_rate": 8.316188413643747e-07,
      "loss": 0.1814,
      "step": 26950
    },
    {
      "epoch": 4.8655477350658725,
      "grad_norm": 0.09821084141731262,
      "learning_rate": 8.207904710341094e-07,
      "loss": 0.1504,
      "step": 26960
    },
    {
      "epoch": 4.86735246345425,
      "grad_norm": 1.0418497323989868,
      "learning_rate": 8.099621007038441e-07,
      "loss": 0.2576,
      "step": 26970
    },
    {
      "epoch": 4.8691571918426275,
      "grad_norm": 0.6665719151496887,
      "learning_rate": 7.991337303735789e-07,
      "loss": 0.2643,
      "step": 26980
    },
    {
      "epoch": 4.8709619202310055,
      "grad_norm": 13.23661994934082,
      "learning_rate": 7.883053600433135e-07,
      "loss": 0.3596,
      "step": 26990
    },
    {
      "epoch": 4.872766648619383,
      "grad_norm": 35.81401443481445,
      "learning_rate": 7.774769897130481e-07,
      "loss": 0.1653,
      "step": 27000
    },
    {
      "epoch": 4.8745713770077606,
      "grad_norm": 0.9986875057220459,
      "learning_rate": 7.666486193827829e-07,
      "loss": 0.2456,
      "step": 27010
    },
    {
      "epoch": 4.876376105396138,
      "grad_norm": 23.442493438720703,
      "learning_rate": 7.558202490525176e-07,
      "loss": 0.1283,
      "step": 27020
    },
    {
      "epoch": 4.878180833784516,
      "grad_norm": 0.2903582453727722,
      "learning_rate": 7.449918787222523e-07,
      "loss": 0.1784,
      "step": 27030
    },
    {
      "epoch": 4.879985562172893,
      "grad_norm": 14.241569519042969,
      "learning_rate": 7.34163508391987e-07,
      "loss": 0.2156,
      "step": 27040
    },
    {
      "epoch": 4.881790290561271,
      "grad_norm": 1.348319172859192,
      "learning_rate": 7.233351380617217e-07,
      "loss": 0.3311,
      "step": 27050
    },
    {
      "epoch": 4.883595018949648,
      "grad_norm": 0.18944036960601807,
      "learning_rate": 7.125067677314564e-07,
      "loss": 0.1734,
      "step": 27060
    },
    {
      "epoch": 4.885399747338026,
      "grad_norm": 6.177163600921631,
      "learning_rate": 7.016783974011911e-07,
      "loss": 0.1019,
      "step": 27070
    },
    {
      "epoch": 4.887204475726403,
      "grad_norm": 84.19198608398438,
      "learning_rate": 6.908500270709259e-07,
      "loss": 0.3026,
      "step": 27080
    },
    {
      "epoch": 4.889009204114781,
      "grad_norm": 4.111339569091797,
      "learning_rate": 6.800216567406606e-07,
      "loss": 0.406,
      "step": 27090
    },
    {
      "epoch": 4.890813932503159,
      "grad_norm": 1.098319411277771,
      "learning_rate": 6.691932864103952e-07,
      "loss": 0.2032,
      "step": 27100
    },
    {
      "epoch": 4.892618660891536,
      "grad_norm": 2.4581336975097656,
      "learning_rate": 6.5836491608013e-07,
      "loss": 0.1927,
      "step": 27110
    },
    {
      "epoch": 4.894423389279913,
      "grad_norm": 57.996150970458984,
      "learning_rate": 6.475365457498647e-07,
      "loss": 0.2202,
      "step": 27120
    },
    {
      "epoch": 4.896228117668291,
      "grad_norm": 10.034281730651855,
      "learning_rate": 6.367081754195994e-07,
      "loss": 0.1857,
      "step": 27130
    },
    {
      "epoch": 4.898032846056669,
      "grad_norm": 0.4626403748989105,
      "learning_rate": 6.258798050893342e-07,
      "loss": 0.3265,
      "step": 27140
    },
    {
      "epoch": 4.899837574445046,
      "grad_norm": 0.20230989158153534,
      "learning_rate": 6.150514347590688e-07,
      "loss": 0.0746,
      "step": 27150
    },
    {
      "epoch": 4.901642302833424,
      "grad_norm": 47.440982818603516,
      "learning_rate": 6.042230644288034e-07,
      "loss": 0.1547,
      "step": 27160
    },
    {
      "epoch": 4.903447031221801,
      "grad_norm": 5.998945713043213,
      "learning_rate": 5.933946940985381e-07,
      "loss": 0.1106,
      "step": 27170
    },
    {
      "epoch": 4.905251759610179,
      "grad_norm": 0.0650024563074112,
      "learning_rate": 5.825663237682729e-07,
      "loss": 0.175,
      "step": 27180
    },
    {
      "epoch": 4.907056487998556,
      "grad_norm": 19.656208038330078,
      "learning_rate": 5.717379534380076e-07,
      "loss": 0.2575,
      "step": 27190
    },
    {
      "epoch": 4.908861216386934,
      "grad_norm": 1.4900379180908203,
      "learning_rate": 5.609095831077423e-07,
      "loss": 0.0828,
      "step": 27200
    },
    {
      "epoch": 4.910665944775311,
      "grad_norm": 8.686006546020508,
      "learning_rate": 5.50081212777477e-07,
      "loss": 0.2877,
      "step": 27210
    },
    {
      "epoch": 4.912470673163689,
      "grad_norm": 26.227624893188477,
      "learning_rate": 5.392528424472117e-07,
      "loss": 0.1252,
      "step": 27220
    },
    {
      "epoch": 4.914275401552066,
      "grad_norm": 1.3906079530715942,
      "learning_rate": 5.284244721169464e-07,
      "loss": 0.1608,
      "step": 27230
    },
    {
      "epoch": 4.916080129940444,
      "grad_norm": 6.032959938049316,
      "learning_rate": 5.175961017866811e-07,
      "loss": 0.0714,
      "step": 27240
    },
    {
      "epoch": 4.917884858328821,
      "grad_norm": 32.047080993652344,
      "learning_rate": 5.067677314564159e-07,
      "loss": 0.182,
      "step": 27250
    },
    {
      "epoch": 4.919689586717199,
      "grad_norm": 9.247130393981934,
      "learning_rate": 4.959393611261506e-07,
      "loss": 0.037,
      "step": 27260
    },
    {
      "epoch": 4.921494315105576,
      "grad_norm": 3.702709436416626,
      "learning_rate": 4.851109907958852e-07,
      "loss": 0.1873,
      "step": 27270
    },
    {
      "epoch": 4.923299043493954,
      "grad_norm": 0.2448604702949524,
      "learning_rate": 4.7428262046561995e-07,
      "loss": 0.2856,
      "step": 27280
    },
    {
      "epoch": 4.925103771882331,
      "grad_norm": 0.8998834490776062,
      "learning_rate": 4.6345425013535464e-07,
      "loss": 0.2765,
      "step": 27290
    },
    {
      "epoch": 4.926908500270709,
      "grad_norm": 2.119974374771118,
      "learning_rate": 4.5262587980508934e-07,
      "loss": 0.245,
      "step": 27300
    },
    {
      "epoch": 4.9287132286590865,
      "grad_norm": 18.014625549316406,
      "learning_rate": 4.4179750947482403e-07,
      "loss": 0.128,
      "step": 27310
    },
    {
      "epoch": 4.930517957047464,
      "grad_norm": 0.4444088935852051,
      "learning_rate": 4.3096913914455877e-07,
      "loss": 0.1727,
      "step": 27320
    },
    {
      "epoch": 4.932322685435842,
      "grad_norm": 80.79579162597656,
      "learning_rate": 4.2014076881429346e-07,
      "loss": 0.039,
      "step": 27330
    },
    {
      "epoch": 4.9341274138242195,
      "grad_norm": 0.20394539833068848,
      "learning_rate": 4.0931239848402816e-07,
      "loss": 0.1788,
      "step": 27340
    },
    {
      "epoch": 4.935932142212597,
      "grad_norm": 10.220396995544434,
      "learning_rate": 3.9848402815376285e-07,
      "loss": 0.1245,
      "step": 27350
    },
    {
      "epoch": 4.9377368706009745,
      "grad_norm": 2.7618448734283447,
      "learning_rate": 3.876556578234976e-07,
      "loss": 0.1846,
      "step": 27360
    },
    {
      "epoch": 4.9395415989893525,
      "grad_norm": 118.44380187988281,
      "learning_rate": 3.768272874932323e-07,
      "loss": 0.1734,
      "step": 27370
    },
    {
      "epoch": 4.94134632737773,
      "grad_norm": 4.448246955871582,
      "learning_rate": 3.65998917162967e-07,
      "loss": 0.2746,
      "step": 27380
    },
    {
      "epoch": 4.943151055766108,
      "grad_norm": 0.3255636394023895,
      "learning_rate": 3.551705468327017e-07,
      "loss": 0.1898,
      "step": 27390
    },
    {
      "epoch": 4.944955784154485,
      "grad_norm": 0.5837899446487427,
      "learning_rate": 3.4434217650243636e-07,
      "loss": 0.1109,
      "step": 27400
    },
    {
      "epoch": 4.946760512542863,
      "grad_norm": 73.71289825439453,
      "learning_rate": 3.335138061721711e-07,
      "loss": 0.2415,
      "step": 27410
    },
    {
      "epoch": 4.94856524093124,
      "grad_norm": 93.3230209350586,
      "learning_rate": 3.226854358419058e-07,
      "loss": 0.2434,
      "step": 27420
    },
    {
      "epoch": 4.950369969319618,
      "grad_norm": 4.3477582931518555,
      "learning_rate": 3.118570655116405e-07,
      "loss": 0.0946,
      "step": 27430
    },
    {
      "epoch": 4.952174697707995,
      "grad_norm": 55.849613189697266,
      "learning_rate": 3.0102869518137524e-07,
      "loss": 0.0876,
      "step": 27440
    },
    {
      "epoch": 4.953979426096373,
      "grad_norm": 3.1665048599243164,
      "learning_rate": 2.902003248511099e-07,
      "loss": 0.2501,
      "step": 27450
    },
    {
      "epoch": 4.95578415448475,
      "grad_norm": 19.926538467407227,
      "learning_rate": 2.793719545208446e-07,
      "loss": 0.2801,
      "step": 27460
    },
    {
      "epoch": 4.957588882873128,
      "grad_norm": 0.8734772205352783,
      "learning_rate": 2.6854358419057937e-07,
      "loss": 0.3788,
      "step": 27470
    },
    {
      "epoch": 4.959393611261505,
      "grad_norm": 68.57051086425781,
      "learning_rate": 2.57715213860314e-07,
      "loss": 0.1737,
      "step": 27480
    },
    {
      "epoch": 4.961198339649883,
      "grad_norm": 2.8600282669067383,
      "learning_rate": 2.4688684353004875e-07,
      "loss": 0.159,
      "step": 27490
    },
    {
      "epoch": 4.96300306803826,
      "grad_norm": 126.01680755615234,
      "learning_rate": 2.3605847319978342e-07,
      "loss": 0.4637,
      "step": 27500
    },
    {
      "epoch": 4.964807796426638,
      "grad_norm": 9.858895301818848,
      "learning_rate": 2.2523010286951814e-07,
      "loss": 0.2944,
      "step": 27510
    },
    {
      "epoch": 4.966612524815015,
      "grad_norm": 41.13008499145508,
      "learning_rate": 2.1440173253925286e-07,
      "loss": 0.2177,
      "step": 27520
    },
    {
      "epoch": 4.968417253203393,
      "grad_norm": 9.184991836547852,
      "learning_rate": 2.0357336220898755e-07,
      "loss": 0.2947,
      "step": 27530
    },
    {
      "epoch": 4.97022198159177,
      "grad_norm": 4.191811561584473,
      "learning_rate": 1.9274499187872224e-07,
      "loss": 0.2393,
      "step": 27540
    },
    {
      "epoch": 4.972026709980148,
      "grad_norm": 4.0181427001953125,
      "learning_rate": 1.8191662154845699e-07,
      "loss": 0.2856,
      "step": 27550
    },
    {
      "epoch": 4.973831438368525,
      "grad_norm": 12.500267028808594,
      "learning_rate": 1.7108825121819168e-07,
      "loss": 0.2166,
      "step": 27560
    },
    {
      "epoch": 4.975636166756903,
      "grad_norm": 114.14889526367188,
      "learning_rate": 1.6025988088792637e-07,
      "loss": 0.2757,
      "step": 27570
    },
    {
      "epoch": 4.97744089514528,
      "grad_norm": 76.35934448242188,
      "learning_rate": 1.4943151055766106e-07,
      "loss": 0.2025,
      "step": 27580
    },
    {
      "epoch": 4.979245623533658,
      "grad_norm": 30.64034080505371,
      "learning_rate": 1.3860314022739578e-07,
      "loss": 0.1838,
      "step": 27590
    },
    {
      "epoch": 4.981050351922036,
      "grad_norm": 12.983577728271484,
      "learning_rate": 1.2777476989713047e-07,
      "loss": 0.1771,
      "step": 27600
    },
    {
      "epoch": 4.982855080310413,
      "grad_norm": 57.46623992919922,
      "learning_rate": 1.1694639956686519e-07,
      "loss": 0.4064,
      "step": 27610
    },
    {
      "epoch": 4.984659808698791,
      "grad_norm": 0.7029456496238708,
      "learning_rate": 1.061180292365999e-07,
      "loss": 0.3527,
      "step": 27620
    },
    {
      "epoch": 4.986464537087168,
      "grad_norm": 1.4951069355010986,
      "learning_rate": 9.52896589063346e-08,
      "loss": 0.1752,
      "step": 27630
    },
    {
      "epoch": 4.988269265475546,
      "grad_norm": 12.482037544250488,
      "learning_rate": 8.44612885760693e-08,
      "loss": 0.312,
      "step": 27640
    },
    {
      "epoch": 4.990073993863923,
      "grad_norm": 0.3873496651649475,
      "learning_rate": 7.363291824580401e-08,
      "loss": 0.1253,
      "step": 27650
    },
    {
      "epoch": 4.991878722252301,
      "grad_norm": 10.157303810119629,
      "learning_rate": 6.28045479155387e-08,
      "loss": 0.1399,
      "step": 27660
    },
    {
      "epoch": 4.993683450640678,
      "grad_norm": 0.13891901075839996,
      "learning_rate": 5.197617758527342e-08,
      "loss": 0.2165,
      "step": 27670
    },
    {
      "epoch": 4.995488179029056,
      "grad_norm": 24.150114059448242,
      "learning_rate": 4.114780725500812e-08,
      "loss": 0.1228,
      "step": 27680
    },
    {
      "epoch": 4.9972929074174335,
      "grad_norm": 2.592759847640991,
      "learning_rate": 3.031943692474282e-08,
      "loss": 0.1961,
      "step": 27690
    },
    {
      "epoch": 4.9990976358058115,
      "grad_norm": 7.809803009033203,
      "learning_rate": 1.949106659447753e-08,
      "loss": 0.2642,
      "step": 27700
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.8376122366105672,
      "eval_f1": 0.7731232426407884,
      "eval_loss": 1.0930113792419434,
      "eval_precision": 0.7718482306686744,
      "eval_recall": 0.7744767716327586,
      "eval_runtime": 160.6251,
      "eval_samples_per_second": 137.98,
      "eval_steps_per_second": 8.629,
      "step": 27705
    }
  ],
  "logging_steps": 10,
  "max_steps": 27705,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.831119563687936e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
