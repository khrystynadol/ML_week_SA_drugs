{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### NLP Pipeline\n",
    "\n",
    "Let's start from importing basic libraries:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64e7fc04fd4eb857"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T09:04:43.951567400Z",
     "start_time": "2025-01-28T09:04:40.333930400Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's read our train data and create a DataFrame:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2267a5f474542ba2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0  patient_id       drugName                   condition  \\\n0           0       89879   Cyclosporine  keratoconjunctivitis sicca   \n1           1      143975   Etonogestrel               birth control   \n2           2      106473       Implanon               birth control   \n3           3      184526    Hydroxyzine                     anxiety   \n4           4       91587  Dalfampridine          multiple sclerosis   \n\n                                              review  rating            date  \\\n0  \"i have used restasis for about a year now and...     2.0  April 20, 2013   \n1  \"my experience has been somewhat mixed. i have...     7.0  August 7, 2016   \n2  \"this is my second implanon would not recommen...     1.0    May 11, 2016   \n3  \"i recommend taking as prescribed, and the bot...    10.0  March 19, 2012   \n4  \"i have been on ampyra for 5 days and have bee...     9.0  August 1, 2010   \n\n   usefulCount  review_length  \n0           69            147  \n1            4            136  \n2            6            140  \n3          124            104  \n4          101             74  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>patient_id</th>\n      <th>drugName</th>\n      <th>condition</th>\n      <th>review</th>\n      <th>rating</th>\n      <th>date</th>\n      <th>usefulCount</th>\n      <th>review_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>89879</td>\n      <td>Cyclosporine</td>\n      <td>keratoconjunctivitis sicca</td>\n      <td>\"i have used restasis for about a year now and...</td>\n      <td>2.0</td>\n      <td>April 20, 2013</td>\n      <td>69</td>\n      <td>147</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>143975</td>\n      <td>Etonogestrel</td>\n      <td>birth control</td>\n      <td>\"my experience has been somewhat mixed. i have...</td>\n      <td>7.0</td>\n      <td>August 7, 2016</td>\n      <td>4</td>\n      <td>136</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>106473</td>\n      <td>Implanon</td>\n      <td>birth control</td>\n      <td>\"this is my second implanon would not recommen...</td>\n      <td>1.0</td>\n      <td>May 11, 2016</td>\n      <td>6</td>\n      <td>140</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>184526</td>\n      <td>Hydroxyzine</td>\n      <td>anxiety</td>\n      <td>\"i recommend taking as prescribed, and the bot...</td>\n      <td>10.0</td>\n      <td>March 19, 2012</td>\n      <td>124</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>91587</td>\n      <td>Dalfampridine</td>\n      <td>multiple sclerosis</td>\n      <td>\"i have been on ampyra for 5 days and have bee...</td>\n      <td>9.0</td>\n      <td>August 1, 2010</td>\n      <td>101</td>\n      <td>74</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/drug_review_train.csv')\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T09:04:55.108997300Z",
     "start_time": "2025-01-28T09:04:53.524716900Z"
    }
   },
   "id": "95b83e3ea798e95",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading libraries, necessary for preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dde856e219b41d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python.exe -m pip install --upgrade pip\n",
    "!pip install nltk --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet\n",
    "!pip install contractions --upgrade --quiet\n",
    "\n",
    "!pip install unidecode --upgrade --quiet\n",
    "!pip install textblob --upgrade --quiet\n",
    "!pip install pyspellchecker --upgrade --quiet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34d415aec6294578"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a new Dataframe for preprocessed data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48cddf016d4a9940"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prep_df = pd.DataFrame()\n",
    "\n",
    "prep_df['patient_id'] = train_df['patient_id']\n",
    "prep_df['review'] = train_df['review']\n",
    "prep_df['drugName'] = train_df['drugName'].apply(lambda x: x.lower())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T09:08:02.861225800Z",
     "start_time": "2025-01-28T09:08:02.815409100Z"
    }
   },
   "id": "ee24c1432d3bf6f0",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Relabeling rating column:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e991f62e5ef9163"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def relabel_rating(rating):\n",
    "    if 0 <= rating <= 4:\n",
    "        return 'Negative'\n",
    "    elif 5 <= rating <= 7:\n",
    "        return 'Neutral'\n",
    "    elif 8 <= rating <= 10:\n",
    "        return 'Positive'\n",
    "\n",
    "prep_df['rating_category'] = train_df['rating'].apply(relabel_rating)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T09:05:10.812193Z",
     "start_time": "2025-01-28T09:05:10.756722300Z"
    }
   },
   "id": "c943870d50002238",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's import libraries for text preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f79d36e8192ba58"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Khrystyna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Khrystyna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Khrystyna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "sw_nltk = stopwords.words('english')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T09:43:17.644999500Z",
     "start_time": "2025-01-28T09:43:17.609231100Z"
    }
   },
   "id": "44b30036cef182e9",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are function for lemmatization with POS tagging.\n",
    "\n",
    "This step will be usable in case of using the vectorization techniques like TF-IDF or Word2Vec\n",
    "\n",
    "It is NOT expected to use this preprocessing step for Transformers like BERT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b156fc6c4d9e2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "# Lemmatize with POS tagging\n",
    "def lemmatize_with_pos(text):\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T09:05:32.836000400Z",
     "start_time": "2025-01-28T09:05:32.811485500Z"
    }
   },
   "id": "95df5ba4cf37d687",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's implement a class of all text preprocessing steps. It'll contain lowercase, some symbols and html tags removal, diacritics replacement, contractions expanding, spellchecking, stopwords removal and lemmatization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "781df65866fba4e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def to_lower(self):\n",
    "        # Let's check if first element is a list\n",
    "        if isinstance(self.X.iloc[0], list):     \n",
    "            self.X = self.X.apply(lambda tokens: [token.lower() for token in tokens])\n",
    "        else:\n",
    "            self.X = self.X.apply(lambda x: x.lower())\n",
    "        print(\"Lowercase done\")\n",
    "        return self\n",
    "    \n",
    "    def remove_numbers(self):\n",
    "        if isinstance(self.X.iloc[0], list):\n",
    "            self.X = self.X.apply(lambda tokens: [re.sub(r'\\d+', '', token) for token in tokens])\n",
    "        else:\n",
    "            self.X = self.X.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "        print(\"Numbers removal done\")\n",
    "        return self\n",
    "\n",
    "    def remove_dots(self):\n",
    "        if isinstance(self.X.iloc[0], list):     \n",
    "            self.X = self.X.apply(lambda tokens: [re.sub(\"[.]\", \"\", token) for token in tokens])\n",
    "        else:\n",
    "            self.X = self.X.apply(lambda x: re.sub(\"[.]\", \"\", x))\n",
    "        print(\"Dots removal done\")\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self):\n",
    "        # '!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~' 32 punctuations in python string module\n",
    "        if isinstance(self.X.iloc[0], list):     \n",
    "            self.X = self.X.apply(lambda tokens: [re.sub('[%s]' % re.escape(string.punctuation), '', token) for token in tokens])\n",
    "        else:\n",
    "            self.X = self.X.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "        print(\"Punctuation removal done\")\n",
    "        return self\n",
    "    \n",
    "    def remove_multi_whitespace(self):\n",
    "        if isinstance(self.X.iloc[0], list):     \n",
    "            self.X = self.X.apply(lambda tokens: [re.sub(' +', ' ', token) for token in tokens])\n",
    "        else:\n",
    "            self.X = self.X.apply(lambda x: re.sub(' +', ' ', x))\n",
    "        print(\"Multi whitespaces removal done\")\n",
    "        return self\n",
    "    \n",
    "    def expand_contractions(self):\n",
    "        if isinstance(self.X.iloc[0], list):\n",
    "            self.X = self.X.apply(\n",
    "                lambda tokens: [contractions.fix(str(token)) for token in tokens if isinstance(token, str)]\n",
    "            )\n",
    "        else: \n",
    "            self.X = self.X.apply(\n",
    "                lambda x: \" \".join([contractions.fix(str(word)) for word in x.split() if isinstance(word, str)])\n",
    "            )\n",
    "        print(\"Contractions expand done\")\n",
    "        return self\n",
    "\n",
    "    # Is this step usable for current dataset?\n",
    "    def remove_html_tags(self):\n",
    "        self.X = self.X.apply(\n",
    "            lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "        print(\"HTML tags removal done\")\n",
    "        return self\n",
    "\n",
    "    def replace_diacritics(self):\n",
    "        def process_tokens(tokens):\n",
    "            try:\n",
    "                return [unidecode(str(token)) for token in tokens]\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing tokens: {tokens}. Error: {e}\")\n",
    "                return tokens\n",
    "    \n",
    "        if isinstance(self.X.iloc[0], list):\n",
    "            self.X = self.X.apply(lambda tokens: process_tokens(tokens) if isinstance(tokens, list) else tokens)\n",
    "        else:\n",
    "            self.X = self.X.apply(lambda x: unidecode(str(x)) if isinstance(x, str) else str(x))\n",
    "        \n",
    "        print(\"Diacritics replacement done\")\n",
    "        return self\n",
    "    \n",
    "    def spellcheck(self):\n",
    "        self.X = self.X.apply(lambda tokens: ' '.join(tokens))\n",
    "        self.X = self.X.apply(lambda x: str(TextBlob(x).correct()))  \n",
    "        self.X = self.X.apply(lambda x: x.split()) \n",
    "        print(\"Spellcheck done\")\n",
    "        return self\n",
    "    \n",
    "    # Will NOT be used for Transformers\n",
    "    def remove_stopwords(self):\n",
    "        # Possible to add custom stopwords\n",
    "        # new_stopwords = ['drugs']\n",
    "        # sw_nltk.extend(new_stopwords)\n",
    "        # Possible to remove already existing stopwords\n",
    "        sw_nltk.remove('not')\n",
    "        if isinstance(self.X.iloc[0], list):\n",
    "            self.X = self.X.apply(lambda tokens: [word for word in tokens if word not in sw_nltk])\n",
    "        else:\n",
    "            self.X = self.X.apply(lambda x: \" \".join([word for word in x.split() if word not in sw_nltk]))\n",
    "        print(\"Stopwords removal done\")\n",
    "        return self\n",
    "    \n",
    "    # Will NOT be used for Transformers\n",
    "    def lemmatize(self):\n",
    "        self.X = self.X.apply(lemmatize_with_pos)\n",
    "        print(\"Lemmatization done\")\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T09:53:17.305478100Z",
     "start_time": "2025-01-28T09:53:17.278809400Z"
    }
   },
   "id": "9d4783a8d85e76bc",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### !!! Important !!!\n",
    "\n",
    "1. Simple word vectorizing techniques like TF-IDF, Word2Vec benefit from lemmatizing.\n",
    "2. Topic Modeling benefits from Lemmatization\n",
    "3. Sentiment Analysis can sometimes get hurt by lemmtization and certainly by removal of certain stop words\n",
    "4. It has been empirically seen that lemmatizing sentences deteriorates accuracy of pre-trained Large Language Models in BERT etc.\n",
    "\n",
    "Source: [Elegant Text Pre-Processing with NLTK in sklearn Pipeline](https://towardsdatascience.com/elegant-text-pre-processing-with-nltk-in-sklearn-pipeline-d6fe18b91eb8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13b43c5b6b87df99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's preprocess our data and save it to .csv file:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bcb384c0e3cebbc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase done\n",
      "Numbers removal done\n",
      "Dots removal done\n",
      "Punctuation removal done\n",
      "Multi whitespaces removal done\n"
     ]
    }
   ],
   "source": [
    "text_preprocessor = Pipeline(train_df['review'].apply(lambda x: x.split()))\n",
    "\n",
    "prep_df['review'] = text_preprocessor.to_lower().remove_numbers().remove_dots().remove_punctuation().remove_multi_whitespace().X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T09:54:16.320212800Z",
     "start_time": "2025-01-28T09:53:21.299256400Z"
    }
   },
   "id": "c82beff45e8e50f6",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### !!! Important !!!\n",
    "Check order of the following steps:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1c0046c31fe9d96"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contractions expand done\n",
      "Diacritics replacement done\n"
     ]
    }
   ],
   "source": [
    "prep_df['review'] = text_preprocessor.expand_contractions().replace_diacritics().X\n",
    "# .spellcheck().X)  # works too long"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T10:03:07.155181300Z",
     "start_time": "2025-01-28T10:02:44.709985300Z"
    }
   },
   "id": "333853d4ba28ee16",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   patient_id                                             review  \\\n0       89879  i have used restasis for about a year now and ...   \n1      143975  my experience has been somewhat mixed i have b...   \n2      106473  this is my second implanon would not recommend...   \n3      184526  i recommend taking as prescribed and the bottl...   \n4       91587  i have been on ampyra for days and have been s...   \n\n        drugName  \n0   cyclosporine  \n1   etonogestrel  \n2       implanon  \n3    hydroxyzine  \n4  dalfampridine  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>review</th>\n      <th>drugName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>89879</td>\n      <td>i have used restasis for about a year now and ...</td>\n      <td>cyclosporine</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>143975</td>\n      <td>my experience has been somewhat mixed i have b...</td>\n      <td>etonogestrel</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106473</td>\n      <td>this is my second implanon would not recommend...</td>\n      <td>implanon</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>184526</td>\n      <td>i recommend taking as prescribed and the bottl...</td>\n      <td>hydroxyzine</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>91587</td>\n      <td>i have been on ampyra for days and have been s...</td>\n      <td>dalfampridine</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T10:03:09.792040100Z",
     "start_time": "2025-01-28T10:03:09.760626Z"
    }
   },
   "id": "e056ed770a683494",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prep_df.to_csv('prep_data/drug_review_train_prep.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T10:04:31.136941200Z",
     "start_time": "2025-01-28T10:04:29.331662100Z"
    }
   },
   "id": "21727957f7dcf523",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removal done\n",
      "Lemmatization done\n"
     ]
    }
   ],
   "source": [
    "prep_df['review'] = text_preprocessor.remove_stopwords().lemmatize().X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T10:08:36.848652700Z",
     "start_time": "2025-01-28T10:04:38.435469800Z"
    }
   },
   "id": "9d6c914c00f594b4",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prep_df.to_csv('prep_data/drug_review_train_prep_full.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-28T10:08:50.968573200Z",
     "start_time": "2025-01-28T10:08:49.582146800Z"
    }
   },
   "id": "4ce79aaf5dc54304",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "671dee2dadbf1702"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
